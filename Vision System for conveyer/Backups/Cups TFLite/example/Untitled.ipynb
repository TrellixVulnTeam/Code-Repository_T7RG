{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tflite_runtime'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-bd390f41d73d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtflite_runtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpreter\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtflite\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tflite_runtime'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tflite_runtime.interpreter as tflite\n",
    "\n",
    "\n",
    "def get_model_and_sig(model_dir):\n",
    "    \"\"\"Method to get name of model file. Assumes model is in the parent directory for script.\"\"\"\n",
    "    with open(os.path.join(model_dir, \"../signature.json\"), \"r\") as f:\n",
    "        signature = json.load(f)\n",
    "    model_file = \"../\" + signature.get(\"filename\")\n",
    "    if not os.path.isfile(model_file):\n",
    "        raise FileNotFoundError(f\"Model file does not exist\")\n",
    "    return model_file, signature\n",
    "\n",
    "\n",
    "def load_model(model_file):\n",
    "    \"\"\"Load the model from path to model file\"\"\"\n",
    "    # Load TFLite model and allocate tensors.\n",
    "    interpreter = tflite.Interpreter(model_path=model_file)\n",
    "    interpreter.allocate_tensors()\n",
    "    return interpreter\n",
    "\n",
    "\n",
    "def get_prediction(image, interpreter, signature):\n",
    "    \"\"\"\n",
    "    Predict with the TFLite interpreter!\n",
    "    \"\"\"\n",
    "    # Combine the information about the inputs and outputs from the signature.json file with the Interpreter runtime\n",
    "    signature_inputs = signature.get(\"inputs\")\n",
    "    input_details = {detail.get(\"name\"): detail for detail in interpreter.get_input_details()}\n",
    "    model_inputs = {key: {**sig, **input_details.get(sig.get(\"name\"))} for key, sig in signature_inputs.items()}\n",
    "    signature_outputs = signature.get(\"outputs\")\n",
    "    output_details = {detail.get(\"name\"): detail for detail in interpreter.get_output_details()}\n",
    "    model_outputs = {key: {**sig, **output_details.get(sig.get(\"name\"))} for key, sig in signature_outputs.items()}\n",
    "\n",
    "    if \"Image\" not in model_inputs:\n",
    "        raise ValueError(\"Tensorflow Lite model doesn't have 'Image' input! Check signature.json, and please report issue to Lobe.\")\n",
    "\n",
    "    # process image to be compatible with the model\n",
    "    input_data = process_image(image, model_inputs.get(\"Image\").get(\"shape\"))\n",
    "\n",
    "    # set the input to run\n",
    "    interpreter.set_tensor(model_inputs.get(\"Image\").get(\"index\"), input_data)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # grab our desired outputs from the interpreter!\n",
    "    # un-batch since we ran an image with batch size of 1, and convert to normal python types with tolist()\n",
    "    outputs = {key: interpreter.get_tensor(value.get(\"index\")).tolist()[0] for key, value in model_outputs.items()}\n",
    "    # postprocessing! convert any byte strings to normal strings with .decode()\n",
    "    for key, val in outputs.items():\n",
    "        if isinstance(val, bytes):\n",
    "            outputs[key] = val.decode()\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def process_image(image, input_shape):\n",
    "    \"\"\"\n",
    "    Given a PIL Image, center square crop and resize to fit the expected model input, and convert from [0,255] to [0,1] values.\n",
    "    \"\"\"\n",
    "    width, height = image.size\n",
    "    # ensure image type is compatible with model and convert if not\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "    # center crop image (you can substitute any other method to make a square image, such as just resizing or padding edges with 0)\n",
    "    if width != height:\n",
    "        square_size = min(width, height)\n",
    "        left = (width - square_size) / 2\n",
    "        top = (height - square_size) / 2\n",
    "        right = (width + square_size) / 2\n",
    "        bottom = (height + square_size) / 2\n",
    "        # Crop the center of the image\n",
    "        image = image.crop((left, top, right, bottom))\n",
    "    # now the image is square, resize it to be the right shape for the model input\n",
    "    input_width, input_height = input_shape[1:3]\n",
    "    if image.width != input_width or image.height != input_height:\n",
    "        image = image.resize((input_width, input_height))\n",
    "\n",
    "    # make 0-1 float instead of 0-255 int (that PIL Image loads by default)\n",
    "    image = np.asarray(image) / 255.0\n",
    "    # format input as model expects\n",
    "    return image.reshape(input_shape).astype(np.float32)\n",
    "\n",
    "\n",
    "def main(image, model_dir):\n",
    "    \"\"\"\n",
    "    Load the model and signature files, start the TF Lite interpreter, and run prediction on the image.\n",
    "\n",
    "    Output prediction will be a dictionary with the same keys as the outputs in the signature.json file.\n",
    "    \"\"\"\n",
    "    model_file, signature = get_model_and_sig(model_dir)\n",
    "    interpreter = load_model(model_file)\n",
    "    prediction = get_prediction(image, interpreter, signature)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Predict a label for an image.\")\n",
    "    parser.add_argument(\"image\", help=\"Path to your image file.\")\n",
    "    args = parser.parse_args()\n",
    "    if os.path.isfile(args.image):\n",
    "        image = Image.open(args.image)\n",
    "        # convert to rgb image if this isn't one\n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        # Assume model is in the parent directory for this file\n",
    "        model_dir = os.getcwd()\n",
    "        print(main(image, model_dir))\n",
    "    else:\n",
    "        print(f\"Couldn't find image file {args.image}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
