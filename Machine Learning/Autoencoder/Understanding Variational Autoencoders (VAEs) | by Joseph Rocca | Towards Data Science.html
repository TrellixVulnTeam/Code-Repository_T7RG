<!DOCTYPE html>
<html data-rh="lang" lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8"><script async="" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/branch-latest.js"></script><script async="" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/analytics.js"></script><script defer="defer" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/16180790160.js"></script><title>Understanding Variational Autoencoders (VAEs) | by Joseph Rocca | Towards Data Science</title><meta data-rh="true" charset="utf-8"><meta data-rh="true" name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1,maximum-scale=1"><meta data-rh="true" name="theme-color" content="#000000"><meta data-rh="true" name="twitter:app:name:iphone" content="Medium"><meta data-rh="true" name="twitter:app:id:iphone" content="828256236"><meta data-rh="true" property="al:ios:app_name" content="Medium"><meta data-rh="true" property="al:ios:app_store_id" content="828256236"><meta data-rh="true" property="al:android:package" content="com.medium.reader"><meta data-rh="true" property="fb:app_id" content="542599432471018"><meta data-rh="true" property="og:site_name" content="Medium"><meta data-rh="true" property="al:android:app_name" content="Medium"><link data-rh="true" rel="search" type="application/opensearchdescription+xml" title="Medium" href="https://towardsdatascience.com/osd.xml"><link data-rh="true" rel="apple-touch-icon" sizes="152x152" href="https://miro.medium.com/fit/c/152/152/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"><link data-rh="true" rel="apple-touch-icon" sizes="120x120" href="https://miro.medium.com/fit/c/120/120/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"><link data-rh="true" rel="apple-touch-icon" sizes="76x76" href="https://miro.medium.com/fit/c/76/76/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"><link data-rh="true" rel="apple-touch-icon" sizes="60x60" href="https://miro.medium.com/fit/c/60/60/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"><link data-rh="true" rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/Medium-Avatar-500x500.svg" color="#171717"><link data-rh="true" rel="preconnect" href="https://glyph.medium.com/" crossorigin=""><link data-rh="true" rel="preconnect" href="https://logx.optimizely.com/"><link data-rh="true" id="glyph_preload_link" rel="preload" as="style" type="text/css" href="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/unbound.css"><link data-rh="true" id="glyph_link" rel="stylesheet" type="text/css" href="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/unbound.css"><link rel="preload" href="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/16180790160.js" as="script"><style type="text/css" data-fela-rehydration="608" data-fela-type="STATIC">html{box-sizing:border-box}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}:root{--reach-tabs:1;--reach-menu-button:1}</style><style type="text/css" data-fela-rehydration="608" data-fela-type="KEYFRAME">@-webkit-keyframes k1{from{filter:hue-rotate(0deg)}to{filter:hue-rotate(360deg)}}@-moz-keyframes k1{from{filter:hue-rotate(0deg)}to{filter:hue-rotate(360deg)}}@keyframes k1{from{filter:hue-rotate(0deg)}to{filter:hue-rotate(360deg)}}@-webkit-keyframes k2{0%{opacity:0;transform:translateY(-60px)}100%{opacity:1;transform:translateY(0px)}}@-moz-keyframes k2{0%{opacity:0;transform:translateY(-60px)}100%{opacity:1;transform:translateY(0px)}}@keyframes k2{0%{opacity:0;transform:translateY(-60px)}100%{opacity:1;transform:translateY(0px)}}@-webkit-keyframes k3{0%{opacity:1;transform:translateY(0px)}100%{opacity:0;transform:translateY(-60px)}}@-moz-keyframes k3{0%{opacity:1;transform:translateY(0px)}100%{opacity:0;transform:translateY(-60px)}}@keyframes k3{0%{opacity:1;transform:translateY(0px)}100%{opacity:0;transform:translateY(-60px)}}@-webkit-keyframes k4{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-moz-keyframes k4{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@keyframes k4{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}</style><style type="text/css" data-fela-rehydration="608" data-fela-type="RULE">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{height:100vh}.m{width:100vw}.n{display:flex}.o{align-items:center}.p{justify-content:center}.q{height:25px}.r{fill:rgba(41, 41, 41, 1)}.s{display:block}.t{margin-bottom:36px}.v{width:100%}.z{flex:0 0 auto}.ab{justify-self:flex-end}.ac{z-index:500}.ae{visibility:hidden}.af{overflow-x:scroll}.ag{white-space:nowrap}.ah{scrollbar-width:none}.ai{-ms-overflow-style:none}.aj::-webkit-scrollbar{display:none}.ak{box-shadow:inset 0 -1px 0 rgba(230, 230, 230, 1)}.al{min-height:184px}.ao{flex-direction:column}.ap{background-color:#355876}.aq{display:none}.as{border-bottom:none}.at{position:relative}.az{max-width:1192px}.ba{min-width:0}.bb{height:62px}.bc{flex-direction:row}.bd{flex:1 0 auto}.be{margin-right:16px}.bf{font-family:sohne, "Helvetica Neue", Helvetica, Arial, sans-serif}.bg{font-size:14px}.bh{line-height:20px}.bi{color:rgba(233, 241, 250, 1)}.bj{padding:7px 16px 9px}.bk{background:0}.bl{fill:rgba(233, 241, 250, 1)}.bm{border-color:rgba(215, 226, 238, 1)}.br:disabled{cursor:inherit !important}.bs:disabled{opacity:0.3}.bt:disabled:hover{color:rgba(233, 241, 250, 1)}.bu:disabled:hover{fill:rgba(233, 241, 250, 1)}.bv:disabled:hover{border-color:rgba(215, 226, 238, 1)}.bw{border-radius:99em}.bx{border-width:1px}.by{border-style:solid}.bz{box-sizing:border-box}.ca{display:inline-block}.cb{text-decoration:none}.cc{margin-left:0px}.cd{color:rgba(197, 210, 225, 1)}.ce{font-size:inherit}.cf{border:inherit}.cg{font-family:inherit}.ch{letter-spacing:inherit}.ci{font-weight:inherit}.cj{padding:0}.ck{margin:0}.cl:disabled{cursor:default}.cm:disabled{color:rgba(26, 137, 23, 0.3)}.cn:disabled{fill:rgba(26, 137, 23, 0.3)}.co{min-height:115px}.cp{justify-content:space-between}.cv{align-items:flex-start}.cw{margin-bottom:0px}.cx{margin-top:-32px}.cy{flex-wrap:wrap}.db{margin-top:32px}.dc{margin-right:24px}.de{height:35px}.df{width:112px}.dg{margin-bottom:-3px}.dh{margin-left:14px}.di{margin-top:-3px}.dj{fill:rgba(251, 255, 255, 1)}.dk{padding-top:1px}.dl{height:70px}.dn{font-size:16px}.do{line-height:24px}.dp:before{margin-bottom:-10px}.dq:before{content:""}.dr:before{display:table}.ds:before{border-collapse:collapse}.dt:after{margin-top:-6px}.du:after{content:""}.dv:after{display:table}.dw:after{border-collapse:collapse}.dx{color:rgba(117, 117, 117, 1)}.dy{margin-right:12px}.dz{display:inline-flex}.ea{color:inherit}.eb{fill:inherit}.ee:disabled{color:rgba(117, 117, 117, 1)}.ef:disabled{fill:rgba(117, 117, 117, 1)}.eg{margin-left:12px}.eh{margin:0 12px}.ei{position:absolute}.ej{right:24px}.ek{margin:0px}.el{border:0px}.em{padding:0px}.en{cursor:pointer}.eo{stroke:rgba(117, 117, 117, 1)}.er{left:0}.es{opacity:0}.et{position:fixed}.eu{right:0}.ev{top:0}.ex{height:60px}.fa{height:100%}.fd{color:rgba(102, 138, 170, 1)}.fe{fill:rgba(102, 138, 170, 1)}.ff{border-color:rgba(102, 138, 170, 1)}.fj:disabled:hover{color:rgba(102, 138, 170, 1)}.fk:disabled:hover{fill:rgba(102, 138, 170, 1)}.fl:disabled:hover{border-color:rgba(102, 138, 170, 1)}.fm{margin-left:16px}.fs{margin-left:auto}.ft{margin-right:auto}.fu{max-width:728px}.fv{background:rgba(255, 255, 255, 1)}.fw{border:1px solid rgba(230, 230, 230, 1)}.fx{border-radius:4px}.fy{box-shadow:0 1px 4px rgba(230, 230, 230, 1)}.fz{max-height:100vh}.ga{overflow-y:auto}.gb{top:calc(100vh + 100px)}.gc{bottom:calc(100vh + 100px)}.gd{width:10px}.ge{pointer-events:none}.gf{word-break:break-word}.gg{word-wrap:break-word}.gh:after{display:block}.gi:after{clear:both}.gj{max-width:680px}.gk{line-height:1.23}.gl{letter-spacing:0}.gm{font-style:normal}.gn{font-weight:700}.hi{margin-bottom:-0.27em}.hj{color:rgba(41, 41, 41, 1)}.hk{line-height:1.394}.ia{margin-bottom:-0.42em}.ie{width:28px}.if{height:28px}.ig{fill:rgba(26, 137, 23, 1)}.ih{width:calc(100% + 24px)}.ii{height:calc(100% + 24px)}.ij{top:50%}.ik{left:50%}.il{transform:translateX(-50%) translateY(-50%)}.im{border-radius:50%}.in{margin-left:8px}.io{margin:0 4px}.ip{margin:0 7px}.iq{align-items:flex-end}.iz{margin:0 6px 0 7px}.ja{clear:both}.jj{max-width:1920px}.jk{margin-top:33px}.jl{padding-bottom:5px}.jm{padding-top:5px}.jo{cursor:zoom-in}.jp{z-index:auto}.jr{max-width:100%}.js{height:auto}.jt{margin-top:10px}.ju{text-align:center}.jx{text-decoration:underline}.jy{line-height:1.58}.jz{letter-spacing:-0.004em}.ka{font-family:charter, Georgia, Cambria, "Times New Roman", Times, serif}.kt{margin-bottom:-0.46em}.ku{background:none}.kv{line-height:1.12}.kw{letter-spacing:-0.022em}.kx{font-weight:500}.lq{margin-bottom:-0.28em}.lw{max-width:1280px}.mc{transition:opacity 100ms 400ms}.md{overflow:hidden}.me{will-change:transform}.mf{transform:translateZ(0)}.mg{margin:auto}.mh{background-color:rgba(242, 242, 242, 1)}.mi{padding-bottom:53.57142857142857%}.mj{height:0}.mk{filter:blur(20px)}.ml{transform:scale(1.1)}.mm{visibility:visible}.mn{line-height:1.18}.mv{margin-bottom:-0.31em}.mw{box-shadow:inset 3px 0 0 0 rgba(41, 41, 41, 1)}.mx{padding-left:23px}.my{margin-left:-20px}.mz{font-style:italic}.na{margin-bottom:14px}.nb{padding-top:24px}.nc{padding-bottom:10px}.nd{background-color:rgba(8, 8, 8, 1)}.ne{height:3px}.nf{width:3px}.ng{margin-right:20px}.nm{max-width:274px}.nn{max-width:104px}.no{padding-bottom:20.19230769230769%}.np{max-width:631px}.nq{max-width:694.5px}.nr{padding-bottom:52.142857142857146%}.ns{list-style-type:disc}.nt{margin-left:30px}.nu{padding-left:0px}.oa{max-width:774px}.ob{max-width:633px}.oc{max-width:282px}.od{padding-bottom:51.41843971631206%}.oe{max-width:332px}.of{padding-bottom:14.759036144578312%}.og{max-width:391px}.oh{padding-bottom:13.428571428571429%}.oi{max-width:407px}.oj{padding-bottom:5.428571428571428%}.ok{max-width:813px}.ol{padding-bottom:31.142857142857146%}.om{max-width:320px}.on{padding-bottom:29.6875%}.oo{max-width:640px}.op{padding-bottom:8.142857142857142%}.oq{max-width:523px}.or{padding-bottom:4.142857142857143%}.os{max-width:643px}.ot{padding-bottom:46.28571428571429%}.ou{padding-bottom:40.57142857142858%}.ov{max-width:302px}.ow{padding-bottom:7.28476821192053%}.pc{box-shadow:inset 0 0 0 1px rgba(230, 230, 230, 1)}.pd{padding:16px 20px}.pe{flex:1 1 auto}.pg{max-height:40px}.ph{text-overflow:ellipsis}.pi{display:-webkit-box}.pj{-webkit-line-clamp:2}.pk{-webkit-box-orient:vertical}.pm{margin-top:8px}.pn{margin-top:12px}.po{font-size:13px}.pp{width:160px}.pq{background-image:url(https://miro.medium.com/max/320/1*kjouN-zV6BgpmCl5SnEjGQ.jpeg)}.pr{background-origin:border-box}.ps{background-size:cover}.pt{height:167px}.pu{background-position:50% 50%}.pv{background-image:url(https://miro.medium.com/max/320/1*q6x_dETZ3wZjpcrYFntmJQ.jpeg)}.pw{will-change:opacity}.px{width:188px}.py{transform:translateX(406px)}.pz{top:calc(65px + 54px + 14px)}.qc{will-change:opacity, transform}.qd{transform:translateY(159px)}.qf{width:197px}.qg{margin-bottom:20px}.qh{padding-top:2px}.qi{padding-top:20px}.qj{color:rgba(255, 255, 255, 1)}.qk{fill:rgba(255, 255, 255, 1)}.ql{background:rgba(102, 138, 170, 1)}.qn:disabled:hover{background:rgba(102, 138, 170, 1)}.qo{stroke:rgba(242, 242, 242, 1)}.qp{height:36px}.qq{width:36px}.qr{color:rgba(242, 242, 242, 1)}.qs{fill:rgba(242, 242, 242, 1)}.qt{background:rgba(242, 242, 242, 1)}.qu{border-color:rgba(242, 242, 242, 1)}.ra{padding-top:32px}.rb{border-top:1px solid rgba(230, 230, 230, 1)}.rc{justify-content:space-evenly}.ri{-webkit-user-select:none}.rj{outline:0}.rk{border:0}.rl{user-select:none}.rm> svg{pointer-events:none}.rx button{text-align:left}.ry{margin-top:2px}.rz{fill:rgba(61, 61, 61, 1)}.sa{opacity:1}.sb{padding-left:6px}.sc{margin-top:1px}.sd{margin-top:40px}.se{padding-bottom:25px}.sf{margin-top:25px}.sg{max-width:155px}.sn{top:1px}.sq{margin-left:24px}.sr{margin-top:4px}.ss{margin-left:4px}.st{margin-top:5px}.su{padding-bottom:40px}.sv{list-style-type:none}.sw{margin-right:8px}.sx{margin-bottom:8px}.sy{line-height:22px}.sz{border-radius:3px}.ta{padding:5px 10px}.tb{padding-bottom:4px}.tc{background-color:rgba(250, 250, 250, 1)}.ts{padding-top:25px}.tv{margin-bottom:96px}.tw{margin-bottom:40px}.tx{margin-top:24px}.ty{padding-bottom:16px}.tz{border-bottom:1px solid rgba(230, 230, 230, 1)}.ua{margin-bottom:24px}.vk{flex-grow:0}.vl{padding-bottom:24px}.vm{max-width:500px}.vn{flex:0 1 auto}.vp{padding-bottom:8px}.wa{padding-bottom:100%}.bn:hover{color:rgba(251, 255, 255, 1)}.bo:hover{fill:rgba(251, 255, 255, 1)}.bp:hover{border-color:rgba(251, 255, 255, 1)}.bq:hover{cursor:pointer}.ec:hover{color:rgba(25, 25, 25, 1)}.ed:hover{fill:rgba(25, 25, 25, 1)}.fg:hover{color:rgba(90, 118, 144, 1)}.fh:hover{fill:rgba(90, 118, 144, 1)}.fi:hover{border-color:rgba(90, 118, 144, 1)}.qm:hover{background:rgba(90, 118, 144, 1)}.qv:hover{background:rgba(242, 242, 242, 1)}.qw:hover{border-color:rgba(242, 242, 242, 1)}.qx:hover{cursor:wait}.qy:hover{color:rgba(242, 242, 242, 1)}.qz:hover{fill:rgba(242, 242, 242, 1)}.rp:hover{fill:rgba(117, 117, 117, 1)}.vx:hover{text-decoration:underline}.jq:focus{transform:scale(1.01)}.ro:focus{fill:rgba(117, 117, 117, 1)}.rn:active{border-style:none}</style><style type="text/css" data-fela-rehydration="608" data-fela-type="RULE" media="all and (min-width: 1080px)">.d{display:none}.w{display:flex}.ay{margin:0 64px}.fr{padding:0 16px}.he{font-size:46px}.hf{margin-top:0.6em}.hg{line-height:56px}.hh{letter-spacing:-0.011em}.hx{font-size:22px}.hy{margin-top:0.92em}.hz{line-height:28px}.ix{margin-left:30px}.ji{max-width:1192px}.kp{font-size:21px}.kq{margin-top:2em}.kr{line-height:32px}.ks{letter-spacing:-0.003em}.lm{font-size:30px}.ln{margin-top:1.95em}.lo{line-height:36px}.lp{letter-spacing:0}.lv{margin-top:0.86em}.mb{margin-top:56px}.mu{margin-top:1.72em}.nl{margin-top:1.25em}.nz{margin-top:1.05em}.pb{margin-top:32px}.rh{margin-right:5px}.rw{margin-top:0px}.sm{margin-top:5px}.sp{display:inline-block}.tp{font-size:20px}.tq{line-height:24px}.tr{max-height:48px}.tu{margin:0}.up{width:calc(100% + 32px)}.uq{margin-left:-16px}.ur{margin-right:-16px}.vg{padding-left:16px}.vh{padding-right:16px}.vi{flex-basis:25%}.vj{max-width:25%}.vu{font-size:16px}.vv{line-height:20px}.wj{min-width:70px}.wk{min-height:70px}</style><style type="text/css" data-fela-rehydration="608" data-fela-type="RULE" media="all and (max-width: 1079.98px)">.e{display:none}.iw{margin-left:30px}.jv{margin-left:auto}.jw{text-align:center}.rv{margin-top:0px}.sl{margin-top:5px}.so{display:inline-block}</style><style type="text/css" data-fela-rehydration="608" data-fela-type="RULE" media="all and (max-width: 903.98px)">.f{display:none}.iv{margin-left:30px}.ru{margin-top:0px}.sj{display:inline-block}.sk{margin-top:5px}.vo{margin-right:16px}</style><style type="text/css" data-fela-rehydration="608" data-fela-type="RULE" media="all and (max-width: 727.98px)">.g{display:none}.u{margin-bottom:20px}.am{box-shadow:inset 0 -1px 0 rgba(230, 230, 230, 1)}.an{min-height:230px}.ar{display:block}.cq{min-height:98px}.cr{display:flex}.cs{align-items:flex-start}.ct{flex-direction:column}.cu{justify-content:flex-end}.cz{margin-bottom:28px}.da{margin-top:0px}.dd{margin-top:28px}.dm{margin:0}.ep{border-top:1px solid rgba(230, 230, 230, 1)}.eq{border-bottom:1px solid rgba(230, 230, 230, 1)}.fb{align-items:center}.fc{flex:1 0 auto}.ic{margin-top:32px}.id{flex-direction:column-reverse}.it{margin-bottom:30px}.iu{margin-left:0px}.pf{padding:10px 12px 10px}.rs{margin-top:2px}.rt{margin-right:16px}.si{display:inline-block}.ub{padding-bottom:12px}.uc{margin-top:16px}.vy{margin-left:16px}.vz{margin-right:0px}</style><style type="text/css" data-fela-rehydration="608" data-fela-type="RULE" media="all and (max-width: 551.98px)">.h{display:none}.au{margin:0 24px}.ey{display:block}.fn{padding:0 8px 24px 8px}.go{font-size:32px}.gp{margin-top:0.64em}.gq{line-height:40px}.gr{letter-spacing:-0.016em}.hl{font-size:18px}.hm{margin-top:0.79em}.hn{line-height:24px}.ib{margin-top:32px}.ir{margin-bottom:30px}.is{margin-left:0px}.jb{margin:0}.jc{max-width:100%}.kb{margin-top:1.56em}.kc{line-height:28px}.kd{letter-spacing:-0.003em}.ky{font-size:22px}.kz{margin-top:1.2em}.la{letter-spacing:0}.lr{margin-top:0.67em}.lx{margin-top:40px}.mo{font-size:20px}.mp{margin-top:1.23em}.nh{margin-top:0.93em}.nv{margin-top:1.34em}.ox{margin-top:24px}.rd{margin-left:8px}.rq{margin-top:2px}.rr{margin-right:16px}.sh{display:inline-block}.td{font-size:16px}.te{line-height:20px}.tf{max-height:40px}.ud{width:calc(100% + 24px)}.ue{margin-left:-12px}.uf{margin-right:-12px}.us{padding-left:12px}.ut{padding-right:12px}.uu{flex-basis:100%}.vw{margin-bottom:0px}.wb{min-width:48px}.wc{min-height:48px}</style><style type="text/css" data-fela-rehydration="608" data-fela-type="RULE" media="all and (min-width: 904px) and (max-width: 1079.98px)">.i{display:none}.x{display:flex}.ax{margin:0 64px}.fq{padding:0 16px}.ha{font-size:46px}.hb{margin-top:0.6em}.hc{line-height:56px}.hd{letter-spacing:-0.011em}.hu{font-size:22px}.hv{margin-top:0.92em}.hw{line-height:28px}.jh{max-width:1192px}.kl{font-size:21px}.km{margin-top:2em}.kn{line-height:32px}.ko{letter-spacing:-0.003em}.li{font-size:30px}.lj{margin-top:1.95em}.lk{line-height:36px}.ll{letter-spacing:0}.lu{margin-top:0.86em}.ma{margin-top:56px}.mt{margin-top:1.72em}.nk{margin-top:1.25em}.ny{margin-top:1.05em}.pa{margin-top:32px}.rg{margin-right:5px}.tm{font-size:20px}.tn{line-height:24px}.to{max-height:48px}.tt{margin:0}.um{width:calc(100% + 32px)}.un{margin-left:-16px}.uo{margin-right:-16px}.vc{padding-left:16px}.vd{padding-right:16px}.ve{flex-basis:25%}.vf{max-width:25%}.vs{font-size:16px}.vt{line-height:20px}.wh{min-width:70px}.wi{min-height:70px}</style><style type="text/css" data-fela-rehydration="608" data-fela-type="RULE" media="all and (min-width: 728px) and (max-width: 903.98px)">.j{display:none}.y{display:flex}.aw{margin:0 48px}.fp{padding:0 16px}.gw{font-size:46px}.gx{margin-top:0.6em}.gy{line-height:56px}.gz{letter-spacing:-0.011em}.hr{font-size:22px}.hs{margin-top:0.92em}.ht{line-height:28px}.jf{margin:0}.jg{max-width:100%}.kh{font-size:21px}.ki{margin-top:2em}.kj{line-height:32px}.kk{letter-spacing:-0.003em}.le{font-size:30px}.lf{margin-top:1.95em}.lg{line-height:36px}.lh{letter-spacing:0}.lt{margin-top:0.86em}.lz{margin-top:56px}.ms{margin-top:1.72em}.nj{margin-top:1.25em}.nx{margin-top:1.05em}.oz{margin-top:32px}.rf{margin-right:5px}.tj{font-size:20px}.tk{line-height:24px}.tl{max-height:48px}.uj{width:calc(100% + 28px)}.uk{margin-left:-14px}.ul{margin-right:-14px}.uy{padding-left:14px}.uz{padding-right:14px}.va{flex-basis:50%}.vb{max-width:50%}.vq{font-size:16px}.vr{line-height:20px}.wf{min-width:48px}.wg{min-height:48px}</style><style type="text/css" data-fela-rehydration="608" data-fela-type="RULE" media="all and (min-width: 552px) and (max-width: 727.98px)">.k{display:none}.av{margin:0 24px}.ez{display:block}.fo{padding:0 8px 24px 8px}.gs{font-size:32px}.gt{margin-top:0.64em}.gu{line-height:40px}.gv{letter-spacing:-0.016em}.ho{font-size:18px}.hp{margin-top:0.79em}.hq{line-height:24px}.jd{margin:0}.je{max-width:100%}.ke{margin-top:1.56em}.kf{line-height:28px}.kg{letter-spacing:-0.003em}.lb{font-size:22px}.lc{margin-top:1.2em}.ld{letter-spacing:0}.ls{margin-top:0.67em}.ly{margin-top:40px}.mq{font-size:20px}.mr{margin-top:1.23em}.ni{margin-top:0.93em}.nw{margin-top:1.34em}.oy{margin-top:24px}.re{margin-left:8px}.tg{font-size:16px}.th{line-height:20px}.ti{max-height:40px}.ug{width:calc(100% + 24px)}.uh{margin-left:-12px}.ui{margin-right:-12px}.uv{padding-left:12px}.uw{padding-right:12px}.ux{flex-basis:100%}.wd{min-width:48px}.we{min-height:48px}</style><style type="text/css" data-fela-rehydration="608" data-fela-type="RULE" media="print">.iy{display:none}</style><style type="text/css" data-fela-rehydration="608" data-fela-type="RULE" media="(prefers-reduced-motion: no-preference)">.ew{animation:k3 .2s ease-in-out both}.jn{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)}.qa{transition:opacity 200ms}</style><style type="text/css" data-fela-rehydration="608" data-fela-type="RULE" media="(orientation: landscape) and (max-width: 903.98px)">.pl{max-height:none}</style><style type="text/css" data-fela-rehydration="608" data-fela-type="RULE" media="all and (max-width: 1230px)">.qb{display:none}</style><style type="text/css" data-fela-rehydration="608" data-fela-type="RULE" media="all and (max-width: 1240px)">.qe{display:none}</style><script data-rh="true">(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-24232453-2', 'auto');
ga('send', 'pageview');</script><script type="text/javascript" data-rh="true">(function(b,r,a,n,c,h,_,s,d,k){if(!b[n]||!b[n]._q){for(;s<_.length;)c(h,_[s++]);d=r.createElement(a);d.async=1;d.src="https://cdn.branch.io/branch-latest.min.js";k=r.getElementsByTagName(a)[0];k.parentNode.insertBefore(d,k);b[n]=h}})(window,document,"script","branch",function(b,r){b[r]=function(){b._q.push([r,arguments])}},{_q:[],_v:1},"addListener applyCode autoAppIndex banner closeBanner closeJourney creditHistory credits data deepview deepviewCta first getCode init link logout redeem referrals removeListener sendSMS setBranchViewData setIdentity track validateCode trackCommerceEvent logEvent".split(" "), 0);
branch.init('key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm', {metadata: {}, 'no_journeys': true, 'disable_exit_animation': true, 'disable_entry_animation': true, 'tracking_disabled': null}, function(err, data) {});</script><link rel="author" href="https://medium.com/@joseph.rocca" data-rh="true"><link rel="canonical" href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73" data-rh="true"><link rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/f70510919f73" data-rh="true"><link rel="icon" href="https://miro.medium.com/fit/c/128/128/1*ChFMdf--f5jbm-AYv6VdYA@2x.png" data-rh="true"><meta property="og:type" content="article" data-rh="true"><meta property="article:published_time" content="2021-03-21T11:38:06.437Z" data-rh="true"><meta name="title" content="Understanding Variational Autoencoders (VAEs) | by Joseph Rocca | Towards Data Science" data-rh="true"><meta property="og:title" content="Understanding Variational Autoencoders (VAEs)" data-rh="true"><meta property="twitter:title" content="Understanding Variational Autoencoders (VAEs)" data-rh="true"><meta name="twitter:site" content="@TDataScience" data-rh="true"><meta name="twitter:app:url:iphone" content="medium://p/f70510919f73" data-rh="true"><meta property="al:android:url" content="medium://p/f70510919f73" data-rh="true"><meta property="al:ios:url" content="medium://p/f70510919f73" data-rh="true"><meta name="description" content="In the last few years, deep learning based generative models have gained more and more interest due to (and implying) some amazing improvements in the field. Relying on huge amount of data…" data-rh="true"><meta property="og:description" content="Building, step by step, the reasoning that leads to VAEs." data-rh="true"><meta property="twitter:description" content="Building, step by step, the reasoning that leads to VAEs." data-rh="true"><meta property="og:url" content="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73" data-rh="true"><meta property="al:web:url" content="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73" data-rh="true"><meta property="og:image" content="https://miro.medium.com/max/1200/1*mbCY2_LZX2bpGX7CH80FAg.jpeg" data-rh="true"><meta name="twitter:image:src" content="https://miro.medium.com/max/1200/1*mbCY2_LZX2bpGX7CH80FAg.jpeg" data-rh="true"><meta name="twitter:card" content="summary_large_image" data-rh="true"><meta property="article:author" content="https://medium.com/@joseph.rocca" data-rh="true"><meta name="twitter:creator" content="@roccajo" data-rh="true"><meta name="author" content="Joseph Rocca" data-rh="true"><meta name="robots" content="index,follow,max-image-preview:large" data-rh="true"><meta name="referrer" content="unsafe-url" data-rh="true"><meta name="twitter:label1" content="Reading time" data-rh="true"><meta name="twitter:data1" content="23 min read" data-rh="true"><script type="application/ld+json" data-rh="true">{"@context":"http:\u002F\u002Fschema.org","@type":"NewsArticle","image":["https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F1200\u002F1*mbCY2_LZX2bpGX7CH80FAg.jpeg"],"url":"https:\u002F\u002Ftowardsdatascience.com\u002Funderstanding-variational-autoencoders-vaes-f70510919f73","dateCreated":"2019-09-24T01:19:04.756Z","datePublished":"2019-09-24T01:19:04.756Z","dateModified":"2021-12-11T23:48:06.305Z","headline":"Understanding Variational Autoencoders (VAEs) - Towards Data Science","name":"Understanding Variational Autoencoders (VAEs) - Towards Data Science","description":"In the last few years, deep learning based generative models have gained more and more interest due to (and implying) some amazing improvements in the field. Relying on huge amount of data…","identifier":"f70510919f73","author":{"@type":"Person","name":"Joseph Rocca","url":"https:\u002F\u002Ftowardsdatascience.com\u002F@joseph.rocca"},"creator":["Joseph Rocca"],"publisher":{"@type":"Organization","name":"Towards Data Science","url":"towardsdatascience.com","logo":{"@type":"ImageObject","width":165,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F165\u002F1*mG6i4Bh_LgixUYXJgQpYsg@2x.png"}},"mainEntityOfPage":"https:\u002F\u002Ftowardsdatascience.com\u002Funderstanding-variational-autoencoders-vaes-f70510919f73"}</script></head><body><div id="root"><div class="a b c"><div class="d e f g h i j k"></div><script>document.domain = document.domain;</script><div><script>if (window.self !== window.top) window.location = "about:blank"</script></div><div class="s"><div class="t s u"><div class="ak al s am an"><div class="n ao ap"><div class="aq ar"><div class="as s at ac"><div class="n p"><div class="au av aw ax ay az ba v"><div class="bb n o"><div class="n o bc bd"><div class="ag"><div class="mm" id="li-ShowPostUnderCollection-navbar-open-in-app-button"><div class="cc aq ar"><span class="bf b bg bh cd"><a href="https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff70510919f73&amp;%7Efeature=LiOpenInAppButton&amp;%7Echannel=ShowPostUnderCollection&amp;%7Estage=mobileNavBar&amp;source=post_page-----f70510919f73-----------------------------------" class="bi bl ce cf cg ch ci cj ck bq bn bo cl cm cn" rel="noopener follow">Open in app</a></span></div></div></div></div><a href="https://medium.com/?source=post_page-----f70510919f73-----------------------------------" aria-label="Homepage" rel="noopener follow"><svg viewBox="0 0 1043.63 592.71" class="q bl"><g data-name="Layer 2"><g data-name="Layer 1"><path d="M588.67 296.36c0 163.67-131.78 296.35-294.33 296.35S0 460 0 296.36 131.78 0 294.34 0s294.33 132.69 294.33 296.36M911.56 296.36c0 154.06-65.89 279-147.17 279s-147.17-124.94-147.17-279 65.88-279 147.16-279 147.17 124.9 147.17 279M1043.63 296.36c0 138-23.17 249.94-51.76 249.94s-51.75-111.91-51.75-249.94 23.17-249.94 51.75-249.94 51.76 111.9 51.76 249.94"></path></g></g></svg></a></div></div></div></div></div><div class="n p"><div class="au av aw ax ay az ba v"><div class="co n o bc cp cq cr cs ct cu"><div class="v n cv cp"><div class="n v"><div class="cw cx v n o bc cy cz da cr cs ct"><div class="db dc s dd"><a aria-label="Publication Homepage" rel="noopener follow" href="https://towardsdatascience.com/?source=post_page-----f70510919f73-----------------------------------"><div class="de df s"><img alt="Towards Data Science" class="" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1AGyTPCaRzVqL77kFwUwHKg.png" width="112" height="35"></div></a></div></div></div><div class="w x y k h z ab o ac mm"><div class="dy abb abc n o"><div class="abd abe"><div class="n" aria-hidden="false" aria-describedby="publisherMenu" aria-labelledby="publisherMenu"><button aria-controls="publisherMenu" aria-expanded="false" aria-label="Publisher Menu" class="ea xw ce cf cg ch ci cj ck bq"><div class="abg s" aria-hidden="true"><svg class="abf bo" width="25" height="25"><path d="M5 12.5c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41A1.93 1.93 0 0 0 7 10.5c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41zm5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59-.39.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59.56 0 1.03-.2 1.42-.59.39-.39.58-.86.58-1.41 0-.55-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59-.39.39-.58.86-.58 1.41z" fill-rule="evenodd"></path></svg></div></button></div></div><div class="in abh s"><div class="abd abe"><div class="ca" aria-hidden="false" aria-describedby="searchResults" aria-labelledby="searchResults"><div class="n"><button class="ea eb ce cf cg ch ci cj ck bq abp abq cl abr abs" aria-label="Search"><span class="abg s"><svg width="25" height="25" viewBox="0 0 25 25" class="abf"><path d="M20.07 18.93l-4.16-4.15a6 6 0 1 0-.88.88l4.15 4.16a.62.62 0 1 0 .89-.89zM6.5 11a4.75 4.75 0 1 1 9.5 0 4.75 4.75 0 0 1-9.5 0z"></path></svg></span></button><input role="combobox" aria-controls="searchResults" aria-expanded="false" aria-hidden="true" aria-label="search" tabindex="-1" class="abi abj abk dn bh es abl bi abm at em abn abo" placeholder="Search"></div></div></div></div><div class="mm" id="li-post-page-navbar-upsell-button"><div class="dy s g"><div><a href="https://medium.com/plans?source=upgrade_membership---nav_full-------------------------------------" class="bf b bg bh bi abt bl bk bm bn bo bp bq br bs bt bu bv bw bx by bz ca cb" rel="noopener follow">Upgrade</a></div></div></div></div><a href="https://medium.com/?source=post_page-----f70510919f73-----------------------------------" aria-label="Homepage" rel="noopener follow"><svg viewBox="0 0 1043.63 592.71" class="q dj"><g data-name="Layer 2"><g data-name="Layer 1"><path d="M588.67 296.36c0 163.67-131.78 296.35-294.33 296.35S0 460 0 296.36 131.78 0 294.34 0s294.33 132.69 294.33 296.36M911.56 296.36c0 154.06-65.89 279-147.17 279s-147.17-124.94-147.17-279 65.88-279 147.16-279 147.17 124.9 147.17 279M1043.63 296.36c0 138-23.17 249.94-51.76 249.94s-51.75-111.91-51.75-249.94 23.17-249.94 51.75-249.94 51.76 111.9 51.76 249.94"></path></g></g></svg></a></div></div></div></div></div></div><div class="s"><div class="n p"><div class="au av aw ax ay az ba v"><div class="af ag ah wl aj"><div class="dk dl n o"><div class="s dm"><span class="bf b dn do dp dq dr ds dt du dv dw dx"><div class="n o"><div class="abz s"><div class="abu abv s"><div class="ca" aria-hidden="false" aria-describedby="collectionFollowPopover" aria-labelledby="collectionFollowPopover"><button class="bf b bg bh qj bj qk ql ff qm fi bq br bs qn fl bw bx by bz ca cb" aria-controls="collectionFollowPopover" aria-expanded="false"><div class="n bc">Follow</div></button></div></div></div><div class="dy dz ao"><span class="bf b dn do dx"><a class="ea eb ce cf cg ch ci cj ck bq ec ed cl ee ef" rel="noopener follow" href="https://towardsdatascience.com/followers?source=post_page-----f70510919f73-----------------------------------">605K Followers</a></span></div><div class="eg s g">·</div><div class="eg s g"><nav class="n o"><span class="eh n ao"><a class="ea eb ce cf cg ch ci cj ck bq ec ed cl ee ef" rel="noopener follow" href="https://towardsdatascience.com/tagged/editors-pick?source=post_page-----f70510919f73-----------------------------------">Editors' Picks</a></span><span class="eh n ao"><a class="ea eb ce cf cg ch ci cj ck bq ec ed cl ee ef" rel="noopener follow" href="https://towardsdatascience.com/tagged/tds-features?source=post_page-----f70510919f73-----------------------------------">Features</a></span><span class="eh n ao"><a class="ea eb ce cf cg ch ci cj ck bq ec ed cl ee ef" rel="noopener follow" href="https://towardsdatascience.com/tagged/deep-dives?source=post_page-----f70510919f73-----------------------------------">Deep Dives</a></span><span class="eh n ao"><a class="ea eb ce cf cg ch ci cj ck bq ec ed cl ee ef" rel="noopener follow" href="https://towardsdatascience.com/how-to-get-the-most-out-of-towards-data-science-3bf37f75a345?source=post_page-----f70510919f73-----------------------------------">Grow</a></span><span class="eh n ao"><a class="ea eb ce cf cg ch ci cj ck bq ec ed cl ee ef" rel="noopener follow" href="https://towardsdatascience.com/questions-96667b06af5?source=post_page-----f70510919f73-----------------------------------">Contribute</a></span></nav></div><div class="s h"></div><div class="eg n ao g"><a class="ea eb ce cf cg ch ci cj ck bq ec ed cl ee ef" rel="noopener follow" href="https://towardsdatascience.com/about?source=post_page-----f70510919f73-----------------------------------">About</a></div></div></span></div><div class="aq ei ej ar"><button class="n o p ek el em en" aria-label="Expand navbar"><svg width="14" height="14" class="eo"><path d="M0 .5h14M0 7h14M0 13.5h14"></path></svg></button></div></div></div></div></div></div></div><div class="ep eq c er sa et eu ev mm ac acz"><div class="n p"><div class="au av aw ax ay az ba v"><div class="ex v ey ez j i d ev ac"><div class="fa n o"><div class="aq cr fb fc"><div class="mm" id="li-ShowPostUnderCollection-navbar-open-in-app-button"><div class="cc aq ar"><span class="bf b bg bh dx"><a href="https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff70510919f73&amp;%7Efeature=LiOpenInAppButton&amp;%7Echannel=ShowPostUnderCollection&amp;%7Estage=mobileNavBar&amp;source=post_page-----f70510919f73-----------------------------------" class="fd fe ce cf cg ch ci cj ck bq fg fh cl cm cn" rel="noopener follow">Open in app</a></span></div></div></div><a href="https://medium.com/?source=post_page-----f70510919f73-----------------------------------" aria-label="Homepage" rel="noopener follow"><svg viewBox="0 0 1043.63 592.71" class="q r"><g data-name="Layer 2"><g data-name="Layer 1"><path d="M588.67 296.36c0 163.67-131.78 296.35-294.33 296.35S0 460 0 296.36 131.78 0 294.34 0s294.33 132.69 294.33 296.36M911.56 296.36c0 154.06-65.89 279-147.17 279s-147.17-124.94-147.17-279 65.88-279 147.16-279 147.17 124.9 147.17 279M1043.63 296.36c0 138-23.17 249.94-51.76 249.94s-51.75-111.91-51.75-249.94 23.17-249.94 51.75-249.94 51.76 111.9 51.76 249.94"></path></g></g></svg></a></div></div></div></div></div></div><div class="iy" role="dialog" aria-modal="true" tabindex="-1"><div class="wy wz v fa et xa xb en es ge xc" aria-hidden="true" role="presentation"></div><div class="xd et xe xf xg wy fa bz xh xi xj sa xk xl ae xm xn xo xp xq xr" aria-hidden="true"><div class="xs xt n o bc cp"><div class="n bc"><h2 class="bf kx xu do gl hj">Responses (71)</h2></div><div class="n bc"><div><div class="ca" role="tooltip" aria-hidden="false" aria-describedby="236" aria-labelledby="236"><a href="https://policy.medium.com/medium-rules-30e5502c4eb4?source=responses-----f70510919f73-----------------------------------" class="rz ed" rel="noopener follow" target="_blank"><svg width="25" height="25" viewBox="0 0 25 25"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.99 5.04c.26-.21.64-.22.91-.01.97.72 1.77 1.21 2.6 1.54.83.32 1.72.48 2.89.5.41.01.74.35.74.76-.02 3.62-.43 6.26-1.45 8.21-1.03 1.98-2.66 3.21-4.97 4.08a.75.75 0 0 1-.53 0c-2.25-.87-3.86-2.1-4.9-4.07-1.02-1.95-1.46-4.59-1.48-8.22 0-.41.33-.75.75-.76 1.19-.02 2.1-.18 2.92-.5.82-.32 1.6-.81 2.52-1.53zm.46.9c-.9.69-1.71 1.21-2.62 1.56a8.9 8.9 0 0 1-3.02.57c.03 3.45.46 5.82 1.36 7.51.88 1.69 2.25 2.77 4.28 3.57 2.1-.8 3.47-1.89 4.34-3.57.89-1.7 1.3-4.07 1.34-7.51a8.8 8.8 0 0 1-3-.57 11.8 11.8 0 0 1-2.68-1.56zm0 9.15a2.67 2.67 0 1 0 0-5.34 2.67 2.67 0 0 0 0 5.34zm0 1a3.67 3.67 0 1 0 0-7.34 3.67 3.67 0 0 0 0 7.34zm-1.82-3.77l.53-.53.91.92 1.63-1.63.52.53-2.15 2.15-1.44-1.44z"></path></svg></a></div></div><div class="s at xv"><div class="s at ev eu"><button class="ea eb ce cf cg ch ci cj ck bq ec ed cl ee ef" data-testid="close-button" aria-label="close"><svg width="25" height="25" viewBox="0 0 25 25" class="xw"><path d="M18.13 6.11l-5.61 5.61-5.6-5.61-.81.8 5.61 5.61-5.61 5.61.8.8 5.61-5.6 5.61 5.6.8-.8-5.6-5.6 5.6-5.62"></path></svg></button></div></div></div></div><div class="xx ju n ao p o mz xy"><p class="bf b dn do dx">There are currently no responses for this story.</p><p class="bf b dn do dx">Be the first to respond.</p></div></div></div><article><section class="fn fo fp fq fr fs ft v fu bz s"></section><span class="s"></span><div><div><div class="ei er wo gc gd ge"></div><section class="gf gg gh du gi"><div class="n p"><div class="au av aw ax ay gj ba v"><div class=""><h1 id="a1e7" class="gk gl gm bf gn go gp gq gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj" data-selectable-paragraph="">Understanding Variational Autoencoders (VAEs)</h1></div><div class=""><h2 id="9db8" class="hk gl gm bf b hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia dx" data-selectable-paragraph="">Building, step by step, the reasoning that leads to VAEs.</h2><div class="db"><div class="n cp ib ic id"><div class="o n"><div><a href="https://medium.com/@joseph.rocca?source=post_page-----f70510919f73-----------------------------------" rel="noopener follow"><div class="at ie if"><div class="ig n bc o p ei ih ii ij ik il ge"><svg width="36" height="36" viewBox="0 0 36 36"><path fill-rule="evenodd" clip-rule="evenodd" d="M18 1.87c-6.63 0-12.4 4.14-15.21 10.21L2 11.71C4.94 5.37 11 1 18 1s13.06 4.37 16 10.71l-.79.37C30.4 6.01 24.63 1.88 18 1.88zM2.79 23.92c2.81 6.07 8.58 10.2 15.21 10.2 6.63 0 12.4-4.13 15.21-10.2l.79.37C31.06 30.63 25 35 18 35S4.94 30.63 2 24.29l.79-.37z"></path></svg></div><img alt="Joseph Rocca" class="s im if ie" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1Ede150EkH-LDgY-m-SrmLg.png" width="28" height="28"></div></a></div><div class="in v n cy"><div class="n"><div style="flex: 1 1 0%;"><span class="bf b bg bh hj"><div><div class="ca" role="tooltip" aria-hidden="false" aria-describedby="237" aria-labelledby="237"><a href="https://medium.com/@joseph.rocca?source=post_page-----f70510919f73-----------------------------------" class="" rel="noopener follow"><p class="bf b bg bh fd">Joseph Rocca</p></a></div></div></span></div></div><span class="bf b bg bh dx"><a class="" rel="noopener follow" href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73?source=post_page-----f70510919f73-----------------------------------"><p class="bf b bg bh dx"><span class="io"></span><span>Sep 24, 2019</span><span class="ip">·</span>23 min read</p></a></span></div></div><div class="n iq ir is it iu iv iw ix iy"><div class="n o"><div class="yb s z"><div><div class="ca" role="tooltip" aria-hidden="false" aria-describedby="216" aria-labelledby="216"><button class="ea eb ce cf cg ch ci cj ck bq ec ed cl ee ef" aria-label="Share on twitter"><svg width="30" height="30" viewBox="0 0 30 30" fill="none" class="xw yc"><path fill-rule="evenodd" clip-rule="evenodd" d="M15 27a12 12 0 1 0 0-24 12 12 0 0 0 0 24zm4.95-16.17a2.67 2.67 0 0 0-4.6 1.84c0 .2.03.41.05.62a7.6 7.6 0 0 1-5.49-2.82 3 3 0 0 0-.38 1.34c.02.94.49 1.76 1.2 2.23a2.53 2.53 0 0 1-1.2-.33v.04c0 1.28.92 2.36 2.14 2.62-.23.05-.46.08-.71.1l-.21-.02-.27-.03a2.68 2.68 0 0 0 2.48 1.86A5.64 5.64 0 0 1 9 19.38a7.62 7.62 0 0 0 4.1 1.19c4.9 0 7.58-4.07 7.57-7.58v-.39c.52-.36.97-.83 1.33-1.38-.48.23-1 .37-1.53.43.56-.33.96-.86 1.15-1.48-.5.31-1.07.53-1.67.66z" fill="#292929"></path></svg></button></div></div></div><div class="yb s z"><div><div class="ca" role="tooltip" aria-hidden="false" aria-describedby="217" aria-labelledby="217"><button class="ea eb ce cf cg ch ci cj ck bq ec ed cl ee ef" aria-label="Share on facebook"><svg width="30" height="30" viewBox="0 0 30 30" fill="none" class="xw yc"><path fill-rule="evenodd" clip-rule="evenodd" d="M15 27a12 12 0 1 0 0-24 12 12 0 0 0 0 24zm-1.23-6.03V15.6H12v-2.15h1.77v-1.6C13.77 10 14.85 9 16.42 9c.75 0 1.4.06 1.58.08v1.93h-1.09c-.85 0-1.02.43-1.02 1.05v1.38h2.04l-.27 2.15H15.9V21l-2.13-.03z" fill="#292929"></path></svg></button></div></div></div><div class="yb s z"><div><div class="ca" role="tooltip" aria-hidden="false" aria-describedby="218" aria-labelledby="218"><button class="ea eb ce cf cg ch ci cj ck bq ec ed cl ee ef" aria-label="Share on linkedin"><svg width="30" height="30" viewBox="0 0 30 30" fill="none" class="xw yc"><path fill-rule="evenodd" clip-rule="evenodd" d="M27 15a12 12 0 1 1-24 0 12 12 0 0 1 24 0zm-14.61 5v-7.42h-2.26V20h2.26zm-1.13-8.44c.79 0 1.28-.57 1.28-1.28-.02-.73-.5-1.28-1.26-1.28-.78 0-1.28.55-1.28 1.28 0 .71.49 1.28 1.25 1.28h.01zM15.88 20h-2.5s.04-6.5 0-7.17h2.5v1.02l-.02.02h.02v-.02a2.5 2.5 0 0 1 2.25-1.18c1.64 0 2.87 1.02 2.87 3.22V20h-2.5v-3.83c0-.97-.36-1.62-1.26-1.62-.69 0-1.1.44-1.28.87-.06.15-.08.36-.08.58v4z" fill="#292929"></path></svg></button></div></div></div><div class="s z"><div><div class="ca" role="tooltip" aria-hidden="false" aria-describedby="219" aria-labelledby="219"><button class="ea eb ce cf cg ch ci cj ck bq ec ed cl ee ef"><svg width="30" height="30" viewBox="0 0 30 30" fill="none" class="xw yc"><path fill-rule="evenodd" clip-rule="evenodd" d="M15 27a12 12 0 1 0 0-24 12 12 0 0 0 0 24zM9.29 16.28c-.2.36-.29.75-.29 1.17a2.57 2.57 0 0 0 .78 1.84l1.01.96c.53.5 1.17.75 1.92.75s1.38-.25 1.9-.75l1.2-1.15.75-.71.51-.5a2.51 2.51 0 0 0 .72-2.34.7.7 0 0 0-.03-.18 2.74 2.74 0 0 0-.23-.5v-.02l-.08-.14-.02-.03-.02-.01a.33.33 0 0 0-.07-.1c0-.02-.01-.03-.03-.05a.2.2 0 0 0-.03-.03l-.03-.04v-.01l-.02-.03-.04-.03a.85.85 0 0 1-.13-.13l-.43-.42-.06.06-.9.84-.05.09a.26.26 0 0 0-.03.1l.37.38c.04.03.08.07.1.11l.01.01.01.03.02.01.04.1.03.04.06.1v.02l.01.02c.03.1.05.2.05.33a1 1 0 0 1-.12.49c-.07.13-.15.22-.22.29l-.88.85-.61.57-.95.92c-.22.2-.5.3-.82.3-.31 0-.58-.1-.8-.3l-.98-.96a1.15 1.15 0 0 1-.3-.42 1.4 1.4 0 0 1-.04-.35c0-.1.01-.2.04-.3a1 1 0 0 1 .3-.49l1.5-1.46v-.24c0-.21 0-.42.04-.6a3.5 3.5 0 0 1 .92-1.72c-.41.1-.78.32-1.11.62l-.01.02-.01.01-2.46 2.33c-.2.21-.35.4-.44.6h-.02c0 .02 0 .02-.02.02v.02l-.01.01zm3.92-1.8a1.83 1.83 0 0 0 .02.97c0 .06 0 .13.02.19.06.17.14.34.22.5v.02l.06.12.02.03.01.02.08.1c0 .02.02.03.04.05l.08.1h.01c0 .01 0 .03.02.03l.14.14.43.41.08-.06.88-.84.05-.09.03-.1-.36-.37a.4.4 0 0 1-.12-.13v-.02l-.02-.02-.05-.09-.04-.04-.04-.1v-.02l-.02-.02a1.16 1.16 0 0 1 .06-.82c.09-.14.16-.24.23-.3l.9-.85.6-.58.93-.92c.23-.2.5-.3.82-.3a1.2 1.2 0 0 1 .82.3l1 .96c.13.15.23.29.28.42a1.43 1.43 0 0 1 0 .66c-.03.17-.12.33-.26.48l-1.54 1.45.02.25a3.28 3.28 0 0 1-.96 2.32 2.5 2.5 0 0 0 1.1-.62l.01-.01 2.46-2.34c.19-.2.35-.4.46-.6l.02-.02v-.02h.01a2.45 2.45 0 0 0 .21-1.82 2.53 2.53 0 0 0-.7-1.19l-1-.96a2.68 2.68 0 0 0-1.91-.75c-.75 0-1.38.25-1.9.76l-1.2 1.14-.76.72-.5.49c-.4.37-.64.83-.74 1.37z" fill="#292929"></path></svg></button></div></div></div><div class="iz s"><div><div class="ca" role="tooltip" aria-hidden="false" aria-describedby="220" aria-labelledby="220"><div><div class="ca" role="tooltip" aria-hidden="false" aria-describedby="221" aria-labelledby="221"><div class="ca v" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="ea xw ce cf cg ch ci cj ck bq"><svg width="25" height="25" viewBox="0 0 25 25" fill="none" class="abw"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="#292929"></path></svg></button></div></div></div></div></div></div><div class="s"><div class="ca" aria-hidden="false"><button class="ea eb ce cf cg ch ci cj ck bq ec ed cl ee ef" aria-label="More options"><svg class="overflow-dots-filled-25px_svg__svgIcon-use" width="25" height="25"><path d="M5 12.5c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41A1.93 1.93 0 0 0 7 10.5c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41zm5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59-.39.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59.56 0 1.03-.2 1.42-.59.39-.39.58-.86.58-1.41 0-.55-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59-.39.39-.58.86-.58 1.41z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div><div class="ja"><div class="n p"><div class="jb jc jd je jf jg ax jh ay ji ba v"><figure class="jk ja jl jm paragraph-image"><div role="button" tabindex="0" class="jn jo at jp v jq"><div class="fs ft jj"><img alt="" class="v jr js" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1mbCY2_LZX2bpGX7CH80FAg.jpeg" role="presentation" width="1000" height="375"></div></div><figcaption class="jt ju fu fs ft jv jw bf b bg bh dx" data-selectable-paragraph="">Credit: <a href="https://pixabay.com/fr/users/free-photos-242387/" class="ea jx" rel="noopener ugc nofollow" target="_blank">Free-Photos</a> on <a href="https://pixabay.com/" class="ea jx" rel="noopener ugc nofollow" target="_blank">Pixabay</a></figcaption></figure></div></div></div><div class="n p"><div class="au av aw ax ay gj ba v"><p id="c1a6" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">This post was co-written with <div class="ca yd"><div><div class="ca" role="tooltip" aria-hidden="false" aria-describedby="222" aria-labelledby="222"><a href="https://medium.com/u/20ad1309823a?source=post_page-----f70510919f73-----------------------------------" class="ku fd cb" rel="noopener" target="_blank">Baptiste Rocca</a></div></div></div>.</p><h1 id="a205" class="kv kw gm bf kx ky kz kc la lb lc kf ld le lf lg lh li lj lk ll lm ln lo lp lq hj" data-selectable-paragraph="">Introduction</h1><p id="8956" class="jy jz gm ka b hl lr kc kd ho ls kf kg kh lt kj kk kl lu kn ko kp lv kr ks kt gf hj" data-selectable-paragraph="">In
 the last few years, deep learning based generative models have gained 
more and more interest due to (and implying) some amazing improvements 
in the field. Relying on huge amount of data, well-designed networks 
architectures and smart training techniques, deep generative models have
 shown an incredible ability to produce highly realistic pieces of 
content of various kind, such as images, texts and sounds. Among these 
deep generative models, two major families stand out and deserve a 
special attention: Generative Adversarial Networks (GANs) and 
Variational Autoencoders (VAEs).</p><figure class="lx ly lz ma mb ja fs ft paragraph-image"><div role="button" tabindex="0" class="jn jo at jp v jq"><div class="fs ft lw"><div class="mg s at mh"><div class="mi mj s"><div class="es mc ei ev er fa v md me mf"><img alt="" class="ei ev er fa v mk ml ae xz" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1BaZPg3SRgZGVigguQCmirA.png" role="presentation" width="700" height="375"></div><img alt="" class="sa wx ei ev er fa v c" role="presentation" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1BaZPg3SRgZGVigguQCmirA_002.png" srcset="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1BaZPg3SRgZGVigguQCmirA_003.png 276w, Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1BaZPg3SRgZGVigguQCmirA_004.png 552w, Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1BaZPg3SRgZGVigguQCmirA_005.png 640w, Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1BaZPg3SRgZGVigguQCmirA_002.png 700w" sizes="700px" width="700" height="375"><noscript></noscript></div></div></div></div><figcaption class="jt ju fu fs ft jv jw bf b bg bh dx" data-selectable-paragraph="">Face images generated with a Variational Autoencoder (source: <a href="https://github.com/WojciechMormul/vae" class="ea jx" rel="noopener ugc nofollow" target="_blank">Wojciech Mormul on Github</a>).</figcaption></figure><p id="f1b2" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">In a pr<span id="rmm"><span id="rmm"><span id="rmm">e</span></span></span>vious post, published in January of this year, we discussed in depth <a class="ea jx" rel="noopener" target="_blank" href="https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29">Generative Adversarial Networks (GANs)</a>
 and showed, in particular, how adversarial training can oppose two 
networks, a generator and a discriminator, to push both of them to 
improve iteration after iteration. We introduce now, in this post, the 
other major kind of deep generative models: Variational Autoencoders 
(VAEs). In a nutshell, a VAE is an autoencoder whose encodings 
distribution is regularised during the training in order to ensure that 
its latent space has good properties allowing us to generate some new 
data. Moreover, the term “variational” comes from the close relation 
there is between the regularisation and the variational inference method
 in statistics.</p><p id="e777" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">If
 the last two sentences summarise pretty well the notion of VAEs, they 
can also raise a lot of questions. What is an autoencoder? What is the 
latent space and why regularising it? How to generate new data from 
VAEs? What is the link between VAEs and variational inference? In order 
to describe VAEs as well as possible, we will try to answer all this 
questions (and many others!) and to provide the reader with as much 
insights as we can (ranging from basic intuitions to more advanced 
mathematical details). Thus, the purpose of this post is not only to 
discuss the fundamental notions Variational Autoencoders rely on but 
also to build step by step and starting from the very beginning the 
reasoning that leads to these notions.</p><p id="ddf4" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">Without further ado, let’s (re)discover VAEs together!</p><h2 id="f01f" class="mn kw gm bf kx mo mp hn la mq mr hq ld hr ms ht lh hu mt hw ll hx mu hz lp mv hj" data-selectable-paragraph="">Outline</h2><p id="f7d4" class="jy jz gm ka b hl lr kc kd ho ls kf kg kh lt kj kk kl lu kn ko kp lv kr ks kt gf hj" data-selectable-paragraph="">In
 the first section, we will review some important notions about 
dimensionality reduction and autoencoder that will be useful for the 
understanding of VAEs. Then, in the second section, we will show why 
autoencoders cannot be used to generate new data and will introduce 
Variational Autoencoders that are regularised versions of autoencoders 
making the generative process possible. Finally in the last section we 
will give a more mathematical presentation of VAEs, based on variational
 inference.</p><blockquote class="mw mx my"><p id="1b2d" class="jy jz mz ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph=""><strong class="ka gn">Note.</strong>
 In the last section we have tried to make the mathematical derivation 
as complete and clear as possible to bridge the gap between intuitions 
and equations. However, the readers that doesn’t want to dive into the 
mathematical details of VAEs can skip this section without hurting the 
understanding of the main concepts. Notice also that in this post we 
will make the following abuse of notation: for a random variable z, we 
will denote p(z) the distribution (or the density, depending on the 
context) of this random variable.</p></blockquote></div></div></section><div class="n p db na nb nc" role="separator"><span class="nd im ca ne nf ng"></span><span class="nd im ca ne nf ng"></span><span class="nd im ca ne nf"></span></div><section class="gf gg gh du gi"><div class="n p"><div class="au av aw ax ay gj ba v"><h1 id="91ed" class="kv kw gm bf kx ky nh kc la lb ni kf ld le nj lg lh li nk lk ll lm nl lo lp lq hj" data-selectable-paragraph="">Dimensionality reduction, PCA and autoencoders</h1><p id="196d" class="jy jz gm ka b hl lr kc kd ho ls kf kg kh lt kj kk kl lu kn ko kp lv kr ks kt gf hj" data-selectable-paragraph="">In
 this first section we will start by discussing some notions related to 
dimensionality reduction. In particular, we will review briefly 
principal component analysis (PCA) and autoencoders, showing how both 
ideas are related to each others.</p><h2 id="2176" class="mn kw gm bf kx mo mp hn la mq mr hq ld hr ms ht lh hu mt hw ll hx mu hz lp mv hj" data-selectable-paragraph="">What is dimensionality reduction?</h2><p id="a5fc" class="jy jz gm ka b hl lr kc kd ho ls kf kg kh lt kj kk kl lu kn ko kp lv kr ks kt gf hj" data-selectable-paragraph="">In machine learning, <a href="https://en.wikipedia.org/wiki/Dimensionality_reduction" class="ea jx" rel="noopener ugc nofollow" target="_blank"><strong class="ka gn">dimensionality reduction</strong></a><strong class="ka gn"> is the process of reducing the number of features that describe some data</strong>.
 This reduction is done either by selection (only some existing features
 are conserved) or by extraction (a reduced number of new features are 
created based on the old features) and can be useful in many situations 
that require low dimensional data (data visualisation, data storage, 
heavy computation…). Although there exists many different methods of 
dimensionality reduction, we can set a global framework that is matched 
by most (if not any!) of these methods.</p><p id="005f" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">First, let’s call <strong class="ka gn">encoder</strong>
 the process that produce the “new features” representation from the 
“old features” representation (by selection or by extraction) and <strong class="ka gn">decoder</strong>
 the reverse process. Dimensionality reduction can then be interpreted 
as data compression where the encoder compress the data (from the 
initial space to the <strong class="ka gn">encoded space</strong>, also called <strong class="ka gn">latent space</strong>)
 whereas the decoder decompress them. Of course, depending on the 
initial data distribution, the latent space dimension and the encoder 
definition, this compression can be lossy, meaning that a part of the 
information is lost during the encoding process and cannot be recovered 
when decoding.</p></div></div><div class="ja"><div class="n p"><div class="jb jc jd je jf jg ax jh ay ji ba v"><figure class="lx ly lz ma mb ja jl jm paragraph-image"><div role="button" tabindex="0" class="jn jo at jp v jq"><div class="fs ft az"><img alt="" class="v jr js" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1UdOybs9wOe3zW8vDAfj9VA2x.png" role="presentation" width="1000" height="421"></div></div><figcaption class="jt ju fu fs ft jv jw bf b bg bh dx" data-selectable-paragraph="">Illustration of the dimensionality reduction principle with encoder and decoder.</figcaption></figure></div></div></div><div class="n p"><div class="au av aw ax ay gj ba v"><p id="07ca" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">The
 main purpose of a dimensionality reduction method is to find the best 
encoder/decoder pair among a given family. In other words, for a given 
set of possible encoders and decoders, we are looking for the pair that <strong class="ka gn">keeps the maximum of information when encoding</strong> and, so, <strong class="ka gn">has the minimum of reconstruction error when decoding</strong>.
 If we denote respectively E and D the families of encoders and decoders
 we are considering, then the dimensionality reduction problem can be 
written</p><figure class="lx ly lz ma mb ja fs ft paragraph-image"><div class="fs ft nm"><img alt="" class="v jr js" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/19_DFaRan_hX9xMZldVGNjg2x.png" role="presentation" width="548" height="74"></div></figure><p id="c7e2" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">where</p><figure class="lx ly lz ma mb ja fs ft paragraph-image"><div class="fs ft nn"><div class="mg s at mh"><div class="no mj s"><div class="es mc ei ev er fa v md me mf"><img alt="" class="ei ev er fa v mk ml ae xz" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1wau5Kuv0xNL8XRjlt8PfaA2x.png" role="presentation" width="208" height="42"></div><img alt="" class="sa wx ei ev er fa v c" role="presentation" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1wau5Kuv0xNL8XRjlt8PfaA2x_002.png" srcset="" sizes="208px" width="208" height="42"><noscript></noscript></div></div></div></figure><p id="b89a" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">defines
 the reconstruction error measure between the input data x and the 
encoded-decoded data d(e(x)). Notice finally that in the following we 
will denote N the number of data, n_d the dimension of the initial 
(decoded) space and n_e the dimension of the reduced (encoded) space.</p><h2 id="0e8a" class="mn kw gm bf kx mo mp hn la mq mr hq ld hr ms ht lh hu mt hw ll hx mu hz lp mv hj" data-selectable-paragraph="">Principal components analysis (PCA)</h2><p id="d9aa" class="jy jz gm ka b hl lr kc kd ho ls kf kg kh lt kj kk kl lu kn ko kp lv kr ks kt gf hj" data-selectable-paragraph="">One of the first methods that come in mind when speaking about dimensionality reduction is <a href="https://en.wikipedia.org/wiki/Principal_component_analysis" class="ea jx" rel="noopener ugc nofollow" target="_blank"><strong class="ka gn">principal component analysis (PCA)</strong></a>.
 In order to show how it fits the framework we just described and make 
the link towards autoencoders, let’s give a very high overview of how 
PCA works, letting most of the details aside <em class="mz">(notice that we plan to write a full post on the subject)</em>.</p><p id="fe0e" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">The idea of PCA is to build n_e new <strong class="ka gn">independent</strong> features that are <strong class="ka gn">linear combinations</strong>
 of the n_d old features and so that the projections of the data on the 
subspace defined by these new features are as close as possible to the 
initial data (in term of euclidean distance). In other words, PCA is 
looking for the best linear subspace of the initial space (described by 
an orthogonal basis of new features) such that the error of 
approximating the data by their projections on this subspace is as small
 as possible.</p></div></div><div class="ja"><div class="n p"><div class="jb jc jd je jf jg ax jh ay ji ba v"><figure class="lx ly lz ma mb ja jl jm paragraph-image"><div role="button" tabindex="0" class="jn jo at jp v jq"><div class="fs ft az"><img alt="" class="v jr js" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1ayo0n2zq_gy7VERYmp4lrA2x.png" role="presentation" width="1000" height="387"></div></div><figcaption class="jt ju fu fs ft jv jw bf b bg bh dx" data-selectable-paragraph="">Principal Component Analysis (PCA) is looking for the best linear subspace using linear algebra.</figcaption></figure></div></div></div><div class="n p"><div class="au av aw ax ay gj ba v"><p id="0603" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">Translated
 in our global framework, we are looking for an encoder in the family E 
of the n_e by n_d matrices (linear transformation) whose rows are 
orthonormal (features independence) and for the associated decoder among
 the family D of n_d by n_e matrices. It can be shown that the unitary 
eigenvectors corresponding to the n_e greatest eigenvalues (in norm) of 
the covariance features matrix are orthogonal (or can be chosen to be 
so) and define the best subspace of dimension n_e to project data on 
with minimal error of approximation. Thus, these n_e eigenvectors can be
 chosen as our new features and, so, the problem of dimension reduction 
can then be expressed as an eigenvalue/eigenvector problem. Moreover, it
 can also be shown that, in such case, the decoder matrix is the 
transposed of the encoder matrix.</p></div></div><div class="ja"><div class="n p"><div class="jb jc jd je jf jg ax jh ay ji ba v"><figure class="lx ly lz ma mb ja jl jm paragraph-image"><div role="button" tabindex="0" class="jn jo at jp v jq"><div class="fs ft az"><img alt="" class="v jr js" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1LRPyMAwDlio7f1_YKYI2hw2x.png" role="presentation" width="1000" height="399"></div></div><figcaption class="jt ju fu fs ft jv jw bf b bg bh dx" data-selectable-paragraph="">PCA matches the encoder-decoder framework we described.</figcaption></figure></div></div></div><div class="n p"><div class="au av aw ax ay gj ba v"><h2 id="11f4" class="mn kw gm bf kx mo mp hn la mq mr hq ld hr ms ht lh hu mt hw ll hx mu hz lp mv hj" data-selectable-paragraph="">Autoencoders</h2><p id="1d95" class="jy jz gm ka b hl lr kc kd ho ls kf kg kh lt kj kk kl lu kn ko kp lv kr ks kt gf hj" data-selectable-paragraph="">Let’s now discuss <strong class="ka gn">autoencoders</strong>
 and see how we can use neural networks for dimensionality reduction. 
The general idea of autoencoders is pretty simple and consists in <strong class="ka gn">setting an encoder and a decoder as neural networks</strong> and to <strong class="ka gn">learn the best encoding-decoding scheme using an iterative optimisation process</strong>.
 So, at each iteration we feed the autoencoder architecture (the encoder
 followed by the decoder) with some data, we compare the encoded-decoded
 output with the initial data and backpropagate the error through the 
architecture to update the weights of the networks.</p><p id="4aed" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">Thus,
 intuitively, the overall autoencoder architecture (encoder+decoder) 
creates a bottleneck for data that ensures only the main structured part
 of the information can go through and be reconstructed. Looking at our 
general framework, the family E of considered encoders is defined by the
 encoder network architecture, the family D of considered decoders is 
defined by the decoder network architecture and the search of encoder 
and decoder that minimise the reconstruction error is done by gradient 
descent over the parameters of these networks.</p><figure class="lx ly lz ma mb ja fs ft paragraph-image"><div role="button" tabindex="0" class="jn jo at jp v jq"><div class="fs ft np"><img alt="" class="v jr js" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1bY_ShNK6lBCQ3D9LYIfwJg2x.png" role="presentation" width="700" height="383"></div></div><figcaption class="jt ju fu fs ft jv jw bf b bg bh dx" data-selectable-paragraph="">Illustration of an autoencoder with its loss function.</figcaption></figure><p id="bf7b" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">Let’s
 first suppose that both our encoder and decoder architectures have only
 one layer without non-linearity (linear autoencoder). Such encoder and 
decoder are then simple linear transformations that can be expressed as 
matrices. In such situation, we can see a clear link with PCA in the 
sense that, just like PCA does, we are looking for the best linear 
subspace to project data on with as few information loss as possible 
when doing so. Encoding and decoding matrices obtained with PCA define 
naturally one of the solutions we would be satisfied to reach by 
gradient descent, but we should outline that this is not the only one. 
Indeed, <strong class="ka gn">several basis can be chosen to describe the same optimal subspace</strong>
 and, so, several encoder/decoder pairs can give the optimal 
reconstruction error. Moreover, for linear autoencoders and contrarily 
to PCA, the new features we end up do not have to be independent (no 
orthogonality constraints in the neural networks).</p></div></div><div class="ja"><div class="n p"><div class="jb jc jd je jf jg ax jh ay ji ba v"><figure class="lx ly lz ma mb ja jl jm paragraph-image"><div role="button" tabindex="0" class="jn jo at jp v jq"><div class="fs ft az"><img alt="" class="v jr js" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1ek9ZFmimq9Sr1sG5Z0jXfQ2x.png" role="presentation" width="1000" height="477"></div></div><figcaption class="jt ju fu fs ft jv jw bf b bg bh dx" data-selectable-paragraph="">Link between linear autoencoder and PCA.</figcaption></figure></div></div></div><div class="n p"><div class="au av aw ax ay gj ba v"><p id="057a" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">Now,
 let’s assume that both the encoder and the decoder are deep and 
non-linear. In such case, the more complex the architecture is, the more
 the autoencoder can proceed to a high dimensionality reduction while 
keeping reconstruction loss low. Intuitively, if our encoder and our 
decoder have enough degrees of freedom, we can reduce any initial 
dimensionality to 1. Indeed, an encoder with “infinite power” could 
theoretically takes our N initial data points and encodes them as 1, 2, 
3, … up to N (or more generally, as N integer on the real axis) and the 
associated decoder could make the reverse transformation, with no loss 
during the process.</p><p id="8d4d" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">Here,
 we should however keep two things in mind. First, an important 
dimensionality reduction with no reconstruction loss often comes with a 
price: the lack of interpretable and exploitable structures in the 
latent space (<strong class="ka gn">lack of regularity</strong>). 
Second, most of the time the final purpose of dimensionality reduction 
is not to only reduce the number of dimensions of the data but to reduce
 this number of dimensions <strong class="ka gn">while keeping the major part of the data structure information in the reduced representations</strong>.
 For these two reasons, the dimension of the latent space and the 
“depth” of autoencoders (that define degree and quality of compression) 
have to be carefully controlled and adjusted depending on the final 
purpose of the dimensionality reduction.</p></div></div><div class="ja"><div class="n p"><div class="jb jc jd je jf jg ax jh ay ji ba v"><figure class="lx ly lz ma mb ja jl jm paragraph-image"><div role="button" tabindex="0" class="jn jo at jp v jq"><div class="fs ft az"><img alt="" class="v jr js" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1F-3zbCL_lp7EclKowfowMA2x.png" role="presentation" width="1000" height="396"></div></div><figcaption class="jt ju fu fs ft jv jw bf b bg bh dx" data-selectable-paragraph="">When reducing dimensionality, we want to keep the main structure there exists among the data.</figcaption></figure></div></div></div></section><div class="n p db na nb nc" role="separator"><span class="nd im ca ne nf ng"></span><span class="nd im ca ne nf ng"></span><span class="nd im ca ne nf"></span></div><section class="gf gg gh du gi"><div class="n p"><div class="au av aw ax ay gj ba v"><h1 id="b974" class="kv kw gm bf kx ky nh kc la lb ni kf ld le nj lg lh li nk lk ll lm nl lo lp lq hj" data-selectable-paragraph="">Variational Autoencoders</h1><p id="0fd2" class="jy jz gm ka b hl lr kc kd ho ls kf kg kh lt kj kk kl lu kn ko kp lv kr ks kt gf hj" data-selectable-paragraph="">Up
 to now, we have discussed dimensionality reduction problem and 
introduce autoencoders that are encoder-decoder architectures that can 
be trained by gradient descent. Let’s now make the link with the content
 generation problem, see the limitations of autoencoders in their 
current form for this problem and introduce Variational Autoencoders.</p><h2 id="9ca9" class="mn kw gm bf kx mo mp hn la mq mr hq ld hr ms ht lh hu mt hw ll hx mu hz lp mv hj" data-selectable-paragraph="">Limitations of autoencoders for content generation</h2><p id="1345" class="jy jz gm ka b hl lr kc kd ho ls kf kg kh lt kj kk kl lu kn ko kp lv kr ks kt gf hj" data-selectable-paragraph="">At
 this point, a natural question that comes in mind is “what is the link 
between autoencoders and content generation?”. Indeed, once the 
autoencoder has been trained, we have both an encoder and a decoder but 
still no real way to produce any new content. At first sight, we could 
be tempted to think that, if the latent space is regular enough (well 
“organized” by the encoder during the training process), we could take a
 point randomly from that latent space and decode it to get a new 
content. The decoder would then act more or less like the generator of a
 Generative Adversarial Network.</p><figure class="lx ly lz ma mb ja fs ft paragraph-image"><div role="button" tabindex="0" class="jn jo at jp v jq"><div class="fs ft nq"><div class="mg s at mh"><div class="nr mj s"><div class="es mc ei ev er fa v md me mf"><img alt="" class="ei ev er fa v mk ml ae xz" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1Qd1xKV9o-AnWtfIDhhNdFg2x.png" role="presentation" width="700" height="365"></div><img alt="" class="sa wx ei ev er fa v c" role="presentation" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1Qd1xKV9o-AnWtfIDhhNdFg2x_002.png" srcset="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1Qd1xKV9o-AnWtfIDhhNdFg2x_005.png 276w, Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1Qd1xKV9o-AnWtfIDhhNdFg2x_004.png 552w, Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1Qd1xKV9o-AnWtfIDhhNdFg2x_003.png 640w, Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1Qd1xKV9o-AnWtfIDhhNdFg2x_002.png 700w" sizes="700px" width="700" height="365"><noscript></noscript></div></div></div></div><figcaption class="jt ju fu fs ft jv jw bf b bg bh dx" data-selectable-paragraph="">We
 can generate new data by decoding points that are randomly sampled from
 the latent space. The quality and relevance of generated data depend on
 the regularity of the latent space.</figcaption></figure><p id="ce28" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">However,
 as we discussed in the previous section, the regularity of the latent 
space for autoencoders is a difficult point that depends on the 
distribution of the data in the initial space, the dimension of the 
latent space and the architecture of the encoder. So, it is pretty 
difficult (if not impossible) to ensure, a priori, that the encoder will
 organize the latent space in a smart way compatible with the generative
 process we just described.</p><p id="09eb" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">To
 illustrate this point, let’s consider the example we gave previously in
 which we described an encoder and a decoder powerful enough to put any N
 initial training data onto the real axis (each data point being encoded
 as a real value) and decode them without any reconstruction loss. In 
such case, the high degree of freedom of the autoencoder that makes 
possible to encode and decode with no information loss (despite the low 
dimensionality of the latent space) <strong class="ka gn">leads to a severe overfitting</strong>
 implying that some points of the latent space will give meaningless 
content once decoded. If this one dimensional example has been 
voluntarily chosen to be quite extreme, we can notice that the problem 
of the autoencoders latent space regularity is much more general than 
that and deserve a special attention.</p></div></div><div class="ja"><div class="n p"><div class="jb jc jd je jf jg ax jh ay ji ba v"><figure class="lx ly lz ma mb ja jl jm paragraph-image"><div role="button" tabindex="0" class="jn jo at jp v jq"><div class="fs ft az"><img alt="" class="v jr js" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1iSfaVxcGi_ELkKgAG0YRlQ2x.png" role="presentation" width="1000" height="333"></div></div><figcaption class="jt ju fu fs ft jv jw bf b bg bh dx" data-selectable-paragraph="">Irregular latent space prevent us from using autoencoder for new content generation.</figcaption></figure></div></div></div><div class="n p"><div class="au av aw ax ay gj ba v"><p id="4384" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">When
 thinking about it for a minute, this lack of structure among the 
encoded data into the latent space is pretty normal. Indeed, nothing in 
the task the autoencoder is trained for enforce to get such 
organisation: <strong class="ka gn">the autoencoder is solely trained to encode and decode with as few loss as possible, no matter how the latent space is organised</strong>.
 Thus, if we are not careful about the definition of the architecture, 
it is natural that, during the training, the network takes advantage of 
any overfitting possibilities to achieve its task as well as it can… 
unless we explicitly regularise it!</p><h2 id="780e" class="mn kw gm bf kx mo mp hn la mq mr hq ld hr ms ht lh hu mt hw ll hx mu hz lp mv hj" data-selectable-paragraph="">Definition of variational autoencoders</h2><p id="a576" class="jy jz gm ka b hl lr kc kd ho ls kf kg kh lt kj kk kl lu kn ko kp lv kr ks kt gf hj" data-selectable-paragraph="">So,
 in order to be able to use the decoder of our autoencoder for 
generative purpose, we have to be sure that the latent space is regular 
enough. One possible solution to obtain such regularity is to introduce 
explicit regularisation during the training process. Thus, as we briefly
 mentioned in the introduction of this post, <mark class="acl acm en"><strong class="ka gn">a
 variational autoencoder can be defined as being an autoencoder whose 
training is regularised to avoid overfitting and ensure that the latent 
space has good properties that enable generative process.</strong></mark></p><p id="cae4" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">Just
 as a standard autoencoder, a variational autoencoder is an architecture
 composed of both an encoder and a decoder and that is trained to 
minimise the reconstruction error between the encoded-decoded data and 
the initial data. However, in order to introduce some regularisation of 
the latent space, we proceed to a slight modification of the 
encoding-decoding process: <strong class="ka gn">instead of encoding an input as a single point, we encode it as a distribution over the latent space</strong>. The model is then trained as follows:</p><ul class=""><li id="4251" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ns nt nu hj" data-selectable-paragraph="">first, the input is encoded as distribution over the latent space</li><li id="ef92" class="jy jz gm ka b hl nv kc kd ho nw kf kg kh nx kj kk kl ny kn ko kp nz kr ks kt ns nt nu hj" data-selectable-paragraph="">second, a point from the latent space is sampled from that distribution</li><li id="ebf2" class="jy jz gm ka b hl nv kc kd ho nw kf kg kh nx kj kk kl ny kn ko kp nz kr ks kt ns nt nu hj" data-selectable-paragraph="">third, the sampled point is decoded and the reconstruction error can be computed</li><li id="8e10" class="jy jz gm ka b hl nv kc kd ho nw kf kg kh nx kj kk kl ny kn ko kp nz kr ks kt ns nt nu hj" data-selectable-paragraph="">finally, the reconstruction error is backpropagated through the network</li></ul></div></div><div class="ja"><div class="n p"><div class="jb jc jd je jf jg ax jh ay ji ba v"><figure class="lx ly lz ma mb ja jl jm paragraph-image"><div role="button" tabindex="0" class="jn jo at jp v jq"><div class="fs ft az"><img alt="" class="v jr js" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1ejNnusxYrn1NRDZf4Kg2lw2x.png" role="presentation" width="1000" height="331"></div></div><figcaption class="jt ju fu fs ft jv jw bf b bg bh dx" data-selectable-paragraph="">Difference between autoencoder (deterministic) and variational autoencoder (probabilistic).</figcaption></figure></div></div></div><div class="n p"><div class="au av aw ax ay gj ba v"><p id="36b1" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">In
 practice, the encoded distributions are chosen to be normal so that the
 encoder can be trained to return the mean and the covariance matrix 
that describe these Gaussians. The reason why an input is encoded as a 
distribution with some variance instead of a single point is that it 
makes possible to express very naturally the latent space 
regularisation: the distributions returned by the encoder are enforced 
to be close to a standard normal distribution. We will see in the next 
subsection that we ensure this way both a local and global 
regularisation of the latent space (local because of the variance 
control and global because of the mean control).</p><p id="4ebc" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">Thus,
 the loss function that is minimised when training a VAE is composed of a
 “reconstruction term” (on the final layer), that tends to make the 
encoding-decoding scheme as performant as possible, and a 
“regularisation term” (on the latent layer), that tends to regularise 
the organisation of the latent space by making the distributions 
returned by the encoder close to a standard normal distribution. That 
regularisation term is expressed as the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" class="ea jx" rel="noopener ugc nofollow" target="_blank">Kulback-Leibler divergence</a>
 between the returned distribution and a standard Gaussian and will be 
further justified in the next section. We can notice that the 
Kullback-Leibler divergence between two Gaussian distributions has a 
closed form that can be directly expressed in terms of the means and the
 covariance matrices of the the two distributions.</p><figure class="lx ly lz ma mb ja fs ft paragraph-image"><div role="button" tabindex="0" class="jn jo at jp v jq"><div class="fs ft oa"><img alt="" class="v jr js" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1Q5dogodt3wzKKktE0v3dMQ2x.png" role="presentation" width="700" height="341"></div></div><figcaption class="jt ju fu fs ft jv jw bf b bg bh dx" data-selectable-paragraph="">In
 variational autoencoders, the loss function is composed of a 
reconstruction term (that makes the encoding-decoding scheme efficient) 
and a regularisation term (that makes the latent space regular).</figcaption></figure><h2 id="6116" class="mn kw gm bf kx mo mp hn la mq mr hq ld hr ms ht lh hu mt hw ll hx mu hz lp mv hj" data-selectable-paragraph="">Intuitions about the regularisation</h2><p id="a0bd" class="jy jz gm ka b hl lr kc kd ho ls kf kg kh lt kj kk kl lu kn ko kp lv kr ks kt gf hj" data-selectable-paragraph="">The
 regularity that is expected from the latent space in order to make 
generative process possible can be expressed through two main 
properties: <strong class="ka gn">continuity</strong> (two close points in the latent space should not give two completely different contents once decoded) and <strong class="ka gn">completeness</strong> (for a chosen distribution, a point sampled from the latent space should give “meaningful” content once decoded).</p></div></div><div class="ja"><div class="n p"><div class="jb jc jd je jf jg ax jh ay ji ba v"><figure class="lx ly lz ma mb ja jl jm paragraph-image"><div role="button" tabindex="0" class="jn jo at jp v jq"><div class="fs ft az"><img alt="" class="v jr js" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/183S0T8IEJyudR_I5rI9now2x.png" role="presentation" width="1000" height="366"></div></div><figcaption class="jt ju fu fs ft jv jw bf b bg bh dx" data-selectable-paragraph="">Difference between a “regular” and an “irregular” latent space.</figcaption></figure></div></div></div><div class="n p"><div class="au av aw ax ay gj ba v"><p id="c1cd" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">The
 only fact that VAEs encode inputs as distributions instead of simple 
points is not sufficient to ensure continuity and completeness. Without a
 well defined regularisation term, the model can learn, in order to 
minimise its reconstruction error, <strong class="ka gn">to “ignore” the fact that distributions are returned and behave almost like classic autoencoders</strong>
 (leading to overfitting). To do so, the encoder can either return 
distributions with tiny variances (that would tend to be punctual 
distributions) or return distributions with very different means (that 
would then be really far apart from each other in the latent space). In 
both cases, distributions are used the wrong way (cancelling the 
expected benefit) and continuity and/or completeness are not satisfied.</p><p id="5aaf" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">So, in order to avoid these effects <strong class="ka gn">we have to regularise both the covariance matrix and the mean of the distributions returned by the encoder</strong>.
 In practice, this regularisation is done by enforcing distributions to 
be close to a standard normal distribution (centred and reduced). This 
way, we require the covariance matrices to be close to the identity, 
preventing punctual distributions, and the mean to be close to 0, 
preventing encoded distributions to be too far apart from each others.</p></div></div><div class="ja"><div class="n p"><div class="jb jc jd je jf jg ax jh ay ji ba v"><figure class="lx ly lz ma mb ja jl jm paragraph-image"><div role="button" tabindex="0" class="jn jo at jp v jq"><div class="fs ft az"><img alt="" class="v jr js" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/19ouOKh2w-b3NNOVx4Mw9bg2x.png" role="presentation" width="1000" height="366"></div></div><figcaption class="jt ju fu fs ft jv jw bf b bg bh dx" data-selectable-paragraph="">The returned distributions of VAEs have to be regularised to obtain a latent space with good properties.</figcaption></figure></div></div></div><div class="n p"><div class="au av aw ax ay gj ba v"><p id="01b0" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">With
 this regularisation term, we prevent the model to encode data far apart
 in the latent space and encourage as much as possible returned 
distributions to “overlap”, satisfying this way the expected continuity 
and completeness conditions. Naturally, as for any regularisation term, 
this comes at the price of a higher reconstruction error on the training
 data. The tradeoff between the reconstruction error and the KL 
divergence can however be adjusted and we will see in the next section 
how the expression of the balance naturally emerge from our formal 
derivation.</p><p id="fa99" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">To conclude this subsection, we can observe that continuity and completeness obtained with regularisation <strong class="ka gn">tend to create a “gradient” over the information encoded in the latent space</strong>.
 For example, a point of the latent space that would be halfway between 
the means of two encoded distributions coming from different training 
data should be decoded in something that is somewhere between the data 
that gave the first distribution and the data that gave the second 
distribution as it may be sampled by the autoencoder in both cases.</p><figure class="lx ly lz ma mb ja fs ft paragraph-image"><div role="button" tabindex="0" class="jn jo at jp v jq"><div class="fs ft ob"><img alt="" class="v jr js" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/179AzftDm7WcQ9OfRH5Y-6g2x.png" role="presentation" width="700" height="378"></div></div><figcaption class="jt ju fu fs ft jv jw bf b bg bh dx" data-selectable-paragraph="">Regularisation tends to create a “gradient” over the information encoded in the latent space.</figcaption></figure><blockquote class="mw mx my"><p id="ec2d" class="jy jz mz ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph=""><strong class="ka gn">Note.</strong>
 As a side note, we can mention that the second potential problem we 
have mentioned (the network put distributions far from each others) is 
in fact almost equivalent to the first one (the network tends to return 
punctual distribution) up to a change of scale: in both case variances 
of distributions become small relatively to distance between their 
means.</p></blockquote></div></div></section><div class="n p db na nb nc" role="separator"><span class="nd im ca ne nf ng"></span><span class="nd im ca ne nf ng"></span><span class="nd im ca ne nf"></span></div><section class="gf gg gh du gi"><div class="n p"><div class="au av aw ax ay gj ba v"><h1 id="31fa" class="kv kw gm bf kx ky nh kc la lb ni kf ld le nj lg lh li nk lk ll lm nl lo lp lq hj" data-selectable-paragraph="">Mathematical details of VAEs</h1><p id="0ed1" class="jy jz gm ka b hl lr kc kd ho ls kf kg kh lt kj kk kl lu kn ko kp lv kr ks kt gf hj" data-selectable-paragraph="">In
 the previous section we gave the following intuitive overview: VAEs are
 autoencoders that encode inputs as distributions instead of points and 
whose latent space “organisation” is regularised by constraining 
distributions returned by the encoder to be close to a standard 
Gaussian. In this section we will give a more mathematical view of VAEs 
that will allow us to justify the regularisation term more rigorously. 
To do so, we will set a clear probabilistic framework and will use, in 
particular, variational inference technique.</p><h2 id="f77b" class="mn kw gm bf kx mo mp hn la mq mr hq ld hr ms ht lh hu mt hw ll hx mu hz lp mv hj" data-selectable-paragraph="">Probabilistic framework and assumptions</h2><p id="cd56" class="jy jz gm ka b hl lr kc kd ho ls kf kg kh lt kj kk kl lu kn ko kp lv kr ks kt gf hj" data-selectable-paragraph="">Let’s
 begin by defining a probabilistic graphical model to describe our data.
 We denote by x the variable that represents our data and assume that x 
is generated from a latent variable z (the encoded representation) that 
is not directly observed. Thus, for each data point, the following two 
steps generative process is assumed:</p><ul class=""><li id="f62a" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ns nt nu hj" data-selectable-paragraph="">first, a latent representation z is sampled from the prior distribution p(z)</li><li id="ee29" class="jy jz gm ka b hl nv kc kd ho nw kf kg kh nx kj kk kl ny kn ko kp nz kr ks kt ns nt nu hj" data-selectable-paragraph="">second, the data x is sampled from the conditional likelihood distribution p(x|z)</li></ul><figure class="lx ly lz ma mb ja fs ft paragraph-image"><div class="fs ft oc"><div class="mg s at mh"><div class="od mj s"><div class="sa wx ei ev er fa v md me mf"><img alt="" class="ei ev er fa v mk ml mm" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1dxlZr07dXNYiTFWL62D7Ag2x.png" role="presentation" width="564" height="290"></div><img alt="" class="es mc ei ev er fa v c" role="presentation" width="564" height="290"><noscript></noscript></div></div></div><figcaption class="jt ju fu fs ft jv jw bf b bg bh dx" data-selectable-paragraph="">Graphical model of the data generation process.</figcaption></figure><p id="bde5" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">With
 such a probabilistic model in mind, we can redefine our notions of 
encoder and decoder. Indeed, contrarily to a simple autoencoder that 
consider deterministic encoder and decoder, <strong class="ka gn">we are going to consider now probabilistic versions of these two objects</strong>. The “probabilistic decoder” is naturally defined by <strong class="ka gn">p(x|z), that describes the distribution of the decoded variable given the encoded one</strong>, whereas the “probabilistic encoder” is defined by <strong class="ka gn">p(z|x), that describes the distribution of the encoded variable given the decoded one</strong>.</p><p id="546d" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">At
 this point, we can already notice that the regularisation of the latent
 space that we lacked in simple autoencoders naturally appears here in 
the definition of the data generation process: encoded representations z
 in the latent space are indeed assumed to follow the prior distribution
 p(z). Otherwise, we can also remind the the well-known Bayes theorem 
that makes the link between the prior p(z), the likelihood p(x|z), and 
the posterior p(z|x)</p><figure class="lx ly lz ma mb ja fs ft paragraph-image"><div class="fs ft oe"><div class="mg s at mh"><div class="of mj s"><div class="sa wx ei ev er fa v md me mf"><img alt="" class="ei ev er fa v mk ml mm" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/15wyhpwWHlLZHGdlho_rboA2x.png" role="presentation" width="664" height="98"></div><img alt="" class="es mc ei ev er fa v c" role="presentation" width="664" height="98"><noscript></noscript></div></div></div></figure><p id="46f1" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">Let’s
 now make the assumption that p(z) is a standard Gaussian distribution 
and that p(x|z) is a Gaussian distribution whose mean is defined by a 
deterministic function f of the variable of z and whose covariance 
matrix has the form of a positive constant c that multiplies the 
identity matrix I. The function f is assumed to belong to a family of 
functions denoted F that is left unspecified for the moment and that 
will be chosen later. Thus, we have</p><figure class="lx ly lz ma mb ja fs ft paragraph-image"><div role="button" tabindex="0" class="jn jo at jp v jq"><div class="fs ft og"><div class="mg s at mh"><div class="oh mj s"><div class="sa wx ei ev er fa v md me mf"><img alt="" class="ei ev er fa v mk ml mm" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1Nnfsrn5-aYLhuwgSYPJQzA2x.png" role="presentation" width="700" height="94"></div><img alt="" class="es mc ei ev er fa v c" role="presentation" width="700" height="94"><noscript></noscript></div></div></div></div></figure><p id="a3c5" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">Let’s
 consider, for now, that f is well defined and fixed. In theory, as we 
know p(z) and p(x|z), we can use the Bayes theorem to compute p(z|x): 
this is a classical <a class="ea jx" rel="noopener" target="_blank" href="https://towardsdatascience.com/bayesian-inference-problem-mcmc-and-variational-inference-25a8aa9bce29">Bayesian inference problem</a>.
 However, as we discussed in our previous article, this kind of 
computation is often intractable (because of the integral at the 
denominator) and require the use of approximation techniques such as 
variational inference.</p><blockquote class="mw mx my"><p id="175d" class="jy jz mz ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph=""><strong class="ka gn">Note.</strong>
 Here we can mention that p(z) and p(x|z) are both Gaussian 
distribution. So, if we had E(x|z) = f(z) = z, it would imply that 
p(z|x) should also follow a Gaussian distribution and, in theory, we 
could “only” try to express the mean and the covariance matrix of p(z|x)
 with respect to the means and the covariance matrices of p(z) and 
p(x|z). However, in practice this condition is not met and we need to 
use of an approximation technique like variational inference that makes 
the approach pretty general and more robust to some changes in the 
hypothesis of the model.</p></blockquote><h2 id="b0ae" class="mn kw gm bf kx mo mp hn la mq mr hq ld hr ms ht lh hu mt hw ll hx mu hz lp mv hj" data-selectable-paragraph="">Variational inference formulation</h2><p id="acb0" class="jy jz gm ka b hl lr kc kd ho ls kf kg kh lt kj kk kl lu kn ko kp lv kr ks kt gf hj" data-selectable-paragraph="">In statistics, <strong class="ka gn">variational inference (VI) is a technique to approximate complex distributions</strong>.
 The idea is to set a parametrised family of distribution (for example 
the family of Gaussians, whose parameters are the mean and the 
covariance) and to look for the best approximation of our target 
distribution among this family. The best element in the family is one 
that minimise a given approximation error measurement (most of the time 
the Kullback-Leibler divergence between approximation and target) and is
 found by gradient descent over the parameters that describe the family.
 For more details, we refer to <a class="ea jx" rel="noopener" target="_blank" href="https://towardsdatascience.com/bayesian-inference-problem-mcmc-and-variational-inference-25a8aa9bce29">our post on variational inference</a> and references therein.</p><p id="1e2b" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">Here
 we are going to approximate p(z|x) by a Gaussian distribution q_x(z) 
whose mean and covariance are defined by two functions, g and h, of the 
parameter x. These two functions are supposed to belong, respectively, 
to the families of functions G and H that will be specified later but 
that are supposed to be parametrised. Thus we can denote</p><figure class="lx ly lz ma mb ja fs ft paragraph-image"><div role="button" tabindex="0" class="jn jo at jp v jq"><div class="fs ft oi"><div class="mg s at mh"><div class="oj mj s"><div class="sa wx ei ev er fa v md me mf"><img alt="" class="ei ev er fa v mk ml mm" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1j9goPhh0meH884uGL70Aqg2x.png" role="presentation" width="700" height="38"></div><img alt="" class="es mc ei ev er fa v c" role="presentation" width="700" height="38"><noscript></noscript></div></div></div></div></figure><p id="9ed6" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">So,
 we have defined this way a family of candidates for variational 
inference and need now to find the best approximation among this family 
by optimising the functions g and h (in fact, their parameters) to 
minimise the Kullback-Leibler divergence between the approximation and 
the target p(z|x). In other words, we are looking for the optimal g* and
 h* such that</p><figure class="lx ly lz ma mb ja fs ft paragraph-image"><div role="button" tabindex="0" class="jn jo at jp v jq"><div class="fs ft ok"><div class="mg s at mh"><div class="ol mj s"><div class="sa wx ei ev er fa v md me mf"><img alt="" class="ei ev er fa v mk ml mm" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1v56YF5KqZk35r85EZBZuAQ2x.png" role="presentation" width="700" height="218"></div><img alt="" class="es mc ei ev er fa v c" role="presentation" width="700" height="218"><noscript></noscript></div></div></div></div></figure><p id="57a5" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">In
 the second last equation, we can observe the tradeoff there exists — 
when approximating the posterior p(z|x) — between maximising the 
likelihood of the “observations” (maximisation of the expected 
log-likelihood, for the first term) and staying close to the prior 
distribution (minimisation of the KL divergence between q_x(z) and p(z),
 for the second term). This tradeoff is natural for Bayesian inference 
problem and express the balance that needs to be found between the 
confidence we have in the data and the confidence we have in the prior.</p><p id="d7ef" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">Up
 to know, we have assumed the function f known and fixed and we have 
showed that, under such assumptions, we can approximate the posterior 
p(z|x) using variational inference technique. However, in practice this 
function f, that defines the decoder, is not known and also need to be 
chosen. To do so, let’s remind that our initial goal is to find a 
performant encoding-decoding scheme whose latent space is regular enough
 to be used for generative purpose. If the regularity is mostly ruled by
 the prior distribution assumed over the latent space, the performance 
of the overall encoding-decoding scheme highly depends on the choice of 
the function f. Indeed, as p(z|x) can be approximate (by variational 
inference) from p(z) and p(x|z) and as p(z) is a simple standard 
Gaussian, the only two levers we have at our disposal in our model to 
make optimisations are the parameter c (that defines the variance of the
 likelihood) and the function f (that defines the mean of the 
likelihood).</p><p id="b47b" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">So,
 let’s consider that, as we discussed earlier, we can get for any 
function f in F (each defining a different probabilistic decoder p(x|z))
 the best approximation of p(z|x), denoted q*_x(z). Despite its 
probabilistic nature, we are looking for an encoding-decoding scheme as 
efficient as possible and, then, we want to choose the function f that 
maximises the expected log-likelihood of x given z when z is sampled 
from q*_x(z). <strong class="ka gn">In other words, for a given input x,
 we want to maximise the probability to have x̂ = x when we sample z 
from the distribution q*_x(z) and then sample x̂ from the distribution 
p(x|z).</strong> Thus, we are looking for the optimal f* such that</p><figure class="lx ly lz ma mb ja fs ft paragraph-image"><div class="fs ft om"><div class="mg s at mh"><div class="on mj s"><div class="sa wx ei ev er fa v md me mf"><img alt="" class="ei ev er fa v mk ml mm" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/13dfxn11raqxXWc2WcADhqQ2x.png" role="presentation" width="640" height="190"></div><img alt="" class="es mc ei ev er fa v c" role="presentation" width="640" height="190"><noscript></noscript></div></div></div></figure><p id="a7fb" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">where
 q*_x(z) depends on the function f and is obtained as described before. 
Gathering all the pieces together, we are looking for optimal f*, g* and
 h* such that</p><figure class="lx ly lz ma mb ja fs ft paragraph-image"><div role="button" tabindex="0" class="jn jo at jp v jq"><div class="fs ft oo"><div class="mg s at mh"><div class="op mj s"><div class="sa wx ei ev er fa v md me mf"><img alt="" class="ei ev er fa v mk ml mm" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1uJPYBQauUfQtb_-r3vjhng2x.png" role="presentation" width="700" height="57"></div><img alt="" class="es mc ei ev er fa v c" role="presentation" width="700" height="57"><noscript></noscript></div></div></div></div></figure><p id="c960" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">We
 can identify in this objective function the elements introduced in the 
intuitive description of VAEs given in the previous section: the 
reconstruction error between x and f(z) and the regularisation term 
given by the KL divergence between q_x(z) and p(z) (which is a standard 
Gaussian). We can also notice the constant c that rules the balance 
between the two previous terms. The higher c is the more we assume a 
high variance around f(z) for the probabilistic decoder in our model 
and, so, the more we favour the regularisation term over the 
reconstruction term (and the opposite stands if c is low).</p><h2 id="31fe" class="mn kw gm bf kx mo mp hn la mq mr hq ld hr ms ht lh hu mt hw ll hx mu hz lp mv hj" data-selectable-paragraph="">Bringing neural networks into the model</h2><p id="8487" class="jy jz gm ka b hl lr kc kd ho ls kf kg kh lt kj kk kl lu kn ko kp lv kr ks kt gf hj" data-selectable-paragraph="">Up
 to know, we have set a probabilistic model that depends on three 
functions, f, g and h, and express, using variational inference, the 
optimisation problem to solve in order to get f*, g* and h* that give 
the optimal encoding-decoding scheme with this model. As we can’t easily
 optimise over the entire space of functions, we constrain the 
optimisation domain and decide to express f, g and h as neural networks.
 Thus, F, G and H correspond respectively to the families of functions 
defined by the networks architectures and the optimisation is done over 
the parameters of these networks.</p><p id="f439" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">In
 practice, g and h are not defined by two completely independent 
networks but share a part of their architecture and their weights so 
that we have</p><figure class="lx ly lz ma mb ja fs ft paragraph-image"><div role="button" tabindex="0" class="jn jo at jp v jq"><div class="fs ft oq"><div class="mg s at mh"><div class="or mj s"><div class="sa wx ei ev er fa v md me mf"><img alt="" class="ei ev er fa v mk ml mm" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1H-q2OirDCYI1sXX3HkitBg2x.png" role="presentation" width="700" height="29"></div><img alt="" class="es mc ei ev er fa v c" role="presentation" width="700" height="29"><noscript></noscript></div></div></div></div></figure><p id="f0f3" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">As
 it defines the covariance matrix of q_x(z), h(x) is supposed to be a 
square matrix. However, in order to simplify the computation and reduce 
the number of parameters, we make the additional assumption that our 
approximation of p(z|x), q_x(z), is a multidimensional Gaussian 
distribution with diagonal covariance matrix (variables independence 
assumption). With this assumption, h(x) is simply the vector of the 
diagonal elements of the covariance matrix and has then the same size as
 g(x). However, we reduce this way the family of distributions we 
consider for variational inference and, so, the approximation of p(z|x) 
obtained can be less accurate.</p><figure class="lx ly lz ma mb ja fs ft paragraph-image"><div role="button" tabindex="0" class="jn jo at jp v jq"><div class="fs ft os"><div class="mg s at mh"><div class="ot mj s"><div class="sa wx ei ev er fa v md me mf"><img alt="" class="ei ev er fa v mk ml mm" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1XYyWimolMhPDMg8qCNlcwg2x.png" role="presentation" width="700" height="324"></div><img alt="" class="es mc ei ev er fa v c" role="presentation" width="700" height="324"><noscript></noscript></div></div></div></div><figcaption class="jt ju fu fs ft jv jw bf b bg bh dx" data-selectable-paragraph="">Encoder part of the VAE.</figcaption></figure><p id="c127" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">Contrarily
 to the encoder part that models p(z|x) and for which we considered a 
Gaussian with both mean and covariance that are functions of x (g and 
h), our model assumes for p(x|z) a Gaussian with fixed covariance. The 
function f of the variable z defining the mean of that Gaussian is 
modelled by a neural network and can be represented as follows</p><figure class="lx ly lz ma mb ja fs ft paragraph-image"><div role="button" tabindex="0" class="jn jo at jp v jq"><div class="fs ft os"><div class="mg s at mh"><div class="ou mj s"><div class="sa wx ei ev er fa v md me mf"><img alt="" class="ei ev er fa v mk ml mm" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/11n6HwjwUWbmE9PvCzOVcbw2x.png" role="presentation" width="700" height="284"></div><img alt="" class="es mc ei ev er fa v c" role="presentation" width="700" height="284"><noscript></noscript></div></div></div></div><figcaption class="jt ju fu fs ft jv jw bf b bg bh dx" data-selectable-paragraph="">Decoder part of the VAE.</figcaption></figure><p id="cab8" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">The
 overall architecture is then obtained by concatenating the encoder and 
the decoder parts. However we still need to be very careful about the 
way we sample from the distribution returned by the encoder during the 
training. The sampling process has to be expressed in a way that allows 
the error to be backpropagated through the network. A simple trick, 
called <strong class="ka gn">reparametrisation trick</strong>, is used 
to make the gradient descent possible despite the random sampling that 
occurs halfway of the architecture and consists in using the fact that 
if z is a random variable following a Gaussian distribution with mean 
g(x) and with covariance H(x)=h(x).h^t(x) then it can be expressed as</p><figure class="lx ly lz ma mb ja fs ft paragraph-image"><div class="fs ft ov"><div class="mg s at mh"><div class="ow mj s"><div class="sa wx ei ev er fa v md me mf"><img alt="" class="ei ev er fa v mk ml mm" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1SE-A1yR7BXIjNL0tsmITZg2x.png" role="presentation" width="604" height="44"></div><img alt="" class="es mc ei ev er fa v c" role="presentation" width="604" height="44"><noscript></noscript></div></div></div></figure></div></div><div class="ja"><div class="n p"><div class="jb jc jd je jf jg ax jh ay ji ba v"><figure class="lx ly lz ma mb ja jl jm paragraph-image"><div role="button" tabindex="0" class="jn jo at jp v jq"><div class="fs ft az"><img alt="" class="v jr js" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1S8CoO3TGtFBpzv8GvmgKeg2x.png" role="presentation" width="1000" height="379"></div></div><figcaption class="jt ju fu fs ft jv jw bf b bg bh dx" data-selectable-paragraph="">Illustration of the reparametrisation trick.</figcaption></figure></div></div></div><div class="n p"><div class="au av aw ax ay gj ba v"><p id="f6c4" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">Finally,
 the objective function of the variational autoencoder architecture 
obtained this way is given by the last equation of the previous 
subsection in which the theoretical expectancy is replaced by a more or 
less accurate Monte-Carlo approximation that consists, most of the time,
 into a single draw. So, considering this approximation and denoting C =
 1/(2c), we recover the loss function derived intuitively in the 
previous section, composed of a reconstruction term, a regularisation 
term and a constant to define the relative weights of these two terms.</p></div></div><div class="ja"><div class="n p"><div class="jb jc jd je jf jg ax jh ay ji ba v"><figure class="lx ly lz ma mb ja jl jm paragraph-image"><div role="button" tabindex="0" class="jn jo at jp v jq"><div class="fs ft az"><img alt="" class="v jr js" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1eRcdr8gczweQHk--1pZF9A2x.png" role="presentation" width="1000" height="603"></div></div><figcaption class="jt ju fu fs ft jv jw bf b bg bh dx" data-selectable-paragraph="">Variational Autoencoders representation.</figcaption></figure></div></div></div></section><div class="n p db na nb nc" role="separator"><span class="nd im ca ne nf ng"></span><span class="nd im ca ne nf ng"></span><span class="nd im ca ne nf"></span></div><section class="gf gg gh du gi"><div class="n p"><div class="au av aw ax ay gj ba v"><h1 id="3a01" class="kv kw gm bf kx ky nh kc la lb ni kf ld le nj lg lh li nk lk ll lm nl lo lp lq hj" data-selectable-paragraph="">Takeaways</h1><p id="afee" class="jy jz gm ka b hl lr kc kd ho ls kf kg kh lt kj kk kl lu kn ko kp lv kr ks kt gf hj" data-selectable-paragraph="">The main takeways of this article are:</p><ul class=""><li id="4bae" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ns nt nu hj" data-selectable-paragraph="">dimensionality
 reduction is the process of reducing the number of features that 
describe some data (either by selecting only a subset of the initial 
features or by combining them into a reduced number new features) and, 
so, can be seen as an encoding process</li><li id="9c45" class="jy jz gm ka b hl nv kc kd ho nw kf kg kh nx kj kk kl ny kn ko kp nz kr ks kt ns nt nu hj" data-selectable-paragraph="">autoencoders
 are neural networks architectures composed of both an encoder and a 
decoder that create a bottleneck to go through for data and that are 
trained to lose a minimal quantity of information during the 
encoding-decoding process (training by gradient descent iterations with 
the goal to reduce the reconstruction error)</li><li id="12d1" class="jy jz gm ka b hl nv kc kd ho nw kf kg kh nx kj kk kl ny kn ko kp nz kr ks kt ns nt nu hj" data-selectable-paragraph="">due
 to overfitting, the latent space of an autoencoder can be extremely 
irregular (close points in latent space can give very different decoded 
data, some point of the latent space can give meaningless content once 
decoded, …) and, so, we can’t really define a generative process that 
simply consists to sample a point from the latent space and make it go 
through the decoder to get a new data</li><li id="1c84" class="jy jz gm ka b hl nv kc kd ho nw kf kg kh nx kj kk kl ny kn ko kp nz kr ks kt ns nt nu hj" data-selectable-paragraph="">variational
 autoencoders (VAEs) are autoencoders that tackle the problem of the 
latent space irregularity by making the encoder return a distribution 
over the latent space instead of a single point and by adding in the 
loss function a regularisation term over that returned distribution in 
order to ensure a better organisation of the latent space</li><li id="8876" class="jy jz gm ka b hl nv kc kd ho nw kf kg kh nx kj kk kl ny kn ko kp nz kr ks kt ns nt nu hj" data-selectable-paragraph="">assuming
 a simple underlying probabilistic model to describe our data, the 
pretty intuitive loss function of VAEs, composed of a reconstruction 
term and a regularisation term, can be carefully derived, using in 
particular the statistical technique of variational inference (hence the
 name “variational” autoencoders)</li></ul><p id="847b" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">To
 conclude, we can outline that, during the last years, GANs have 
benefited from much more scientific contributions than VAEs. Among other
 reasons, the higher interest that has been shown by the community for 
GANs can be partly explained by the higher degree of complexity in VAEs 
theoretical basis (probabilistic model and variational inference) 
compared to the simplicity of the adversarial training concept that 
rules GANs. With this post we hope that we managed to share valuable 
intuitions as well as strong theoretical foundations to make VAEs more 
accessible to newcomers, <a class="ea jx" rel="noopener" target="_blank" href="https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29">as we did for GANs earlier this year</a>. However, now that we have discussed in depth both of them, one question remains… are you more GANs or VAEs?</p><p id="9b72" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">Thanks for reading!</p></div></div></section><div class="n p db na nb nc" role="separator"><span class="nd im ca ne nf ng"></span><span class="nd im ca ne nf ng"></span><span class="nd im ca ne nf"></span></div><section class="gf gg gh du gi"><div class="n p"><div class="au av aw ax ay gj ba v"><p id="5633" class="jy jz gm ka b hl kb kc kd ho ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt gf hj" data-selectable-paragraph="">Other articles written with <div class="ca yd"><div><div class="ca" role="tooltip" aria-hidden="false" aria-describedby="223" aria-labelledby="223"><a href="https://medium.com/u/20ad1309823a?source=post_page-----f70510919f73-----------------------------------" class="ku fd cb" rel="noopener" target="_blank">Baptiste Rocca</a></div></div></div>:</p><div class="ox oy oz pa pb pc"><a rel="noopener follow" target="_blank" href="https://towardsdatascience.com/introduction-to-recommender-systems-6c66cf15ada"><div class="em n z"><div class="pd n ao p pe pf"><h2 class="bf gn dn bh md pg ph pi pj pk pl gl hj">Introduction to recommender systems</h2><div class="pm s"><h3 class="bf b dn bh md pg ph pi pj pk pl dx">Overview of some major recommendation algorithms.</h3></div><div class="pn s"><p class="bf b po bh md pg ph pi pj pk pl dx">towardsdatascience.com</p></div></div><div class="pp s"><div class="wm s pr ps pt pp pu jr pc"></div></div></div></a></div><div class="ox oy oz pa pb pc"><a rel="noopener follow" target="_blank" href="https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205"><div class="em n z"><div class="pd n ao p pe pf"><h2 class="bf gn dn bh md pg ph pi pj pk pl gl hj">Ensemble methods: bagging, boosting and stacking</h2><div class="pm s"><h3 class="bf b dn bh md pg ph pi pj pk pl dx">Understanding the key concepts of ensemble learning.</h3></div><div class="pn s"><p class="bf b po bh md pg ph pi pj pk pl dx">towardsdatascience.com</p></div></div><div class="pp s"><div class="wn s pr ps pt pp pu jr pc"></div></div></div></a></div></div></div></section></div></div></article><div class="sa ge et qc v yg ev qa qe" data-test-id="post-sidebar"><div class="n p"><div class="au av aw ax ay az ba v"><div class="qf n ao"><div class="acp"><div><div class="qg s"><div class="jl s"><a href="https://medium.com/@joseph.rocca?source=post_sidebar--------------------------post_sidebar--------------" class="ea eb ce cf cg ch ci cj ck bq ec ed cl ee ef" rel="noopener follow"><h2 class="bf kx dn bh gl hj gf">Joseph Rocca</h2></a></div><div class="qh s"><p class="bf b bg bh dx">Data Scientist at Teads. Towards Data Science editorial associate. Mathematics instructor at UTC. <a href="http://www.linkedin.com/in/joseph-rocca-b01365158" class="ea eb ce cf cg ch ci cj ck bq cl ee ef jx gg" rel="noopener follow">www.linkedin.com/in/joseph-rocca-b01365158</a></p></div><div class="qi n"><button class="bf b bg bh qj bj qk ql ff qm fi bq br bs qn fl bw bx by bz ca cb">Follow</button><div class="in s"><div><div><div class="ca" role="tooltip" aria-hidden="false" aria-describedby="448" aria-labelledby="448"><div class="s"><button class="bf b bg bh qj cj qk ql ff qm fi bq br bs qn fl bw bx by bz ca cb" aria-label="Subscribe"><svg width="38" height="38" viewBox="0 0 38 38" fill="none" class="yf qp qq"><rect x="26.25" y="9.25" width="0.5" height="6.5" rx="0.25" stroke-width="0.5"></rect><rect x="29.75" y="12.25" width="0.5" height="6.5" rx="0.25" transform="rotate(90 29.75 12.25)" stroke-width="0.5"></rect><path d="M19.5 12.5h-7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1v-5" stroke-linecap="round"></path><path d="M11.5 14.5L19 20l4-3" stroke-linecap="round"></path></svg></button></div></div></div></div></div></div></div><div class="as rb s at"><div class="ace db s"><span class="acb sx"><span class="bf b po bh dx">Joseph Rocca Follows</span></span><ul class="pm"><li class="acc acd"><div class="acf s"><a href="https://medium.com/@chintan.t93?source=blogrolls_sidebar-----f70510919f73-----------------------------------" rel="noopener follow"><img alt="Chintan Trivedi" class="s im acg ach" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/26GsssxyFW5zCdc9DAcBHnA.jpeg" width="20" height="20"></a></div><section class="gf"><span class="ack sx s"><a href="https://medium.com/@chintan.t93?source=blogrolls_sidebar-----f70510919f73-----------------------------------" class="ea eb ce cf cg ch ci cj ck bq ec ed cl ee ef" rel="noopener follow"><div><div class="ca" role="tooltip" aria-hidden="false" aria-describedby="258" aria-labelledby="258"><h4 class="bf b po bh md aci ph pi wv pk pl dx acj">Chintan Trivedi</h4></div></div></a></span></section></li><li class="acc acd"><div class="acf s"><a href="https://concerningphysicsandmath.com/?source=blogrolls_sidebar-----f70510919f73-----------------------------------" rel="noopener follow"><img alt="Marco Tavora Ph.D." class="s im acg ach" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1tnWgl7SMgsQPyjvTSRuPaw.jpeg" width="20" height="20"></a></div><section class="gf"><span class="ack sx s"><a href="https://concerningphysicsandmath.com/?source=blogrolls_sidebar-----f70510919f73-----------------------------------" class="ea eb ce cf cg ch ci cj ck bq ec ed cl ee ef" rel="noopener follow"><div><div class="ca" role="tooltip" aria-hidden="false" aria-describedby="259" aria-labelledby="259"><h4 class="bf b po bh md aci ph pi wv pk pl dx acj">Marco Tavora Ph.D.</h4></div></div></a></span></section></li><li class="acc acd"><div class="acf s"><a href="https://medium.com/@brettelizabethberry?source=blogrolls_sidebar-----f70510919f73-----------------------------------" rel="noopener follow"><img alt="Brett Berry" class="s im acg ach" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1XzXIQJBSafo-dql5pQsOdw2x.jpeg" width="20" height="20"></a></div><section class="gf"><span class="ack sx s"><a href="https://medium.com/@brettelizabethberry?source=blogrolls_sidebar-----f70510919f73-----------------------------------" class="ea eb ce cf cg ch ci cj ck bq ec ed cl ee ef" rel="noopener follow"><div><div class="ca" role="tooltip" aria-hidden="false" aria-describedby="260" aria-labelledby="260"><h4 class="bf b po bh md aci ph pi wv pk pl dx acj">Brett Berry</h4></div></div></a></span></section></li><li class="acc acd"><div class="acf s"><a href="https://medium.com/@baptiste.rocca?source=blogrolls_sidebar-----f70510919f73-----------------------------------" rel="noopener follow"><img alt="Baptiste Rocca" class="s im acg ach" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1GMEE-JrFBerusVnk-iKbZw.jpeg" width="20" height="20"></a></div><section class="gf"><span class="ack sx s"><a href="https://medium.com/@baptiste.rocca?source=blogrolls_sidebar-----f70510919f73-----------------------------------" class="ea eb ce cf cg ch ci cj ck bq ec ed cl ee ef" rel="noopener follow"><div><div class="ca" role="tooltip" aria-hidden="false" aria-describedby="261" aria-labelledby="261"><h4 class="bf b po bh md aci ph pi wv pk pl dx acj">Baptiste Rocca</h4></div></div></a></span></section></li></ul><p class="bf b po bh dx"><a href="https://towardsdatascience.com/@joseph.rocca/following?source=blogrolls_sidebar-----f70510919f73-----------------------------------" class="ea eb ce cf cg ch ci cj ck bq ec ed cl ee ef" rel="noopener follow">See all (7)</a></p></div></div><div class="ra rb v n o bc rc"><div class="ng n"><div class="n o bc"><div class="at rd re rf rg rh ri"><div class=""><button class="cj rj rk rl en rm rn ri r ro rp"><svg width="29" height="29" aria-label="clap"><g fill-rule="evenodd"><path d="M13.74 1l.76 2.97.76-2.97zM16.82 4.78l1.84-2.56-1.43-.47zM10.38 2.22l1.84 2.56-.41-3.03zM22.38 22.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M9.1 22.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L6.1 15.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L6.4 11.26l-1.18-1.18a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L11.96 14a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L8.43 9.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L20.63 15c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM13 6.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 23 23.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div><div class="s rq rr rs rt ru rv rw"><div class="rx"><p class="bf b bg bh dx"><button class="ea eb ce cf cg ch ci cj ck bq ec ed cl ee ef">7.9K </button></p></div></div></div></div><div class="ry ng s"><div class="n"><button class="en rk cj"><div class="n o bc"><div class="n o"><div><div class="ca" role="tooltip" aria-hidden="false" aria-describedby="225" aria-labelledby="225"><svg width="25" height="25" aria-label="responses" class="rz sa en rp"><path d="M19.07 21.12a6.33 6.33 0 0 1-3.53-1.1 7.8 7.8 0 0 1-.7-.52c-.77.21-1.57.32-2.38.32-4.67 0-8.46-3.5-8.46-7.8C4 7.7 7.79 4.2 12.46 4.2c4.66 0 8.46 3.5 8.46 7.8 0 2.06-.85 3.99-2.4 5.45a6.28 6.28 0 0 0 1.14 2.59c.15.21.17.48.06.7a.69.69 0 0 1-.62.38h-.03zm0-1v.5l.03-.5h-.03zm-3.92-1.64l.21.2a6.09 6.09 0 0 0 3.24 1.54 7.14 7.14 0 0 1-.83-1.84 5.15 5.15 0 0 1-.16-.75 2.4 2.4 0 0 1-.02-.29v-.23l.18-.15a6.6 6.6 0 0 0 2.3-4.96c0-3.82-3.4-6.93-7.6-6.93-4.19 0-7.6 3.11-7.6 6.93 0 3.83 3.41 6.94 7.6 6.94.83 0 1.64-.12 2.41-.35l.28-.08z" fill-rule="evenodd"></path></svg></div></div><p class="bf b bg bh dx"><span class="sb sa">71</span></p></div></div></button></div></div><div class="sc s"><div><div class="ca" role="tooltip" aria-hidden="false" aria-describedby="226" aria-labelledby="226"><div><div class="ca" role="tooltip" aria-hidden="false" aria-describedby="227" aria-labelledby="227"><div class="ca v" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="ea xw ce cf cg ch ci cj ck bq"><svg width="25" height="25" viewBox="0 0 25 25" fill="none" class="abw"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="#292929"></path></svg></button></div></div></div></div></div></div></div></div></div></div></div></div></div><div class="sa acp pw et px ik py pz qa qb"></div><div class="acn et eu" style="right: 600px;"><div class="es ge et qc v acy ev qa qe"><div class="n p"><div class="au av aw ax ay az ba v"><div class="qf n ao"><div class="ge"><div class="s"><div class="s"><p class="bf b bg bh dx"><strong class="kx">Related</strong></p><div class="tx n ao acq"><div class="v fa"><a rel="noopener follow" href="https://towardsdatascience.com/what-it-means-for-something-to-be-learnable-pac-learnability-c84de9c061ad?source=read_next_recirc---------0---------------------29cea3cf_5ba1_4b12_bf88_fe6a42e76146----------"><div class="acs ry n ao p"><div class="act s"><span class="bf b po acu acv dx acb"><p class="bf b po bh hj"><div class="ag ph md">Data Science</div></p></span></div><div class="n bc"><img alt="" class="sw sr" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/0hO3ZEc2_TsOexXc3.jpeg" role="presentation" width="58" height="58"><div class="n ao"><div class="bf b po bh md acw ph pi acx pk pl hj"><div class="acr">Defining Learnability with the PAC Framework</div></div><div class="bf b po bh md aci ph pi wv pk pl dx"><div class="acr">In machine learning we often say something is “learnable”. What does that really mean? Read on to find out.</div></div></div></div></div></a></div><div class="v fa"><a href="https://mandowara.medium.com/why-there-is-need-of-activation-function-in-neurons-2346a0ddb861?source=read_next_recirc---------1---------------------29cea3cf_5ba1_4b12_bf88_fe6a42e76146----------" rel="noopener follow"><div class="acs ry n ao p"><div class="act s"><span class="bf b po acu acv dx acb"><p class="bf b po bh hj"><div class="ag ph md"></div></p></span></div><div class="n bc"><img alt="Why there is need activation function in each neurons?" class="sw sr" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1NptQMUfBt8MUI2mdbBI8xw.png" width="58" height="58"><div class="n ao"><div class="bf b po bh md acw ph pi acx pk pl hj"><div class="acr">Why there is need of activation function in neurons?</div></div></div></div></div></a></div><div class="v fa"><a rel="noopener follow" href="https://towardsdatascience.com/developing-the-concepts-of-information-and-entropy-from-scratch-11faca089d44?source=read_next_recirc---------2---------------------29cea3cf_5ba1_4b12_bf88_fe6a42e76146----------"><div class="acs ry n ao p"><div class="act s"><span class="bf b po acu acv dx acb"><p class="bf b po bh hj"><div class="ag ph md">Data Science</div></p></span></div><div class="n bc"><img alt="" class="sw sr" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/0OIevYq97aAc-1_9F.jpeg" role="presentation" width="58" height="58"><div class="n ao"><div class="bf b po bh md acw ph pi acx pk pl hj"><div class="acr">Developing the Concepts of Information and Entropy From Scratch</div></div></div></div></div></a></div><div class="v fa"><a rel="noopener follow" href="https://towardsdatascience.com/an-explainer-on-tree-ensemble-layer-b7e445621a3f?source=read_next_recirc---------3---------------------29cea3cf_5ba1_4b12_bf88_fe6a42e76146----------"><div class="acs ry n ao p"><div class="act s"><span class="bf b po acu acv dx acb"><p class="bf b po bh hj"><div class="ag ph md">Machine Learning</div></p></span></div><div class="n bc"><img alt="" class="sw sr" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/0b4nu7g0XqSOYtyoq.jpeg" role="presentation" width="58" height="58"><div class="n ao"><div class="bf b po bh md acw ph pi acx pk pl hj"><div class="acr">An explainer on Tree Ensemble Layer</div></div><div class="bf b po bh md aci ph pi wv pk pl dx"><div class="acr">A new type of neural network layer with the benefits of decision trees.</div></div></div></div></div></a></div></div></div></div></div></div></div></div></div></div><div><div class="sd ja n ao p"><div class="n p"><div class="au av aw ax ay gj ba v"><div class="n cy"></div><div class="n o cy"></div><div class="yh yi pm yj tc yk yv"><div class="ym s"><h2 class="bf kx mo hn la mq hq ld hr ht lh hu hw ll hx hz lp hj">Sign up for The Variable</h2></div><div class="yn s"><h3 class="bf b po bh hj">By Towards Data Science</h3></div><div class="nc yo s"><p class="bf b yp te yq th vq tk vs tn vu tq hj">Every
 Thursday, the Variable delivers the very best of Towards Data Science: 
from hands-on tutorials and cutting-edge research to original features 
you don't want to miss.&nbsp;<a href="https://medium.com/towards-data-science/newsletters/the-variable?source=newsletter_v3_promo--------------------------newsletter_v3_promo--------------" class="ea eb ce cf cg ch ci cj ck bq cl ee ef jx" rel="noopener follow">Take a look.</a></p></div><div class="n cy"><div class="yr s ys"><button class="bf b dn do qj yt qk ql ff qm fi bq br bs qn fl bw bx by bz ca cb"><div class="yu n o p"><span class="yf" aria-hidden="true"><svg width="38" height="38" viewBox="0 0 38 38" fill="none"><rect x="26.25" y="9.25" width="0.5" height="6.5" rx="0.25" stroke-width="0.5"></rect><rect x="29.75" y="12.25" width="0.5" height="6.5" rx="0.25" transform="rotate(90 29.75 12.25)" stroke-width="0.5"></rect><path d="M19.5 12.5h-7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1v-5" stroke-linecap="round"></path><path d="M11.5 14.5L19 20l4-3" stroke-linecap="round"></path></svg></span>Get this newsletter</div></button></div><div class="abx aby s"><p class="bf b po bh hj">Emails will be sent to mydev324@gmail.com.<span><a href="https://medium.com/m/signin?operation=login&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-variational-autoencoders-vaes-f70510919f73&amp;collection=Towards+Data+Science&amp;collectionId=7f60cf5620c9&amp;newsletterV3=The+Variable&amp;newsletterV3Id=d6fe9076899&amp;source=newsletter_v3_promo--------------------------newsletter_v3_promo----------d6fe9076899----" class="ea eb ce cf cg ch ci cj ck bq ec ed cl ee ef" rel="noopener follow"><button class="ea eb ce cf cg ch ci cj ck bq cl ee ef jx s" target="_blank">Not you?</button></a></span></p></div></div></div><div class="sf s"><ul class="cj ck"><li class="ca sv sw sx"><a href="https://towardsdatascience.com/tagged/machine-learning" class="bf b po sy dx sz ta cb s qt">Machine Learning</a></li><li class="ca sv sw sx"><a href="https://towardsdatascience.com/tagged/data-science" class="bf b po sy dx sz ta cb s qt">Data Science</a></li><li class="ca sv sw sx"><a href="https://towardsdatascience.com/tagged/deep-learning" class="bf b po sy dx sz ta cb s qt">Deep Learning</a></li><li class="ca sv sw sx"><a href="https://towardsdatascience.com/tagged/artificial-intelligence" class="bf b po sy dx sz ta cb s qt">Artificial Intelligence</a></li><li class="ca sv sw sx"><a href="https://towardsdatascience.com/tagged/towards-data-science" class="bf b po sy dx sz ta cb s qt">Towards Data Science</a></li></ul></div><div class="sf s"><div class="n cp iy"><div class="n o bc"><div class="sg s"><span class="s sh si sj e d"><div class="n o bc"><div class="at rd re rf rg rh ri"><div class=""><button class="cj rj rk rl en rm rn ri r ro rp"><svg width="25" height="25" viewBox="0 0 25 25" aria-label="clap"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div><div class="s rq rr rs rt sk sl sm"><div class="at sn rx"><p class="bf b bg bh hj"><button class="ea eb ce cf cg ch ci cj ck bq ec ed cl ee ef">7.9K<span class="s h g f so sp">&nbsp;claps</span></button><span class="s h g f so sp"></span></p></div></div></div></span><span class="s h g f so sp"><div class="n o bc"><div class="at rd re rf rg rh ri"><div class=""><button class="cj rj rk rl en rm rn ri r ro rp"><svg width="25" height="25" viewBox="0 0 25 25" aria-label="clap"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div><div class="s rq rr rs rt sk sl sm"><div class="rx"><p class="bf b bg bh dx"><button class="ea eb ce cf cg ch ci cj ck bq ec ed cl ee ef">7.9K </button></p></div></div></div></span></div><div class="sq n"><div class="n"><button class="en rk cj"><div class="n o bc"><div class="n o"><div><div class="ca" role="tooltip" aria-hidden="false" aria-describedby="228" aria-labelledby="228"><svg width="29" height="29" aria-label="responses" class="rz sa en rp sr"><path d="M21.27 20.06a9.04 9.04 0 0 0 2.75-6.68C24.02 8.21 19.67 4 14.1 4S4 8.21 4 13.38c0 5.18 4.53 9.39 10.1 9.39 1 0 2-.14 2.95-.41.28.25.6.49.92.7a7.46 7.46 0 0 0 4.19 1.3c.27 0 .5-.13.6-.35a.63.63 0 0 0-.05-.65 8.08 8.08 0 0 1-1.29-2.58 5.42 5.42 0 0 1-.15-.75zm-3.85 1.32l-.08-.28-.4.12a9.72 9.72 0 0 1-2.84.43c-4.96 0-9-3.71-9-8.27 0-4.55 4.04-8.26 9-8.26 4.95 0 8.77 3.71 8.77 8.27 0 2.25-.75 4.35-2.5 5.92l-.24.21v.32a5.59 5.59 0 0 0 .21 1.29c.19.7.49 1.4.89 2.08a6.43 6.43 0 0 1-2.67-1.06c-.34-.22-.88-.48-1.16-.74z"></path></svg></div></div><p class="bf b bg bh hj"><span class="ss st sa">71</span></p></div></div></button></div></div></div><div class="n o"><div class="yb s z"><div><div class="ca" role="tooltip" aria-hidden="false" aria-describedby="229" aria-labelledby="229"><button class="ea eb ce cf cg ch ci cj ck bq ec ed cl ee ef" aria-label="Share on twitter"><svg width="30" height="30" viewBox="0 0 30 30" fill="none" class="xw yc"><path fill-rule="evenodd" clip-rule="evenodd" d="M15 27a12 12 0 1 0 0-24 12 12 0 0 0 0 24zm4.95-16.17a2.67 2.67 0 0 0-4.6 1.84c0 .2.03.41.05.62a7.6 7.6 0 0 1-5.49-2.82 3 3 0 0 0-.38 1.34c.02.94.49 1.76 1.2 2.23a2.53 2.53 0 0 1-1.2-.33v.04c0 1.28.92 2.36 2.14 2.62-.23.05-.46.08-.71.1l-.21-.02-.27-.03a2.68 2.68 0 0 0 2.48 1.86A5.64 5.64 0 0 1 9 19.38a7.62 7.62 0 0 0 4.1 1.19c4.9 0 7.58-4.07 7.57-7.58v-.39c.52-.36.97-.83 1.33-1.38-.48.23-1 .37-1.53.43.56-.33.96-.86 1.15-1.48-.5.31-1.07.53-1.67.66z" fill="#292929"></path></svg></button></div></div></div><div class="yb s z"><div><div class="ca" role="tooltip" aria-hidden="false" aria-describedby="230" aria-labelledby="230"><button class="ea eb ce cf cg ch ci cj ck bq ec ed cl ee ef" aria-label="Share on facebook"><svg width="30" height="30" viewBox="0 0 30 30" fill="none" class="xw yc"><path fill-rule="evenodd" clip-rule="evenodd" d="M15 27a12 12 0 1 0 0-24 12 12 0 0 0 0 24zm-1.23-6.03V15.6H12v-2.15h1.77v-1.6C13.77 10 14.85 9 16.42 9c.75 0 1.4.06 1.58.08v1.93h-1.09c-.85 0-1.02.43-1.02 1.05v1.38h2.04l-.27 2.15H15.9V21l-2.13-.03z" fill="#292929"></path></svg></button></div></div></div><div class="yb s z"><div><div class="ca" role="tooltip" aria-hidden="false" aria-describedby="231" aria-labelledby="231"><button class="ea eb ce cf cg ch ci cj ck bq ec ed cl ee ef" aria-label="Share on linkedin"><svg width="30" height="30" viewBox="0 0 30 30" fill="none" class="xw yc"><path fill-rule="evenodd" clip-rule="evenodd" d="M27 15a12 12 0 1 1-24 0 12 12 0 0 1 24 0zm-14.61 5v-7.42h-2.26V20h2.26zm-1.13-8.44c.79 0 1.28-.57 1.28-1.28-.02-.73-.5-1.28-1.26-1.28-.78 0-1.28.55-1.28 1.28 0 .71.49 1.28 1.25 1.28h.01zM15.88 20h-2.5s.04-6.5 0-7.17h2.5v1.02l-.02.02h.02v-.02a2.5 2.5 0 0 1 2.25-1.18c1.64 0 2.87 1.02 2.87 3.22V20h-2.5v-3.83c0-.97-.36-1.62-1.26-1.62-.69 0-1.1.44-1.28.87-.06.15-.08.36-.08.58v4z" fill="#292929"></path></svg></button></div></div></div><div class="s z"><div><div class="ca" role="tooltip" aria-hidden="false" aria-describedby="232" aria-labelledby="232"><button class="ea eb ce cf cg ch ci cj ck bq ec ed cl ee ef"><svg width="30" height="30" viewBox="0 0 30 30" fill="none" class="xw yc"><path fill-rule="evenodd" clip-rule="evenodd" d="M15 27a12 12 0 1 0 0-24 12 12 0 0 0 0 24zM9.29 16.28c-.2.36-.29.75-.29 1.17a2.57 2.57 0 0 0 .78 1.84l1.01.96c.53.5 1.17.75 1.92.75s1.38-.25 1.9-.75l1.2-1.15.75-.71.51-.5a2.51 2.51 0 0 0 .72-2.34.7.7 0 0 0-.03-.18 2.74 2.74 0 0 0-.23-.5v-.02l-.08-.14-.02-.03-.02-.01a.33.33 0 0 0-.07-.1c0-.02-.01-.03-.03-.05a.2.2 0 0 0-.03-.03l-.03-.04v-.01l-.02-.03-.04-.03a.85.85 0 0 1-.13-.13l-.43-.42-.06.06-.9.84-.05.09a.26.26 0 0 0-.03.1l.37.38c.04.03.08.07.1.11l.01.01.01.03.02.01.04.1.03.04.06.1v.02l.01.02c.03.1.05.2.05.33a1 1 0 0 1-.12.49c-.07.13-.15.22-.22.29l-.88.85-.61.57-.95.92c-.22.2-.5.3-.82.3-.31 0-.58-.1-.8-.3l-.98-.96a1.15 1.15 0 0 1-.3-.42 1.4 1.4 0 0 1-.04-.35c0-.1.01-.2.04-.3a1 1 0 0 1 .3-.49l1.5-1.46v-.24c0-.21 0-.42.04-.6a3.5 3.5 0 0 1 .92-1.72c-.41.1-.78.32-1.11.62l-.01.02-.01.01-2.46 2.33c-.2.21-.35.4-.44.6h-.02c0 .02 0 .02-.02.02v.02l-.01.01zm3.92-1.8a1.83 1.83 0 0 0 .02.97c0 .06 0 .13.02.19.06.17.14.34.22.5v.02l.06.12.02.03.01.02.08.1c0 .02.02.03.04.05l.08.1h.01c0 .01 0 .03.02.03l.14.14.43.41.08-.06.88-.84.05-.09.03-.1-.36-.37a.4.4 0 0 1-.12-.13v-.02l-.02-.02-.05-.09-.04-.04-.04-.1v-.02l-.02-.02a1.16 1.16 0 0 1 .06-.82c.09-.14.16-.24.23-.3l.9-.85.6-.58.93-.92c.23-.2.5-.3.82-.3a1.2 1.2 0 0 1 .82.3l1 .96c.13.15.23.29.28.42a1.43 1.43 0 0 1 0 .66c-.03.17-.12.33-.26.48l-1.54 1.45.02.25a3.28 3.28 0 0 1-.96 2.32 2.5 2.5 0 0 0 1.1-.62l.01-.01 2.46-2.34c.19-.2.35-.4.46-.6l.02-.02v-.02h.01a2.45 2.45 0 0 0 .21-1.82 2.53 2.53 0 0 0-.7-1.19l-1-.96a2.68 2.68 0 0 0-1.91-.75c-.75 0-1.38.25-1.9.76l-1.2 1.14-.76.72-.5.49c-.4.37-.64.83-.74 1.37z" fill="#292929"></path></svg></button></div></div></div><div class="iz s z"><div><div class="ca" role="tooltip" aria-hidden="false" aria-describedby="233" aria-labelledby="233"><div><div class="ca" role="tooltip" aria-hidden="false" aria-describedby="234" aria-labelledby="234"><div class="ca v" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="ea xw ce cf cg ch ci cj ck bq"><svg width="25" height="25" viewBox="0 0 25 25" fill="none" class="abw"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="#292929"></path></svg></button></div></div></div></div></div></div><div><div class="ca" role="tooltip" aria-hidden="false" aria-describedby="235" aria-labelledby="235"><div class="s"><div class="ca" aria-hidden="false"><button class="ea eb ce cf cg ch ci cj ck bq ec ed cl ee ef" aria-label="More options"><svg class="overflow-dots-filled-25px_svg__svgIcon-use" width="25" height="25"><path d="M5 12.5c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41A1.93 1.93 0 0 0 7 10.5c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41zm5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59-.39.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59.56 0 1.03-.2 1.42-.59.39-.39.58-.86.58-1.41 0-.55-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59-.39.39-.58.86-.58 1.41z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div><div><div class="n p"><div class="au av aw ax ay gj ba v"></div></div><div class="s iy"><div class="tb ra s tc"><div class="n p"><div class="au av aw ax ay gj ba v"><div class="n o cp"><h2 class="bf kx td te wq la tg th wr ld tj tk ws lh tm tn wt ll tp tq wu lp md ph pi wv pk pl hj"><a href="https://towardsdatascience.com/?source=follow_footer-----f70510919f73-----------------------------------" class="ea eb ce cf cg ch ci cj ck bq ec ed cl ee ef" rel="noopener follow">More from Towards Data Science</a></h2><div class="ca" aria-hidden="false" aria-describedby="collectionFollowPopover" aria-labelledby="collectionFollowPopover"><button class="bf b bg bh qj bj qk ql ff qm fi bq br bs qn fl bw bx by bz ca cb" aria-controls="collectionFollowPopover" aria-expanded="false"><div class="n bc">Follow</div></button></div></div><div class="jm ww s"><p class="bf b bg bh dx">Your home for data science. A Medium publication sharing concepts, ideas and codes.</p></div></div></div></div></div><div class="ts s tc iy"><div class="n p"><div class="jb jd jf tt tu jr ba v"><div class="tv sd s"><div class="jr s ju"><a href="https://towardsdatascience.com/?source=follow_footer-----f70510919f73-----------------------------------" class="bf b bg bh qj bj qk ql ff qm fi bq br bs qn fl bw bx by bz ca cb" rel="noopener follow">Read more from Towards Data Science</a></div></div></div></div></div><div class="s fv iy"><div class="n p"><div class="au av aw ax ay az ba v"></div></div></div></div></div></div><div class="yw s yx yy"><div class="n p"><div class="au av aw ax ay az ba v"><div class="n ao"><div class="n yz za zb zc zd ze zf zg zh zi cp"><a href="https://medium.com/?source=post_page-----f70510919f73-----------------------------------" aria-label="Go to homepage" class="ea eb ce cf cg ch ci cj ck bq zj zk cl zl zm" rel="noopener follow"><svg viewBox="0 0 3940 610" class="qk zn"><path d="M594.79 308.2c0 163.76-131.85 296.52-294.5 296.52S5.8 472 5.8 308.2 137.65 11.69 300.29 11.69s294.5 132.75 294.5 296.51M917.86 308.2c0 154.16-65.93 279.12-147.25 279.12s-147.25-125-147.25-279.12S689.29 29.08 770.61 29.08s147.25 125 147.25 279.12M1050 308.2c0 138.12-23.19 250.08-51.79 250.08s-51.79-112-51.79-250.08 23.19-250.08 51.8-250.08S1050 170.09 1050 308.2M1862.77 37.4l.82-.18v-6.35h-167.48l-155.51 365.5-155.51-365.5h-180.48v6.35l.81.18c30.57 6.9 46.09 17.19 46.09 54.3v434.45c0 37.11-15.58 47.4-46.15 54.3l-.81.18V587H1327v-6.35l-.81-.18c-30.57-6.9-46.09-17.19-46.09-54.3V116.9L1479.87 587h11.33l205.59-483.21V536.9c-2.62 29.31-18 38.36-45.68 44.61l-.82.19v6.3h213.3v-6.3l-.82-.19c-27.71-6.25-43.46-15.3-46.08-44.61l-.14-445.2h.14c0-37.11 15.52-47.4 46.08-54.3m97.43 287.8c3.49-78.06 31.52-134.4 78.56-135.37 14.51.24 26.68 5 36.14 14.16 20.1 19.51 29.55 60.28 28.09 121.21zm-2.11 22h250v-1.05c-.71-59.69-18-106.12-51.34-138-28.82-27.55-71.49-42.71-116.31-42.71h-1c-23.26 0-51.79 5.64-72.09 15.86-23.11 10.7-43.49 26.7-60.45 47.7-27.3 33.83-43.84 79.55-47.86 130.93-.13 1.54-.24 3.08-.35 4.62s-.18 2.92-.25 4.39a332.64 332.64 0 0 0-.36 21.69C1860.79 507 1923.65 600 2035.3 600c98 0 155.07-71.64 169.3-167.8l-7.19-2.53c-25 51.68-69.9 83-121 79.18-69.76-5.22-123.2-75.95-118.35-161.63m532.69 157.68c-8.2 19.45-25.31 30.15-48.24 30.15s-43.89-15.74-58.78-44.34c-16-30.7-24.42-74.1-24.42-125.51 0-107 33.28-176.21 84.79-176.21 21.57 0 38.55 10.7 46.65 29.37zm165.84 76.28c-30.57-7.23-46.09-18-46.09-57V5.28L2424.77 60v6.7l1.14-.09c25.62-2.07 43 1.47 53.09 10.79 7.9 7.3 11.75 18.5 11.75 34.26v71.14c-18.31-11.69-40.09-17.38-66.52-17.38-53.6 0-102.59 22.57-137.92 63.56-36.83 42.72-56.3 101.1-56.3 168.81C2230 518.72 2289.53 600 2378.13 600c51.83 0 93.53-28.4 112.62-76.3V588h166.65v-6.66zm159.29-505.33c0-37.76-28.47-66.24-66.24-66.24-37.59 0-67 29.1-67 66.24s29.44 66.24 67 66.24c37.77 0 66.24-28.48 66.24-66.24m43.84 505.33c-30.57-7.23-46.09-18-46.09-57h-.13V166.65l-166.66 47.85v6.5l1 .09c36.06 3.21 45.93 15.63 45.93 57.77V588h166.8v-6.66zm427.05 0c-30.57-7.23-46.09-18-46.09-57V166.65L3082 212.92v6.52l.94.1c29.48 3.1 38 16.23 38 58.56v226c-9.83 19.45-28.27 31-50.61 31.78-36.23 0-56.18-24.47-56.18-68.9V166.66l-166.66 47.85V221l1 .09c36.06 3.2 45.94 15.62 45.94 57.77v191.27a214.48 214.48 0 0 0 3.47 39.82l3 13.05c14.11 50.56 51.08 77 109 77 49.06 0 92.06-30.37 111-77.89v66h166.66v-6.66zM3934.2 588v-6.67l-.81-.19c-33.17-7.65-46.09-22.07-46.09-51.43v-243.2c0-75.83-42.59-121.09-113.93-121.09-52 0-95.85 30.05-112.73 76.86-13.41-49.6-52-76.86-109.06-76.86-50.12 0-89.4 26.45-106.25 71.13v-69.87l-166.66 45.89v6.54l1 .09c35.63 3.16 45.93 15.94 45.93 57V588h155.5v-6.66l-.82-.2c-26.46-6.22-35-17.56-35-46.66V255.72c7-16.35 21.11-35.72 49-35.72 34.64 0 52.2 24 52.2 71.28V588h155.54v-6.66l-.82-.2c-26.46-6.22-35-17.56-35-46.66v-248a160.45 160.45 0 0 0-2.2-27.68c7.42-17.77 22.34-38.8 51.37-38.8 35.13 0 52.2 23.31 52.2 71.28V588z"></path></svg></a><div class="zo zp zq zr zs zt n cp"><p class="bf b dn do zu"><a href="https://medium.com/about?autoplay=1&amp;source=post_page-----f70510919f73-----------------------------------" class="ea eb ce cf cg ch ci cj ck bq vx cl zl zm" rel="noopener follow">About</a></p><p class="bf b dn do zu"><a href="https://medium.com/new-story?source=post_page-----f70510919f73-----------------------------------" class="ea eb ce cf cg ch ci cj ck bq vx cl zl zm" rel="noopener follow">Write</a></p><p class="bf b dn do zu"><a href="https://help.medium.com/hc/en-us?source=post_page-----f70510919f73-----------------------------------" class="ea eb ce cf cg ch ci cj ck bq vx cl zl zm" rel="noopener follow">Help</a></p><p class="bf b dn do zu"><a href="https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f70510919f73-----------------------------------" class="ea eb ce cf cg ch ci cj ck bq vx cl zl zm" rel="noopener follow">Legal</a></p></div></div><div class="aq zv zw zx cr"><p class="bf b dn do zy">Get the Medium app</p></div><div class="aq zz xn cr aba"><div class="be s"><a href="https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&amp;mt=8&amp;ct=post_page&amp;source=post_page-----f70510919f73-----------------------------------" class="ea eb ce cf cg ch ci cj ck bq zj zk cl zl zm" rel="noopener follow"><img alt="A button that says 'Download on the App Store', and if clicked it will lead you to the iOS App store" class="" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1Crl55Tm6yDNMoucPo1tvDg.png" width="135" height="41"></a></div><div class="s"><a href="https://play.google.com/store/apps/details?id=com.medium.reader&amp;source=post_page-----f70510919f73-----------------------------------" class="ea eb ce cf cg ch ci cj ck bq zj zk cl zl zm" rel="noopener follow"><img alt="A button that says 'Get it on, Google Play', and if clicked it will lead you to the Google Play store" class="" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/1W_RAPQ62h0em559zluJLdQ.png" width="135" height="41"></a></div></div></div></div></div></div></div></div></div><script>window.__BUILD_ID__="main-20211210-232626-79d0eb6cc9"</script><script>window.__GRAPHQL_URI__ = "https://towardsdatascience.com/_/graphql"</script><script>window.__PRELOADED_STATE__ = {"algolia":{"queries":{}},"auroraPage":{"isAuroraPageEnabled":true},"bookReader":{"assets":{},"reader":{"currentAsset":null,"currentGFI":null,"settingsPanelIsOpen":false,"settings":{"fontFamily":"CHARTER","fontScale":"M","publisherStyling":false,"textAlignment":"start","theme":"White","lineSpacing":0,"wordSpacing":0,"letterSpacing":0},"internalNavCounter":0,"currentSelection":null}},"cache":{"experimentGroupSet":true,"reason":"","group":"enabled","tags":["group-edgeCachePosts","post-f70510919f73","user-b17ebd108358","collection-7f60cf5620c9"],"serverVariantState":"44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a","middlewareEnabled":true,"cacheStatus":"DYNAMIC","shouldUseCache":true,"vary":[]},"client":{"hydrated":false,"isUs":false,"isNativeMedium":false,"isSafariMobile":false,"isSafari":false,"routingEntity":{"type":"COLLECTION","id":"7f60cf5620c9","explicit":true},"viewerIsBot":false},"debug":{"requestId":"0b4f7efd-fdd3-4339-a23b-1576b1272fc2","hybridDevServices":[],"showBookReaderDebugger":false,"originalSpanCarrier":{"ot-tracer-spanid":"33f4e24e267253c5","ot-tracer-traceid":"70964e2cfa58a589","ot-tracer-sampled":"true"}},"multiVote":{"clapsPerPost":{}},"navigation":{"branch":{"show":null,"hasRendered":null,"blockedByCTA":false},"hideGoogleOneTap":false,"hasRenderedGoogleOneTap":null,"hasRenderedAlternateUserBanner":null,"currentLocation":"https:\u002F\u002Ftowardsdatascience.com\u002Funderstanding-variational-autoencoders-vaes-f70510919f73","host":"towardsdatascience.com","hostname":"towardsdatascience.com","referrer":"","hasSetReferrer":false,"susiModal":{"step":null,"operation":"register"},"postRead":false,"queryString":"","currentHash":""},"tracing":{},"userOnboarding":{"showFirstBookPurchaseTooltip":false},"config":{"nodeEnv":"production","version":"main-20211210-232626-79d0eb6cc9","isTaggedVersion":false,"isMediumDotApp":false,"isMediumDotAppVariant":false,"target":"production","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","glyphUrl":"https:\u002F\u002Fglyph.medium.com","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","lightStep":{"name":"lite-web","host":"lightstep.medium.systems","token":"ce5be895bef60919541332990ac9fef2","appVersion":"main-20211210-232626-79d0eb6cc9","disableClientReporting":true},"algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk","recaptcha3Key":"6Lf8R9wUAAAAABMI_85Wb8melS7Zj6ziuf99Yot5","datadog":{"applicationId":"6702d87d-a7e0-42fe-bbcb-95b469547ea0","clientToken":"pub853ea8d17ad6821d9f8f11861d23dfed","rumToken":"pubf9cc52896502b9413b68ba36fc0c7162","context":{"deployment":{"target":"production","tag":"main-20211210-232626-79d0eb6cc9","commit":"79d0eb6cc9a6f50e45a64cbd8b4b7b9d0bcaa8ea"}},"datacenter":"us"},"googleAnalyticsCode":"UA-24232453-2","googlePay":{"apiVersion":"2","apiVersionMinor":"0","merchantId":"BCR2DN6TV7EMTGBM","merchantName":"Medium","instanceMerchantId":"13685562959212738550"},"applePay":{"version":3},"signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"mediumOwnedAndOperatedCollectionIds":["8a9336e5bb4","b7e45b22fec3","193b68bd4fba","8d6b8a439e32","54c98c43354d","3f6ecf56618","d944778ce714","92d2092dc598","ae2a65f35510","1285ba81cada","544c7006046e","fc8964313712","40187e704f1c","88d9857e584e","7b6769f2748b","bcc38c8f6edf","cef6983b292","cb8577c9149e","444d13b52878","713d7dbc99b0","ef8e90590e66","191186aaafa0","55760f21cdc5","9dc80918cc93","bdc4052bbdba","8ccfed20cbb2"],"tierOneDomains":["medium.com","thebolditalic.com","arcdigital.media","towardsdatascience.com","uxdesign.cc","codeburst.io","psiloveyou.xyz","writingcooperative.com","entrepreneurshandbook.co","prototypr.io","betterhumans.coach.me","theascent.pub"],"topicsToFollow":["d61cf867d93f","8a146bc21b28","1eca0103fff3","4d562ee63426","aef1078a3ef5","e15e46793f8d","6158eb913466","55f1c20aba7a","3d18b94f6858","4861fee224fd","63c6f1f93ee","1d98b3a9a871","decb52b64abf","ae5d4995e225","830cded25262"],"topicToTagMappings":{"accessibility":"accessibility","addiction":"addiction","android-development":"android-development","art":"art","artificial-intelligence":"artificial-intelligence","astrology":"astrology","basic-income":"basic-income","beauty":"beauty","biotech":"biotech","blockchain":"blockchain","books":"books","business":"business","cannabis":"cannabis","cities":"cities","climate-change":"climate-change","comics":"comics","coronavirus":"coronavirus","creativity":"creativity","cryptocurrency":"cryptocurrency","culture":"culture","cybersecurity":"cybersecurity","data-science":"data-science","design":"design","digital-life":"digital-life","disability":"disability","economy":"economy","education":"education","equality":"equality","family":"family","feminism":"feminism","fiction":"fiction","film":"film","fitness":"fitness","food":"food","freelancing":"freelancing","future":"future","gadgets":"gadgets","gaming":"gaming","gun-control":"gun-control","health":"health","history":"history","humor":"humor","immigration":"immigration","ios-development":"ios-development","javascript":"javascript","justice":"justice","language":"language","leadership":"leadership","lgbtqia":"lgbtqia","lifestyle":"lifestyle","machine-learning":"machine-learning","makers":"makers","marketing":"marketing","math":"math","media":"media","mental-health":"mental-health","mindfulness":"mindfulness","money":"money","music":"music","neuroscience":"neuroscience","nonfiction":"nonfiction","outdoors":"outdoors","parenting":"parenting","pets":"pets","philosophy":"philosophy","photography":"photography","podcasts":"podcast","poetry":"poetry","politics":"politics","privacy":"privacy","product-management":"product-management","productivity":"productivity","programming":"programming","psychedelics":"psychedelics","psychology":"psychology","race":"race","relationships":"relationships","religion":"religion","remote-work":"remote-work","san-francisco":"san-francisco","science":"science","self":"self","self-driving-cars":"self-driving-cars","sexuality":"sexuality","social-media":"social-media","society":"society","software-engineering":"software-engineering","space":"space","spirituality":"spirituality","sports":"sports","startups":"startup","style":"style","technology":"technology","transportation":"transportation","travel":"travel","true-crime":"true-crime","tv":"tv","ux":"ux","venture-capital":"venture-capital","visual-design":"visual-design","work":"work","world":"world","writing":"writing"},"defaultImages":{"avatar":{"imageId":"1*dmbNkD5D-u45r44go_cf0g.png","height":150,"width":150},"orgLogo":{"imageId":"1*OMF3fSqH8t4xBJ9-6oZDZw.png","height":106,"width":545},"postLogo":{"imageId":"1*kFrc4tBFM_tCis-2Ic87WA.png","height":810,"width":1440},"postPreviewImage":{"imageId":"1*hn4v1tCaJy7cWMyb0bpNpQ.png","height":386,"width":579}},"collectionStructuredData":{"8d6b8a439e32":{"name":"Elemental","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F980\u002F1*9ygdqoKprhwuTVKUM0DLPA@2x.png","width":980,"height":159}}},"3f6ecf56618":{"name":"Forge","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F596\u002F1*uULpIlImcO5TDuBZ6lm7Lg@2x.png","width":596,"height":183}}},"ae2a65f35510":{"name":"GEN","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F264\u002F1*RdVZMdvfV3YiZTw6mX7yWA.png","width":264,"height":140}}},"88d9857e584e":{"name":"LEVEL","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*JqYMhNX6KNNb2UlqGqO2WQ.png","width":540,"height":108}}},"7b6769f2748b":{"name":"Marker","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F383\u002F1*haCUs0wF6TgOOvfoY-jEoQ@2x.png","width":383,"height":92}}},"444d13b52878":{"name":"OneZero","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*cw32fIqCbRWzwJaoQw6BUg.png","width":540,"height":123}}},"8ccfed20cbb2":{"name":"Zora","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*tZUQqRcCCZDXjjiZ4bDvgQ.png","width":540,"height":106}}}},"embeddedPostIds":{"coronavirus":"cd3010f9d81f"},"sharedCdcMessaging":{"COVID_APPLICABLE_TAG_SLUGS":[],"COVID_APPLICABLE_TOPIC_NAMES":[],"COVID_APPLICABLE_TOPIC_NAMES_FOR_TOPIC_PAGE":[],"COVID_MESSAGES":{"tierA":{"text":"For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":66,"end":73,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"tierB":{"text":"Anyone can publish on Medium per our Policies, but we don’t fact-check every story. For more info about the coronavirus, see cdc.gov.","markups":[{"start":37,"end":45,"href":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Fcategories\u002F201931128-Policies-Safety"},{"start":125,"end":132,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"paywall":{"text":"This article has been made free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":56,"end":70,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":138,"end":145,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"unbound":{"text":"This article is free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":45,"end":59,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":127,"end":134,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]}},"COVID_BANNER_POST_ID_OVERRIDE_WHITELIST":["3b31a67bff4a"]},"sharedVoteMessaging":{"TAGS":["politics","election-2020","government","us-politics","election","2020-presidential-race","trump","donald-trump","democrats","republicans","congress","republican-party","democratic-party","biden","joe-biden","maga"],"TOPICS":["politics","election"],"MESSAGE":{"text":"Find out more about the U.S. election results here.","markups":[{"start":46,"end":50,"href":"https:\u002F\u002Fcookpolitical.com\u002F2020-national-popular-vote-tracker"}]},"EXCLUDE_POSTS":["397ef29e3ca5"]},"embedPostRules":[],"recircOptions":{"v1":{"limit":3},"v2":{"limit":8}},"braintreeClientKey":"production_zjkj96jm_m56f8fqpf7ngnrd4","braintree":{"enabled":true,"merchantId":"m56f8fqpf7ngnrd4","merchantAccountId":{"usd":"AMediumCorporation_instant","eur":"amediumcorporation_EUR"},"publicKey":"cwr8xtycwgjryv82","braintreeEnvironment":"production","dashboardUrl":"https:\u002F\u002Fwww.braintreegateway.com\u002Fmerchants","gracePeriodDurationInDays":14,"mediumMembershipPlanId":{"monthly":"ce105f8c57a3","monthlyWithTrial":"d5ee3dbe3db8","yearly":"a40ad4a43185","yearlyStaff":"d74fb811198a","yearlyWithTrial":"b3bc7350e5c7"},"braintreeDiscountId":{"oneMonthFree":"MONTHS_FREE_01","threeMonthsFree":"MONTHS_FREE_03","sixMonthsFree":"MONTHS_FREE_06"},"3DSecureVersion":"2"},"paypalClientId":"AXj1G4fotC2GE8KzWX9mSxCH1wmPE3nJglf4Z2ig_amnhvlMVX87otaq58niAg9iuLktVNF_1WCMnN7v","paypal":{"host":"https:\u002F\u002Fapi.paypal.com:443","clientMode":"production","serverMode":"live","webhookId":"4G466076A0294510S","monthlyPlan":{"planId":"P-9WR0658853113943TMU5FDQA","name":"Medium Membership (Monthly) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"yearlyPlan":{"planId":"P-7N8963881P8875835MU5JOPQ","name":"Medium Membership (Annual) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"oneYearGift":{"name":"Medium Membership (1 Year, Digital Gift Code)","description":"Unlimited access to the best and brightest stories on Medium. Gift codes can be redeemed at medium.com\u002Fredeem.","price":"50.00","currency":"USD","sku":"membership-gift-1-yr"},"oldMonthlyPlan":{"planId":"P-96U02458LM656772MJZUVH2Y","name":"Medium Membership (Monthly)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"oldYearlyPlan":{"planId":"P-59P80963JF186412JJZU3SMI","name":"Medium Membership (Annual)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"monthlyPlanWithTrial":{"planId":"P-66C21969LR178604GJPVKUKY","name":"Medium Membership (Monthly) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"yearlyPlanWithTrial":{"planId":"P-6XW32684EX226940VKCT2MFA","name":"Medium Membership (Annual) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"oldMonthlyPlanNoSetupFee":{"planId":"P-4N046520HR188054PCJC7LJI","name":"Medium Membership (Monthly)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"oldYearlyPlanNoSetupFee":{"planId":"P-7A4913502Y5181304CJEJMXQ","name":"Medium Membership (Annual)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"sdkUrl":"https:\u002F\u002Fwww.paypal.com\u002Fsdk\u002Fjs"},"stripePublishableKey":"pk_live_7FReX44VnNIInZwrIIx6ghjl"},"session":{"xsrf":""}}</script><script>window.__APOLLO_STATE__ = {"ROOT_QUERY":{"__typename":"Query","meterPost({\"postId\":\"f70510919f73\",\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\",\"sk\":null,\"source\":null}})":{"__ref":"MeteringInfo:{}"},"postResult({\"id\":\"f70510919f73\"})":{"__ref":"Post:f70510919f73"}},"MeteringInfo:{}":{"__typename":"MeteringInfo","postIds":[],"maxUnlockCount":3,"unlocksRemaining":0},"User:b17ebd108358":{"id":"b17ebd108358","__typename":"User","name":"Joseph Rocca","username":"joseph.rocca","newsletterV3":{"__ref":"NewsletterV3:164289aafbe3"},"customStyleSheet":{"__ref":"CustomStyleSheet:5a2595a71917"},"isSuspended":false,"bio":"Data Scientist at Teads. Towards Data Science editorial associate. Mathematics instructor at UTC. www.linkedin.com\u002Fin\u002Fjoseph-rocca-b01365158","imageId":"1*Ede150EkH-LDgY-m-SrmLg.png","hasCompletedProfile":false,"isAuroraVisible":true,"mediumMemberAt":1609605523465,"socialStats":{"__typename":"SocialStats","followerCount":3430,"followingCount":5,"collectionFollowingCount":2},"customDomainState":null,"hasSubdomain":false,"bookAuthor":null,"isPartnerProgramEnrolled":false,"viewerEdge":{"__ref":"UserViewerEdge:userId:b17ebd108358-viewerId:2bf0e8de62c3"},"viewerIsUser":false,"homepagePostsConnection({\"paging\":{\"limit\":1}})":{"__typename":"PostConnection","posts":[{"__ref":"Post:f5622fbe1e82"}]},"postSubscribeMembershipUpsellShownAt":0,"allowNotes":true,"replyToEmailBannerShownCount":0,"twitterScreenName":"roccajo","followedCollections":2,"referredMembershipCustomHeadline":"","referredMembershipCustomBody":"","atsQualifiedAt":1612205316085},"ImageMetadata:1*ChFMdf--f5jbm-AYv6VdYA@2x.png":{"id":"1*ChFMdf--f5jbm-AYv6VdYA@2x.png","__typename":"ImageMetadata"},"CustomStyleSheet:dc5d6afeee0a":{"id":"dc5d6afeee0a","__typename":"CustomStyleSheet","global":{"__typename":"GlobalStyles","colorPalette":{"__typename":"StyleSheetColorPalette","primary":{"__typename":"ColorValue","colorPalette":{"__typename":"ColorPalette","highlightSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FFEDF4FC","point":0},{"__typename":"ColorPoint","color":"#FFE9F2FD","point":0.1},{"__typename":"ColorPoint","color":"#FFE6F1FD","point":0.2},{"__typename":"ColorPoint","color":"#FFE2EFFD","point":0.3},{"__typename":"ColorPoint","color":"#FFDFEEFD","point":0.4},{"__typename":"ColorPoint","color":"#FFDBECFE","point":0.5},{"__typename":"ColorPoint","color":"#FFD7EBFE","point":0.6},{"__typename":"ColorPoint","color":"#FFD4E9FE","point":0.7},{"__typename":"ColorPoint","color":"#FFD0E7FF","point":0.8},{"__typename":"ColorPoint","color":"#FFCCE6FF","point":0.9},{"__typename":"ColorPoint","color":"#FFC8E4FF","point":1}]},"defaultBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FF668AAA","point":0},{"__typename":"ColorPoint","color":"#FF61809D","point":0.1},{"__typename":"ColorPoint","color":"#FF5A7690","point":0.2},{"__typename":"ColorPoint","color":"#FF546C83","point":0.3},{"__typename":"ColorPoint","color":"#FF4D6275","point":0.4},{"__typename":"ColorPoint","color":"#FF455768","point":0.5},{"__typename":"ColorPoint","color":"#FF3D4C5A","point":0.6},{"__typename":"ColorPoint","color":"#FF34414C","point":0.7},{"__typename":"ColorPoint","color":"#FF2B353E","point":0.8},{"__typename":"ColorPoint","color":"#FF21282F","point":0.9},{"__typename":"ColorPoint","color":"#FF161B1F","point":1}]},"tintBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FF355876","colorPoints":[{"__typename":"ColorPoint","color":"#FF355876","point":0},{"__typename":"ColorPoint","color":"#FF4D6C88","point":0.1},{"__typename":"ColorPoint","color":"#FF637F99","point":0.2},{"__typename":"ColorPoint","color":"#FF7791A8","point":0.3},{"__typename":"ColorPoint","color":"#FF8CA2B7","point":0.4},{"__typename":"ColorPoint","color":"#FF9FB3C6","point":0.5},{"__typename":"ColorPoint","color":"#FFB2C3D4","point":0.6},{"__typename":"ColorPoint","color":"#FFC5D2E1","point":0.7},{"__typename":"ColorPoint","color":"#FFD7E2EE","point":0.8},{"__typename":"ColorPoint","color":"#FFE9F1FA","point":0.9},{"__typename":"ColorPoint","color":"#FFFBFFFF","point":1}]}}},"background":null},"fonts":{"__typename":"StyleSheetFonts","font1":{"__typename":"StyleSheetFont","name":"SANS_SERIF_1"},"font2":{"__typename":"StyleSheetFont","name":"SANS_SERIF_1"},"font3":{"__typename":"StyleSheetFont","name":"SERIF_2"}}},"header":{"__typename":"HeaderStyles","backgroundColor":{"__typename":"ColorValue","colorPalette":{"__typename":"ColorPalette","tintBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FF355876","colorPoints":[{"__typename":"ColorPoint","color":"#FF355876","point":0},{"__typename":"ColorPoint","color":"#FF4D6C88","point":0.1},{"__typename":"ColorPoint","color":"#FF637F99","point":0.2},{"__typename":"ColorPoint","color":"#FF7791A8","point":0.3},{"__typename":"ColorPoint","color":"#FF8CA2B7","point":0.4},{"__typename":"ColorPoint","color":"#FF9FB3C6","point":0.5},{"__typename":"ColorPoint","color":"#FFB2C3D4","point":0.6},{"__typename":"ColorPoint","color":"#FFC5D2E1","point":0.7},{"__typename":"ColorPoint","color":"#FFD7E2EE","point":0.8},{"__typename":"ColorPoint","color":"#FFE9F1FA","point":0.9},{"__typename":"ColorPoint","color":"#FFFBFFFF","point":1}]}},"rgb":"355876","alpha":"99"},"postBackgroundColor":null,"backgroundImage":{"__ref":"ImageMetadata:1*sfUruIusLq6tbpLx0sDYZQ.png"},"headerScale":"HEADER_SCALE_MEDIUM","horizontalAlignment":"CENTER","backgroundImageDisplayMode":"IMAGE_DISPLAY_MODE_FILL","backgroundImageVerticalAlignment":"END","backgroundColorDisplayMode":"COLOR_DISPLAY_MODE_SOLID","secondaryBackgroundColor":null,"nameColor":null,"nameTreatment":"NAME_TREATMENT_LOGO","postNameTreatment":"NAME_TREATMENT_LOGO","logoImage":{"__ref":"ImageMetadata:1*AGyTPCaRzVqL77kFwUwHKg.png"},"logoScale":"HEADER_SCALE_LARGE","taglineColor":{"__typename":"ColorValue","rgb":"ffffff","alpha":"ff"},"taglineTreatment":"TAGLINE_TREATMENT_HEADER"},"navigation":{"__typename":"HeaderNavigation","navItems":[{"__typename":"HeaderNavigationItem","name":"Editors' Picks","href":null,"type":"NAV_TYPE_TAG","tags":[{"__ref":"Tag:editors-pick"}],"tagSlugs":["editors-pick"]},{"__typename":"HeaderNavigationItem","name":"Features","href":null,"type":"NAV_TYPE_TAG","tags":[{"__ref":"Tag:tds-features"}],"tagSlugs":["tds-features"]},{"__typename":"HeaderNavigationItem","name":"Deep Dives","href":null,"type":"NAV_TYPE_TAG","tags":[{"__ref":"Tag:deep-dives"}],"tagSlugs":["deep-dives"]},{"__typename":"HeaderNavigationItem","name":"Grow","href":"https:\u002F\u002Ftowardsdatascience.com\u002Fhow-to-get-the-most-out-of-towards-data-science-3bf37f75a345","type":"NAV_TYPE_LINK","tags":[],"tagSlugs":[]},{"__typename":"HeaderNavigationItem","name":"Contribute","href":"https:\u002F\u002Ftowardsdatascience.com\u002Fquestions-96667b06af5","type":"NAV_TYPE_LINK","tags":[],"tagSlugs":[]}]},"postBody":null,"blogroll":{"__typename":"BlogrollConfiguration","visibility":"BLOGROLL_VISIBILITY_SIDEBAR"}},"ImageMetadata:1*sfUruIusLq6tbpLx0sDYZQ.png":{"id":"1*sfUruIusLq6tbpLx0sDYZQ.png","__typename":"ImageMetadata","originalWidth":1401},"CollectionViewerEdge:collectionId:7f60cf5620c9-viewerId:2bf0e8de62c3":{"id":"collectionId:7f60cf5620c9-viewerId:2bf0e8de62c3","__typename":"CollectionViewerEdge","isEditor":false},"ImageMetadata:1*mG6i4Bh_LgixUYXJgQpYsg@2x.png":{"id":"1*mG6i4Bh_LgixUYXJgQpYsg@2x.png","__typename":"ImageMetadata","originalWidth":337,"originalHeight":122},"User:7e12c71dfa81":{"id":"7e12c71dfa81","__typename":"User","atsQualifiedAt":1612205680542},"NewsletterV3:d6fe9076899":{"id":"d6fe9076899","__typename":"NewsletterV3","slug":"the-variable","showPromo":true,"name":"The Variable","description":"Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss.","promoHeadline":"","promoBody":"","collection":{"__ref":"Collection:7f60cf5620c9"},"type":"NEWSLETTER_TYPE_COLLECTION","user":{"__ref":"User:895063a310f4"}},"ImageMetadata:1*eLxNtw6hQ4-3HrHda5BCCw.png":{"id":"1*eLxNtw6hQ4-3HrHda5BCCw.png","__typename":"ImageMetadata"},"Collection:7f60cf5620c9":{"id":"7f60cf5620c9","__typename":"Collection","domain":"towardsdatascience.com","googleAnalyticsId":null,"slug":"towards-data-science","colorBehavior":"ACCENT_COLOR_AND_FILL_BACKGROUND","isAuroraVisible":true,"favicon":{"__ref":"ImageMetadata:1*ChFMdf--f5jbm-AYv6VdYA@2x.png"},"name":"Towards Data Science","colorPalette":{"__typename":"ColorPalette","highlightSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FFEDF4FC","point":0},{"__typename":"ColorPoint","color":"#FFE9F2FD","point":0.1},{"__typename":"ColorPoint","color":"#FFE6F1FD","point":0.2},{"__typename":"ColorPoint","color":"#FFE2EFFD","point":0.3},{"__typename":"ColorPoint","color":"#FFDFEEFD","point":0.4},{"__typename":"ColorPoint","color":"#FFDBECFE","point":0.5},{"__typename":"ColorPoint","color":"#FFD7EBFE","point":0.6},{"__typename":"ColorPoint","color":"#FFD4E9FE","point":0.7},{"__typename":"ColorPoint","color":"#FFD0E7FF","point":0.8},{"__typename":"ColorPoint","color":"#FFCCE6FF","point":0.9},{"__typename":"ColorPoint","color":"#FFC8E4FF","point":1}]},"defaultBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FF668AAA","point":0},{"__typename":"ColorPoint","color":"#FF61809D","point":0.1},{"__typename":"ColorPoint","color":"#FF5A7690","point":0.2},{"__typename":"ColorPoint","color":"#FF546C83","point":0.3},{"__typename":"ColorPoint","color":"#FF4D6275","point":0.4},{"__typename":"ColorPoint","color":"#FF455768","point":0.5},{"__typename":"ColorPoint","color":"#FF3D4C5A","point":0.6},{"__typename":"ColorPoint","color":"#FF34414C","point":0.7},{"__typename":"ColorPoint","color":"#FF2B353E","point":0.8},{"__typename":"ColorPoint","color":"#FF21282F","point":0.9},{"__typename":"ColorPoint","color":"#FF161B1F","point":1}]},"tintBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FF355876","colorPoints":[{"__typename":"ColorPoint","color":"#FF355876","point":0},{"__typename":"ColorPoint","color":"#FF4D6C88","point":0.1},{"__typename":"ColorPoint","color":"#FF637F99","point":0.2},{"__typename":"ColorPoint","color":"#FF7791A8","point":0.3},{"__typename":"ColorPoint","color":"#FF8CA2B7","point":0.4},{"__typename":"ColorPoint","color":"#FF9FB3C6","point":0.5},{"__typename":"ColorPoint","color":"#FFB2C3D4","point":0.6},{"__typename":"ColorPoint","color":"#FFC5D2E1","point":0.7},{"__typename":"ColorPoint","color":"#FFD7E2EE","point":0.8},{"__typename":"ColorPoint","color":"#FFE9F1FA","point":0.9},{"__typename":"ColorPoint","color":"#FFFBFFFF","point":1}]}},"customStyleSheet":{"__ref":"CustomStyleSheet:dc5d6afeee0a"},"tagline":"A Medium publication sharing concepts, ideas and codes.","isAuroraEligible":true,"viewerEdge":{"__ref":"CollectionViewerEdge:collectionId:7f60cf5620c9-viewerId:2bf0e8de62c3"},"logo":{"__ref":"ImageMetadata:1*mG6i4Bh_LgixUYXJgQpYsg@2x.png"},"navItems":[{"__typename":"NavItem","title":"Data Science","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fdata-science\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"Machine Learning","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fmachine-learning\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"Programming","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fprogramming\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"Visualization","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fdata-visualization\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"Video","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fvideo\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"★","url":"https:\u002F\u002Ftowardsdatascience.com\u002Feditors-picks\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"About","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fabout-us\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"Contribute","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fcontribute\u002Fhome","type":"EXTERNAL_LINK_NAV_ITEM"}],"creator":{"__ref":"User:7e12c71dfa81"},"subscriberCount":604983,"newsletterV3":{"__ref":"NewsletterV3:d6fe9076899"},"avatar":{"__ref":"ImageMetadata:1*eLxNtw6hQ4-3HrHda5BCCw.png"},"canToggleEmail":true,"description":"Your home for data science. A Medium publication sharing concepts, ideas and codes.","ampEnabled":false,"twitterUsername":"TDataScience","facebookPageId":null,"customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","status":"ACTIVE","isSubdomain":false}},"ptsQualifiedAt":1616092952992},"CustomStyleSheet:5a2595a71917":{"id":"5a2595a71917","__typename":"CustomStyleSheet","blogroll":{"__typename":"BlogrollConfiguration","visibility":"BLOGROLL_VISIBILITY_SIDEBAR"},"global":{"__typename":"GlobalStyles","colorPalette":{"__typename":"StyleSheetColorPalette","primary":{"__typename":"ColorValue","colorPalette":{"__typename":"ColorPalette","tintBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FF668aaa","colorPoints":[{"__typename":"ColorPoint","color":"#FF668AAA","point":0},{"__typename":"ColorPoint","color":"#FF7697B5","point":0.1},{"__typename":"ColorPoint","color":"#FF86A4BF","point":0.2},{"__typename":"ColorPoint","color":"#FF96B0C9","point":0.3},{"__typename":"ColorPoint","color":"#FFA5BCD2","point":0.4},{"__typename":"ColorPoint","color":"#FFB4C8DB","point":0.5},{"__typename":"ColorPoint","color":"#FFC2D3E5","point":0.6},{"__typename":"ColorPoint","color":"#FFD1DFED","point":0.7},{"__typename":"ColorPoint","color":"#FFDFEAF6","point":0.8},{"__typename":"ColorPoint","color":"#FFEDF5FE","point":0.9},{"__typename":"ColorPoint","color":"#FFFBFFFF","point":1}]},"highlightSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FFEDF4FC","point":0},{"__typename":"ColorPoint","color":"#FFE9F2FD","point":0.1},{"__typename":"ColorPoint","color":"#FFE6F1FD","point":0.2},{"__typename":"ColorPoint","color":"#FFE2EFFD","point":0.3},{"__typename":"ColorPoint","color":"#FFDFEEFD","point":0.4},{"__typename":"ColorPoint","color":"#FFDBECFE","point":0.5},{"__typename":"ColorPoint","color":"#FFD7EBFE","point":0.6},{"__typename":"ColorPoint","color":"#FFD4E9FE","point":0.7},{"__typename":"ColorPoint","color":"#FFD0E8FF","point":0.8},{"__typename":"ColorPoint","color":"#FFCCE6FF","point":0.9},{"__typename":"ColorPoint","color":"#FFC8E4FF","point":1}]},"defaultBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FF668AAA","point":0},{"__typename":"ColorPoint","color":"#FF60809D","point":0.1},{"__typename":"ColorPoint","color":"#FF5A7690","point":0.2},{"__typename":"ColorPoint","color":"#FF536C83","point":0.3},{"__typename":"ColorPoint","color":"#FF4C6275","point":0.4},{"__typename":"ColorPoint","color":"#FF455768","point":0.5},{"__typename":"ColorPoint","color":"#FF3D4C5A","point":0.6},{"__typename":"ColorPoint","color":"#FF34414C","point":0.7},{"__typename":"ColorPoint","color":"#FF2B353E","point":0.8},{"__typename":"ColorPoint","color":"#FF21282F","point":0.9},{"__typename":"ColorPoint","color":"#FF161B1F","point":1}]}}},"background":null},"fonts":{"__typename":"StyleSheetFonts","font1":{"__typename":"StyleSheetFont","name":"SANS_SERIF_1"},"font2":{"__typename":"StyleSheetFont","name":"SANS_SERIF_1"},"font3":{"__typename":"StyleSheetFont","name":"SERIF_2"}}},"header":{"__typename":"HeaderStyles","backgroundColor":{"__typename":"ColorValue","colorPalette":{"__typename":"ColorPalette","tintBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FF345876","colorPoints":[{"__typename":"ColorPoint","color":"#FF345876","point":0},{"__typename":"ColorPoint","color":"#FF4C6C88","point":0.1},{"__typename":"ColorPoint","color":"#FF627F99","point":0.2},{"__typename":"ColorPoint","color":"#FF7791A8","point":0.3},{"__typename":"ColorPoint","color":"#FF8BA2B7","point":0.4},{"__typename":"ColorPoint","color":"#FF9FB3C6","point":0.5},{"__typename":"ColorPoint","color":"#FFB2C3D4","point":0.6},{"__typename":"ColorPoint","color":"#FFC4D3E1","point":0.7},{"__typename":"ColorPoint","color":"#FFD7E2EE","point":0.8},{"__typename":"ColorPoint","color":"#FFE9F1FB","point":0.9},{"__typename":"ColorPoint","color":"#FFFBFFFF","point":1}]}},"rgb":"345876","alpha":"ff"},"postBackgroundColor":{"__typename":"ColorValue","colorPalette":{"__typename":"ColorPalette","tintBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FF345876","colorPoints":[{"__typename":"ColorPoint","color":"#FF345876","point":0},{"__typename":"ColorPoint","color":"#FF4C6C88","point":0.1},{"__typename":"ColorPoint","color":"#FF627F99","point":0.2},{"__typename":"ColorPoint","color":"#FF7791A8","point":0.3},{"__typename":"ColorPoint","color":"#FF8BA2B7","point":0.4},{"__typename":"ColorPoint","color":"#FF9FB3C6","point":0.5},{"__typename":"ColorPoint","color":"#FFB2C3D4","point":0.6},{"__typename":"ColorPoint","color":"#FFC4D3E1","point":0.7},{"__typename":"ColorPoint","color":"#FFD7E2EE","point":0.8},{"__typename":"ColorPoint","color":"#FFE9F1FB","point":0.9},{"__typename":"ColorPoint","color":"#FFFBFFFF","point":1}]}},"rgb":"345876","alpha":"ff"},"backgroundImage":null,"headerScale":"HEADER_SCALE_MEDIUM","horizontalAlignment":"START","backgroundImageDisplayMode":"IMAGE_DISPLAY_MODE_FILL","backgroundImageVerticalAlignment":"START","backgroundColorDisplayMode":"COLOR_DISPLAY_MODE_SOLID","secondaryBackgroundColor":null,"nameColor":{"__typename":"ColorValue","rgb":"ffffff","alpha":"ff"},"nameTreatment":"NAME_TREATMENT_LOGO","postNameTreatment":"NAME_TREATMENT_LOGO","logoImage":null,"logoScale":"HEADER_SCALE_SMALL","taglineColor":{"__typename":"ColorValue","rgb":"345876","alpha":"ff"},"taglineTreatment":"TAGLINE_TREATMENT_SIDEBAR"},"navigation":null},"UserViewerEdge:userId:b17ebd108358-viewerId:2bf0e8de62c3":{"id":"userId:b17ebd108358-viewerId:2bf0e8de62c3","__typename":"UserViewerEdge","isFollowing":false,"isUser":false},"NewsletterV3:164289aafbe3":{"id":"164289aafbe3","__typename":"NewsletterV3","type":"NEWSLETTER_TYPE_AUTHOR","slug":"b17ebd108358","name":"b17ebd108358","collection":null,"user":{"__ref":"User:b17ebd108358"},"description":"","promoHeadline":"","promoBody":"","replyToEmail":"","showPromo":false,"subscribersCount":13},"Post:f5622fbe1e82":{"id":"f5622fbe1e82","__typename":"Post"},"ImageMetadata:1*AGyTPCaRzVqL77kFwUwHKg.png":{"id":"1*AGyTPCaRzVqL77kFwUwHKg.png","__typename":"ImageMetadata","originalWidth":1376,"originalHeight":429},"Tag:editors-pick":{"id":"editors-pick","__typename":"Tag","normalizedTagSlug":"editors-pick"},"Tag:tds-features":{"id":"tds-features","__typename":"Tag","normalizedTagSlug":"tds-features"},"Tag:deep-dives":{"id":"deep-dives","__typename":"Tag","normalizedTagSlug":"deep-dives"},"Topic:ae5d4995e225":{"id":"ae5d4995e225","__typename":"Topic","name":"Data Science","slug":"data-science"},"Paragraph:c4eac4150242_0":{"id":"c4eac4150242_0","__typename":"Paragraph","name":"a1e7","text":"Understanding Variational Autoencoders (VAEs)","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_1":{"id":"c4eac4150242_1","__typename":"Paragraph","name":"9db8","text":"Building, step by step, the reasoning that leads to VAEs.","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_2":{"id":"c4eac4150242_2","__typename":"Paragraph","name":"9796","text":"Credit: Free-Photos on Pixabay","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"__ref":"ImageMetadata:1*mbCY2_LZX2bpGX7CH80FAg.jpeg"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":8,"end":19,"type":"A","href":"https:\u002F\u002Fpixabay.com\u002Ffr\u002Fusers\u002Ffree-photos-242387\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":23,"end":30,"type":"A","href":"https:\u002F\u002Fpixabay.com\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:c4eac4150242_3":{"id":"c4eac4150242_3","__typename":"Paragraph","name":"c1a6","text":"This post was co-written with Baptiste Rocca.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":30,"end":44,"type":"A","href":null,"anchorType":"USER","userId":"20ad1309823a","linkMetadata":null}],"dropCapImage":null},"Paragraph:c4eac4150242_4":{"id":"c4eac4150242_4","__typename":"Paragraph","name":"a205","text":"Introduction","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_5":{"id":"c4eac4150242_5","__typename":"Paragraph","name":"8956","text":"In the last few years, deep learning based generative models have gained more and more interest due to (and implying) some amazing improvements in the field. Relying on huge amount of data, well-designed networks architectures and smart training techniques, deep generative models have shown an incredible ability to produce highly realistic pieces of content of various kind, such as images, texts and sounds. Among these deep generative models, two major families stand out and deserve a special attention: Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_6":{"id":"c4eac4150242_6","__typename":"Paragraph","name":"33bb","text":"Face images generated with a Variational Autoencoder (source: Wojciech Mormul on Github).","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*BaZPg3SRgZGVigguQCmirA.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":62,"end":87,"type":"A","href":"https:\u002F\u002Fgithub.com\u002FWojciechMormul\u002Fvae","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:c4eac4150242_7":{"id":"c4eac4150242_7","__typename":"Paragraph","name":"f1b2","text":"In a previous post, published in January of this year, we discussed in depth Generative Adversarial Networks (GANs) and showed, in particular, how adversarial training can oppose two networks, a generator and a discriminator, to push both of them to improve iteration after iteration. We introduce now, in this post, the other major kind of deep generative models: Variational Autoencoders (VAEs). In a nutshell, a VAE is an autoencoder whose encodings distribution is regularised during the training in order to ensure that its latent space has good properties allowing us to generate some new data. Moreover, the term “variational” comes from the close relation there is between the regularisation and the variational inference method in statistics.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":77,"end":115,"type":"A","href":"https:\u002F\u002Ftowardsdatascience.com\u002Funderstanding-generative-adversarial-networks-gans-cd6e4651a29","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:c4eac4150242_8":{"id":"c4eac4150242_8","__typename":"Paragraph","name":"e777","text":"If the last two sentences summarise pretty well the notion of VAEs, they can also raise a lot of questions. What is an autoencoder? What is the latent space and why regularising it? How to generate new data from VAEs? What is the link between VAEs and variational inference? In order to describe VAEs as well as possible, we will try to answer all this questions (and many others!) and to provide the reader with as much insights as we can (ranging from basic intuitions to more advanced mathematical details). Thus, the purpose of this post is not only to discuss the fundamental notions Variational Autoencoders rely on but also to build step by step and starting from the very beginning the reasoning that leads to these notions.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_9":{"id":"c4eac4150242_9","__typename":"Paragraph","name":"ddf4","text":"Without further ado, let’s (re)discover VAEs together!","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_10":{"id":"c4eac4150242_10","__typename":"Paragraph","name":"f01f","text":"Outline","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_11":{"id":"c4eac4150242_11","__typename":"Paragraph","name":"f7d4","text":"In the first section, we will review some important notions about dimensionality reduction and autoencoder that will be useful for the understanding of VAEs. Then, in the second section, we will show why autoencoders cannot be used to generate new data and will introduce Variational Autoencoders that are regularised versions of autoencoders making the generative process possible. Finally in the last section we will give a more mathematical presentation of VAEs, based on variational inference.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_12":{"id":"c4eac4150242_12","__typename":"Paragraph","name":"1b2d","text":"Note. In the last section we have tried to make the mathematical derivation as complete and clear as possible to bridge the gap between intuitions and equations. However, the readers that doesn’t want to dive into the mathematical details of VAEs can skip this section without hurting the understanding of the main concepts. Notice also that in this post we will make the following abuse of notation: for a random variable z, we will denote p(z) the distribution (or the density, depending on the context) of this random variable.","type":"BQ","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":5,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:c4eac4150242_13":{"id":"c4eac4150242_13","__typename":"Paragraph","name":"91ed","text":"Dimensionality reduction, PCA and autoencoders","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_14":{"id":"c4eac4150242_14","__typename":"Paragraph","name":"196d","text":"In this first section we will start by discussing some notions related to dimensionality reduction. In particular, we will review briefly principal component analysis (PCA) and autoencoders, showing how both ideas are related to each others.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_15":{"id":"c4eac4150242_15","__typename":"Paragraph","name":"2176","text":"What is dimensionality reduction?","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_16":{"id":"c4eac4150242_16","__typename":"Paragraph","name":"a5fc","text":"In machine learning, dimensionality reduction is the process of reducing the number of features that describe some data. This reduction is done either by selection (only some existing features are conserved) or by extraction (a reduced number of new features are created based on the old features) and can be useful in many situations that require low dimensional data (data visualisation, data storage, heavy computation…). Although there exists many different methods of dimensionality reduction, we can set a global framework that is matched by most (if not any!) of these methods.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":21,"end":45,"type":"A","href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FDimensionality_reduction","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":21,"end":119,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:c4eac4150242_17":{"id":"c4eac4150242_17","__typename":"Paragraph","name":"005f","text":"First, let’s call encoder the process that produce the “new features” representation from the “old features” representation (by selection or by extraction) and decoder the reverse process. Dimensionality reduction can then be interpreted as data compression where the encoder compress the data (from the initial space to the encoded space, also called latent space) whereas the decoder decompress them. Of course, depending on the initial data distribution, the latent space dimension and the encoder definition, this compression can be lossy, meaning that a part of the information is lost during the encoding process and cannot be recovered when decoding.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":18,"end":25,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":160,"end":167,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":325,"end":338,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":352,"end":364,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:c4eac4150242_18":{"id":"c4eac4150242_18","__typename":"Paragraph","name":"8df0","text":"Illustration of the dimensionality reduction principle with encoder and decoder.","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"__ref":"ImageMetadata:1*UdOybs9wOe3zW8vDAfj9VA@2x.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_19":{"id":"c4eac4150242_19","__typename":"Paragraph","name":"07ca","text":"The main purpose of a dimensionality reduction method is to find the best encoder\u002Fdecoder pair among a given family. In other words, for a given set of possible encoders and decoders, we are looking for the pair that keeps the maximum of information when encoding and, so, has the minimum of reconstruction error when decoding. If we denote respectively E and D the families of encoders and decoders we are considering, then the dimensionality reduction problem can be written","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":217,"end":263,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":273,"end":326,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:c4eac4150242_20":{"id":"c4eac4150242_20","__typename":"Paragraph","name":"c343","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*9_DFaRan_hX9xMZldVGNjg@2x.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_21":{"id":"c4eac4150242_21","__typename":"Paragraph","name":"c7e2","text":"where","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_22":{"id":"c4eac4150242_22","__typename":"Paragraph","name":"285c","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*wau5Kuv0xNL8XRjlt8PfaA@2x.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_23":{"id":"c4eac4150242_23","__typename":"Paragraph","name":"b89a","text":"defines the reconstruction error measure between the input data x and the encoded-decoded data d(e(x)). Notice finally that in the following we will denote N the number of data, n_d the dimension of the initial (decoded) space and n_e the dimension of the reduced (encoded) space.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_24":{"id":"c4eac4150242_24","__typename":"Paragraph","name":"0e8a","text":"Principal components analysis (PCA)","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_25":{"id":"c4eac4150242_25","__typename":"Paragraph","name":"d9aa","text":"One of the first methods that come in mind when speaking about dimensionality reduction is principal component analysis (PCA). In order to show how it fits the framework we just described and make the link towards autoencoders, let’s give a very high overview of how PCA works, letting most of the details aside (notice that we plan to write a full post on the subject).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":91,"end":125,"type":"A","href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FPrincipal_component_analysis","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":91,"end":125,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":312,"end":369,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:c4eac4150242_26":{"id":"c4eac4150242_26","__typename":"Paragraph","name":"fe0e","text":"The idea of PCA is to build n_e new independent features that are linear combinations of the n_d old features and so that the projections of the data on the subspace defined by these new features are as close as possible to the initial data (in term of euclidean distance). In other words, PCA is looking for the best linear subspace of the initial space (described by an orthogonal basis of new features) such that the error of approximating the data by their projections on this subspace is as small as possible.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":36,"end":47,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":66,"end":85,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:c4eac4150242_27":{"id":"c4eac4150242_27","__typename":"Paragraph","name":"2d49","text":"Principal Component Analysis (PCA) is looking for the best linear subspace using linear algebra.","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"__ref":"ImageMetadata:1*ayo0n2zq_gy7VERYmp4lrA@2x.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_28":{"id":"c4eac4150242_28","__typename":"Paragraph","name":"0603","text":"Translated in our global framework, we are looking for an encoder in the family E of the n_e by n_d matrices (linear transformation) whose rows are orthonormal (features independence) and for the associated decoder among the family D of n_d by n_e matrices. It can be shown that the unitary eigenvectors corresponding to the n_e greatest eigenvalues (in norm) of the covariance features matrix are orthogonal (or can be chosen to be so) and define the best subspace of dimension n_e to project data on with minimal error of approximation. Thus, these n_e eigenvectors can be chosen as our new features and, so, the problem of dimension reduction can then be expressed as an eigenvalue\u002Feigenvector problem. Moreover, it can also be shown that, in such case, the decoder matrix is the transposed of the encoder matrix.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_29":{"id":"c4eac4150242_29","__typename":"Paragraph","name":"a84d","text":"PCA matches the encoder-decoder framework we described.","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"__ref":"ImageMetadata:1*LRPyMAwDlio7f1_YKYI2hw@2x.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_30":{"id":"c4eac4150242_30","__typename":"Paragraph","name":"11f4","text":"Autoencoders","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_31":{"id":"c4eac4150242_31","__typename":"Paragraph","name":"1d95","text":"Let’s now discuss autoencoders and see how we can use neural networks for dimensionality reduction. The general idea of autoencoders is pretty simple and consists in setting an encoder and a decoder as neural networks and to learn the best encoding-decoding scheme using an iterative optimisation process. So, at each iteration we feed the autoencoder architecture (the encoder followed by the decoder) with some data, we compare the encoded-decoded output with the initial data and backpropagate the error through the architecture to update the weights of the networks.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":18,"end":30,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":166,"end":217,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":225,"end":304,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:c4eac4150242_32":{"id":"c4eac4150242_32","__typename":"Paragraph","name":"4aed","text":"Thus, intuitively, the overall autoencoder architecture (encoder+decoder) creates a bottleneck for data that ensures only the main structured part of the information can go through and be reconstructed. Looking at our general framework, the family E of considered encoders is defined by the encoder network architecture, the family D of considered decoders is defined by the decoder network architecture and the search of encoder and decoder that minimise the reconstruction error is done by gradient descent over the parameters of these networks.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_33":{"id":"c4eac4150242_33","__typename":"Paragraph","name":"aac5","text":"Illustration of an autoencoder with its loss function.","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*bY_ShNK6lBCQ3D9LYIfwJg@2x.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_34":{"id":"c4eac4150242_34","__typename":"Paragraph","name":"bf7b","text":"Let’s first suppose that both our encoder and decoder architectures have only one layer without non-linearity (linear autoencoder). Such encoder and decoder are then simple linear transformations that can be expressed as matrices. In such situation, we can see a clear link with PCA in the sense that, just like PCA does, we are looking for the best linear subspace to project data on with as few information loss as possible when doing so. Encoding and decoding matrices obtained with PCA define naturally one of the solutions we would be satisfied to reach by gradient descent, but we should outline that this is not the only one. Indeed, several basis can be chosen to describe the same optimal subspace and, so, several encoder\u002Fdecoder pairs can give the optimal reconstruction error. Moreover, for linear autoencoders and contrarily to PCA, the new features we end up do not have to be independent (no orthogonality constraints in the neural networks).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":641,"end":706,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:c4eac4150242_35":{"id":"c4eac4150242_35","__typename":"Paragraph","name":"3a0b","text":"Link between linear autoencoder and PCA.","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"__ref":"ImageMetadata:1*ek9ZFmimq9Sr1sG5Z0jXfQ@2x.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_36":{"id":"c4eac4150242_36","__typename":"Paragraph","name":"057a","text":"Now, let’s assume that both the encoder and the decoder are deep and non-linear. In such case, the more complex the architecture is, the more the autoencoder can proceed to a high dimensionality reduction while keeping reconstruction loss low. Intuitively, if our encoder and our decoder have enough degrees of freedom, we can reduce any initial dimensionality to 1. Indeed, an encoder with “infinite power” could theoretically takes our N initial data points and encodes them as 1, 2, 3, … up to N (or more generally, as N integer on the real axis) and the associated decoder could make the reverse transformation, with no loss during the process.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_37":{"id":"c4eac4150242_37","__typename":"Paragraph","name":"8d4d","text":"Here, we should however keep two things in mind. First, an important dimensionality reduction with no reconstruction loss often comes with a price: the lack of interpretable and exploitable structures in the latent space (lack of regularity). Second, most of the time the final purpose of dimensionality reduction is not to only reduce the number of dimensions of the data but to reduce this number of dimensions while keeping the major part of the data structure information in the reduced representations. For these two reasons, the dimension of the latent space and the “depth” of autoencoders (that define degree and quality of compression) have to be carefully controlled and adjusted depending on the final purpose of the dimensionality reduction.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":222,"end":240,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":413,"end":506,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:c4eac4150242_38":{"id":"c4eac4150242_38","__typename":"Paragraph","name":"463c","text":"When reducing dimensionality, we want to keep the main structure there exists among the data.","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"__ref":"ImageMetadata:1*F-3zbCL_lp7EclKowfowMA@2x.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_39":{"id":"c4eac4150242_39","__typename":"Paragraph","name":"b974","text":"Variational Autoencoders","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_40":{"id":"c4eac4150242_40","__typename":"Paragraph","name":"0fd2","text":"Up to now, we have discussed dimensionality reduction problem and introduce autoencoders that are encoder-decoder architectures that can be trained by gradient descent. Let’s now make the link with the content generation problem, see the limitations of autoencoders in their current form for this problem and introduce Variational Autoencoders.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_41":{"id":"c4eac4150242_41","__typename":"Paragraph","name":"9ca9","text":"Limitations of autoencoders for content generation","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_42":{"id":"c4eac4150242_42","__typename":"Paragraph","name":"1345","text":"At this point, a natural question that comes in mind is “what is the link between autoencoders and content generation?”. Indeed, once the autoencoder has been trained, we have both an encoder and a decoder but still no real way to produce any new content. At first sight, we could be tempted to think that, if the latent space is regular enough (well “organized” by the encoder during the training process), we could take a point randomly from that latent space and decode it to get a new content. The decoder would then act more or less like the generator of a Generative Adversarial Network.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_43":{"id":"c4eac4150242_43","__typename":"Paragraph","name":"a43a","text":"We can generate new data by decoding points that are randomly sampled from the latent space. The quality and relevance of generated data depend on the regularity of the latent space.","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*Qd1xKV9o-AnWtfIDhhNdFg@2x.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_44":{"id":"c4eac4150242_44","__typename":"Paragraph","name":"ce28","text":"However, as we discussed in the previous section, the regularity of the latent space for autoencoders is a difficult point that depends on the distribution of the data in the initial space, the dimension of the latent space and the architecture of the encoder. So, it is pretty difficult (if not impossible) to ensure, a priori, that the encoder will organize the latent space in a smart way compatible with the generative process we just described.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_45":{"id":"c4eac4150242_45","__typename":"Paragraph","name":"09eb","text":"To illustrate this point, let’s consider the example we gave previously in which we described an encoder and a decoder powerful enough to put any N initial training data onto the real axis (each data point being encoded as a real value) and decode them without any reconstruction loss. In such case, the high degree of freedom of the autoencoder that makes possible to encode and decode with no information loss (despite the low dimensionality of the latent space) leads to a severe overfitting implying that some points of the latent space will give meaningless content once decoded. If this one dimensional example has been voluntarily chosen to be quite extreme, we can notice that the problem of the autoencoders latent space regularity is much more general than that and deserve a special attention.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":465,"end":494,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:c4eac4150242_46":{"id":"c4eac4150242_46","__typename":"Paragraph","name":"fef2","text":"Irregular latent space prevent us from using autoencoder for new content generation.","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"__ref":"ImageMetadata:1*iSfaVxcGi_ELkKgAG0YRlQ@2x.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_47":{"id":"c4eac4150242_47","__typename":"Paragraph","name":"4384","text":"When thinking about it for a minute, this lack of structure among the encoded data into the latent space is pretty normal. Indeed, nothing in the task the autoencoder is trained for enforce to get such organisation: the autoencoder is solely trained to encode and decode with as few loss as possible, no matter how the latent space is organised. Thus, if we are not careful about the definition of the architecture, it is natural that, during the training, the network takes advantage of any overfitting possibilities to achieve its task as well as it can… unless we explicitly regularise it!","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":216,"end":344,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:c4eac4150242_48":{"id":"c4eac4150242_48","__typename":"Paragraph","name":"780e","text":"Definition of variational autoencoders","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_49":{"id":"c4eac4150242_49","__typename":"Paragraph","name":"a576","text":"So, in order to be able to use the decoder of our autoencoder for generative purpose, we have to be sure that the latent space is regular enough. One possible solution to obtain such regularity is to introduce explicit regularisation during the training process. Thus, as we briefly mentioned in the introduction of this post, a variational autoencoder can be defined as being an autoencoder whose training is regularised to avoid overfitting and ensure that the latent space has good properties that enable generative process.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":327,"end":527,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:c4eac4150242_50":{"id":"c4eac4150242_50","__typename":"Paragraph","name":"cae4","text":"Just as a standard autoencoder, a variational autoencoder is an architecture composed of both an encoder and a decoder and that is trained to minimise the reconstruction error between the encoded-decoded data and the initial data. However, in order to introduce some regularisation of the latent space, we proceed to a slight modification of the encoding-decoding process: instead of encoding an input as a single point, we encode it as a distribution over the latent space. The model is then trained as follows:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":373,"end":473,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:c4eac4150242_51":{"id":"c4eac4150242_51","__typename":"Paragraph","name":"4251","text":"first, the input is encoded as distribution over the latent space","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_52":{"id":"c4eac4150242_52","__typename":"Paragraph","name":"ef92","text":"second, a point from the latent space is sampled from that distribution","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_53":{"id":"c4eac4150242_53","__typename":"Paragraph","name":"ebf2","text":"third, the sampled point is decoded and the reconstruction error can be computed","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_54":{"id":"c4eac4150242_54","__typename":"Paragraph","name":"8e10","text":"finally, the reconstruction error is backpropagated through the network","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_55":{"id":"c4eac4150242_55","__typename":"Paragraph","name":"0d84","text":"Difference between autoencoder (deterministic) and variational autoencoder (probabilistic).","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"__ref":"ImageMetadata:1*ejNnusxYrn1NRDZf4Kg2lw@2x.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_56":{"id":"c4eac4150242_56","__typename":"Paragraph","name":"36b1","text":"In practice, the encoded distributions are chosen to be normal so that the encoder can be trained to return the mean and the covariance matrix that describe these Gaussians. The reason why an input is encoded as a distribution with some variance instead of a single point is that it makes possible to express very naturally the latent space regularisation: the distributions returned by the encoder are enforced to be close to a standard normal distribution. We will see in the next subsection that we ensure this way both a local and global regularisation of the latent space (local because of the variance control and global because of the mean control).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_57":{"id":"c4eac4150242_57","__typename":"Paragraph","name":"4ebc","text":"Thus, the loss function that is minimised when training a VAE is composed of a “reconstruction term” (on the final layer), that tends to make the encoding-decoding scheme as performant as possible, and a “regularisation term” (on the latent layer), that tends to regularise the organisation of the latent space by making the distributions returned by the encoder close to a standard normal distribution. That regularisation term is expressed as the Kulback-Leibler divergence between the returned distribution and a standard Gaussian and will be further justified in the next section. We can notice that the Kullback-Leibler divergence between two Gaussian distributions has a closed form that can be directly expressed in terms of the means and the covariance matrices of the the two distributions.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":449,"end":475,"type":"A","href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FKullback–Leibler_divergence","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:c4eac4150242_58":{"id":"c4eac4150242_58","__typename":"Paragraph","name":"6181","text":"In variational autoencoders, the loss function is composed of a reconstruction term (that makes the encoding-decoding scheme efficient) and a regularisation term (that makes the latent space regular).","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*Q5dogodt3wzKKktE0v3dMQ@2x.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_59":{"id":"c4eac4150242_59","__typename":"Paragraph","name":"6116","text":"Intuitions about the regularisation","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_60":{"id":"c4eac4150242_60","__typename":"Paragraph","name":"a0bd","text":"The regularity that is expected from the latent space in order to make generative process possible can be expressed through two main properties: continuity (two close points in the latent space should not give two completely different contents once decoded) and completeness (for a chosen distribution, a point sampled from the latent space should give “meaningful” content once decoded).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":145,"end":155,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":262,"end":274,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:c4eac4150242_61":{"id":"c4eac4150242_61","__typename":"Paragraph","name":"da42","text":"Difference between a “regular” and an “irregular” latent space.","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"__ref":"ImageMetadata:1*83S0T8IEJyudR_I5rI9now@2x.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_62":{"id":"c4eac4150242_62","__typename":"Paragraph","name":"c1cd","text":"The only fact that VAEs encode inputs as distributions instead of simple points is not sufficient to ensure continuity and completeness. Without a well defined regularisation term, the model can learn, in order to minimise its reconstruction error, to “ignore” the fact that distributions are returned and behave almost like classic autoencoders (leading to overfitting). To do so, the encoder can either return distributions with tiny variances (that would tend to be punctual distributions) or return distributions with very different means (that would then be really far apart from each other in the latent space). In both cases, distributions are used the wrong way (cancelling the expected benefit) and continuity and\u002For completeness are not satisfied.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":249,"end":345,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:c4eac4150242_63":{"id":"c4eac4150242_63","__typename":"Paragraph","name":"5aaf","text":"So, in order to avoid these effects we have to regularise both the covariance matrix and the mean of the distributions returned by the encoder. In practice, this regularisation is done by enforcing distributions to be close to a standard normal distribution (centred and reduced). This way, we require the covariance matrices to be close to the identity, preventing punctual distributions, and the mean to be close to 0, preventing encoded distributions to be too far apart from each others.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":36,"end":142,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:c4eac4150242_64":{"id":"c4eac4150242_64","__typename":"Paragraph","name":"25bb","text":"The returned distributions of VAEs have to be regularised to obtain a latent space with good properties.","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"__ref":"ImageMetadata:1*9ouOKh2w-b3NNOVx4Mw9bg@2x.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_65":{"id":"c4eac4150242_65","__typename":"Paragraph","name":"01b0","text":"With this regularisation term, we prevent the model to encode data far apart in the latent space and encourage as much as possible returned distributions to “overlap”, satisfying this way the expected continuity and completeness conditions. Naturally, as for any regularisation term, this comes at the price of a higher reconstruction error on the training data. The tradeoff between the reconstruction error and the KL divergence can however be adjusted and we will see in the next section how the expression of the balance naturally emerge from our formal derivation.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_66":{"id":"c4eac4150242_66","__typename":"Paragraph","name":"fa99","text":"To conclude this subsection, we can observe that continuity and completeness obtained with regularisation tend to create a “gradient” over the information encoded in the latent space. For example, a point of the latent space that would be halfway between the means of two encoded distributions coming from different training data should be decoded in something that is somewhere between the data that gave the first distribution and the data that gave the second distribution as it may be sampled by the autoencoder in both cases.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":106,"end":182,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:c4eac4150242_67":{"id":"c4eac4150242_67","__typename":"Paragraph","name":"9a66","text":"Regularisation tends to create a “gradient” over the information encoded in the latent space.","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*79AzftDm7WcQ9OfRH5Y-6g@2x.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_68":{"id":"c4eac4150242_68","__typename":"Paragraph","name":"ec2d","text":"Note. As a side note, we can mention that the second potential problem we have mentioned (the network put distributions far from each others) is in fact almost equivalent to the first one (the network tends to return punctual distribution) up to a change of scale: in both case variances of distributions become small relatively to distance between their means.","type":"BQ","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":5,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:c4eac4150242_69":{"id":"c4eac4150242_69","__typename":"Paragraph","name":"31fa","text":"Mathematical details of VAEs","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_70":{"id":"c4eac4150242_70","__typename":"Paragraph","name":"0ed1","text":"In the previous section we gave the following intuitive overview: VAEs are autoencoders that encode inputs as distributions instead of points and whose latent space “organisation” is regularised by constraining distributions returned by the encoder to be close to a standard Gaussian. In this section we will give a more mathematical view of VAEs that will allow us to justify the regularisation term more rigorously. To do so, we will set a clear probabilistic framework and will use, in particular, variational inference technique.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_71":{"id":"c4eac4150242_71","__typename":"Paragraph","name":"f77b","text":"Probabilistic framework and assumptions","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_72":{"id":"c4eac4150242_72","__typename":"Paragraph","name":"cd56","text":"Let’s begin by defining a probabilistic graphical model to describe our data. We denote by x the variable that represents our data and assume that x is generated from a latent variable z (the encoded representation) that is not directly observed. Thus, for each data point, the following two steps generative process is assumed:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_73":{"id":"c4eac4150242_73","__typename":"Paragraph","name":"f62a","text":"first, a latent representation z is sampled from the prior distribution p(z)","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_74":{"id":"c4eac4150242_74","__typename":"Paragraph","name":"ee29","text":"second, the data x is sampled from the conditional likelihood distribution p(x|z)","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_75":{"id":"c4eac4150242_75","__typename":"Paragraph","name":"d142","text":"Graphical model of the data generation process.","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*dxlZr07dXNYiTFWL62D7Ag@2x.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_76":{"id":"c4eac4150242_76","__typename":"Paragraph","name":"bde5","text":"With such a probabilistic model in mind, we can redefine our notions of encoder and decoder. Indeed, contrarily to a simple autoencoder that consider deterministic encoder and decoder, we are going to consider now probabilistic versions of these two objects. The “probabilistic decoder” is naturally defined by p(x|z), that describes the distribution of the decoded variable given the encoded one, whereas the “probabilistic encoder” is defined by p(z|x), that describes the distribution of the encoded variable given the decoded one.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":185,"end":257,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":311,"end":396,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":448,"end":533,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:c4eac4150242_77":{"id":"c4eac4150242_77","__typename":"Paragraph","name":"546d","text":"At this point, we can already notice that the regularisation of the latent space that we lacked in simple autoencoders naturally appears here in the definition of the data generation process: encoded representations z in the latent space are indeed assumed to follow the prior distribution p(z). Otherwise, we can also remind the the well-known Bayes theorem that makes the link between the prior p(z), the likelihood p(x|z), and the posterior p(z|x)","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_78":{"id":"c4eac4150242_78","__typename":"Paragraph","name":"192c","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*5wyhpwWHlLZHGdlho_rboA@2x.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_79":{"id":"c4eac4150242_79","__typename":"Paragraph","name":"46f1","text":"Let’s now make the assumption that p(z) is a standard Gaussian distribution and that p(x|z) is a Gaussian distribution whose mean is defined by a deterministic function f of the variable of z and whose covariance matrix has the form of a positive constant c that multiplies the identity matrix I. The function f is assumed to belong to a family of functions denoted F that is left unspecified for the moment and that will be chosen later. Thus, we have","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_80":{"id":"c4eac4150242_80","__typename":"Paragraph","name":"c063","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*Nnfsrn5-aYLhuwgSYPJQzA@2x.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_81":{"id":"c4eac4150242_81","__typename":"Paragraph","name":"a3c5","text":"Let’s consider, for now, that f is well defined and fixed. In theory, as we know p(z) and p(x|z), we can use the Bayes theorem to compute p(z|x): this is a classical Bayesian inference problem. However, as we discussed in our previous article, this kind of computation is often intractable (because of the integral at the denominator) and require the use of approximation techniques such as variational inference.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":166,"end":192,"type":"A","href":"https:\u002F\u002Ftowardsdatascience.com\u002Fbayesian-inference-problem-mcmc-and-variational-inference-25a8aa9bce29","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:c4eac4150242_82":{"id":"c4eac4150242_82","__typename":"Paragraph","name":"175d","text":"Note. Here we can mention that p(z) and p(x|z) are both Gaussian distribution. So, if we had E(x|z) = f(z) = z, it would imply that p(z|x) should also follow a Gaussian distribution and, in theory, we could “only” try to express the mean and the covariance matrix of p(z|x) with respect to the means and the covariance matrices of p(z) and p(x|z). However, in practice this condition is not met and we need to use of an approximation technique like variational inference that makes the approach pretty general and more robust to some changes in the hypothesis of the model.","type":"BQ","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":5,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:c4eac4150242_83":{"id":"c4eac4150242_83","__typename":"Paragraph","name":"b0ae","text":"Variational inference formulation","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_84":{"id":"c4eac4150242_84","__typename":"Paragraph","name":"acb0","text":"In statistics, variational inference (VI) is a technique to approximate complex distributions. The idea is to set a parametrised family of distribution (for example the family of Gaussians, whose parameters are the mean and the covariance) and to look for the best approximation of our target distribution among this family. The best element in the family is one that minimise a given approximation error measurement (most of the time the Kullback-Leibler divergence between approximation and target) and is found by gradient descent over the parameters that describe the family. For more details, we refer to our post on variational inference and references therein.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":610,"end":643,"type":"A","href":"https:\u002F\u002Ftowardsdatascience.com\u002Fbayesian-inference-problem-mcmc-and-variational-inference-25a8aa9bce29","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":15,"end":93,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:c4eac4150242_85":{"id":"c4eac4150242_85","__typename":"Paragraph","name":"1e2b","text":"Here we are going to approximate p(z|x) by a Gaussian distribution q_x(z) whose mean and covariance are defined by two functions, g and h, of the parameter x. These two functions are supposed to belong, respectively, to the families of functions G and H that will be specified later but that are supposed to be parametrised. Thus we can denote","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_86":{"id":"c4eac4150242_86","__typename":"Paragraph","name":"0bf6","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*j9goPhh0meH884uGL70Aqg@2x.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_87":{"id":"c4eac4150242_87","__typename":"Paragraph","name":"9ed6","text":"So, we have defined this way a family of candidates for variational inference and need now to find the best approximation among this family by optimising the functions g and h (in fact, their parameters) to minimise the Kullback-Leibler divergence between the approximation and the target p(z|x). In other words, we are looking for the optimal g* and h* such that","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_88":{"id":"c4eac4150242_88","__typename":"Paragraph","name":"7c63","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*v56YF5KqZk35r85EZBZuAQ@2x.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_89":{"id":"c4eac4150242_89","__typename":"Paragraph","name":"57a5","text":"In the second last equation, we can observe the tradeoff there exists — when approximating the posterior p(z|x) — between maximising the likelihood of the “observations” (maximisation of the expected log-likelihood, for the first term) and staying close to the prior distribution (minimisation of the KL divergence between q_x(z) and p(z), for the second term). This tradeoff is natural for Bayesian inference problem and express the balance that needs to be found between the confidence we have in the data and the confidence we have in the prior.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_90":{"id":"c4eac4150242_90","__typename":"Paragraph","name":"d7ef","text":"Up to know, we have assumed the function f known and fixed and we have showed that, under such assumptions, we can approximate the posterior p(z|x) using variational inference technique. However, in practice this function f, that defines the decoder, is not known and also need to be chosen. To do so, let’s remind that our initial goal is to find a performant encoding-decoding scheme whose latent space is regular enough to be used for generative purpose. If the regularity is mostly ruled by the prior distribution assumed over the latent space, the performance of the overall encoding-decoding scheme highly depends on the choice of the function f. Indeed, as p(z|x) can be approximate (by variational inference) from p(z) and p(x|z) and as p(z) is a simple standard Gaussian, the only two levers we have at our disposal in our model to make optimisations are the parameter c (that defines the variance of the likelihood) and the function f (that defines the mean of the likelihood).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_91":{"id":"c4eac4150242_91","__typename":"Paragraph","name":"b47b","text":"So, let’s consider that, as we discussed earlier, we can get for any function f in F (each defining a different probabilistic decoder p(x|z)) the best approximation of p(z|x), denoted q*_x(z). Despite its probabilistic nature, we are looking for an encoding-decoding scheme as efficient as possible and, then, we want to choose the function f that maximises the expected log-likelihood of x given z when z is sampled from q*_x(z). In other words, for a given input x, we want to maximise the probability to have x̂ = x when we sample z from the distribution q*_x(z) and then sample x̂ from the distribution p(x|z). Thus, we are looking for the optimal f* such that","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":431,"end":614,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:c4eac4150242_92":{"id":"c4eac4150242_92","__typename":"Paragraph","name":"016a","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*3dfxn11raqxXWc2WcADhqQ@2x.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_93":{"id":"c4eac4150242_93","__typename":"Paragraph","name":"a7fb","text":"where q*_x(z) depends on the function f and is obtained as described before. Gathering all the pieces together, we are looking for optimal f*, g* and h* such that","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_94":{"id":"c4eac4150242_94","__typename":"Paragraph","name":"0121","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*uJPYBQauUfQtb_-r3vjhng@2x.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_95":{"id":"c4eac4150242_95","__typename":"Paragraph","name":"c960","text":"We can identify in this objective function the elements introduced in the intuitive description of VAEs given in the previous section: the reconstruction error between x and f(z) and the regularisation term given by the KL divergence between q_x(z) and p(z) (which is a standard Gaussian). We can also notice the constant c that rules the balance between the two previous terms. The higher c is the more we assume a high variance around f(z) for the probabilistic decoder in our model and, so, the more we favour the regularisation term over the reconstruction term (and the opposite stands if c is low).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_96":{"id":"c4eac4150242_96","__typename":"Paragraph","name":"31fe","text":"Bringing neural networks into the model","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_97":{"id":"c4eac4150242_97","__typename":"Paragraph","name":"8487","text":"Up to know, we have set a probabilistic model that depends on three functions, f, g and h, and express, using variational inference, the optimisation problem to solve in order to get f*, g* and h* that give the optimal encoding-decoding scheme with this model. As we can’t easily optimise over the entire space of functions, we constrain the optimisation domain and decide to express f, g and h as neural networks. Thus, F, G and H correspond respectively to the families of functions defined by the networks architectures and the optimisation is done over the parameters of these networks.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_98":{"id":"c4eac4150242_98","__typename":"Paragraph","name":"f439","text":"In practice, g and h are not defined by two completely independent networks but share a part of their architecture and their weights so that we have","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_99":{"id":"c4eac4150242_99","__typename":"Paragraph","name":"ad84","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*H-q2OirDCYI1sXX3HkitBg@2x.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_100":{"id":"c4eac4150242_100","__typename":"Paragraph","name":"f0f3","text":"As it defines the covariance matrix of q_x(z), h(x) is supposed to be a square matrix. However, in order to simplify the computation and reduce the number of parameters, we make the additional assumption that our approximation of p(z|x), q_x(z), is a multidimensional Gaussian distribution with diagonal covariance matrix (variables independence assumption). With this assumption, h(x) is simply the vector of the diagonal elements of the covariance matrix and has then the same size as g(x). However, we reduce this way the family of distributions we consider for variational inference and, so, the approximation of p(z|x) obtained can be less accurate.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_101":{"id":"c4eac4150242_101","__typename":"Paragraph","name":"0816","text":"Encoder part of the VAE.","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*XYyWimolMhPDMg8qCNlcwg@2x.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_102":{"id":"c4eac4150242_102","__typename":"Paragraph","name":"c127","text":"Contrarily to the encoder part that models p(z|x) and for which we considered a Gaussian with both mean and covariance that are functions of x (g and h), our model assumes for p(x|z) a Gaussian with fixed covariance. The function f of the variable z defining the mean of that Gaussian is modelled by a neural network and can be represented as follows","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_103":{"id":"c4eac4150242_103","__typename":"Paragraph","name":"2c7d","text":"Decoder part of the VAE.","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*1n6HwjwUWbmE9PvCzOVcbw@2x.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_104":{"id":"c4eac4150242_104","__typename":"Paragraph","name":"cab8","text":"The overall architecture is then obtained by concatenating the encoder and the decoder parts. However we still need to be very careful about the way we sample from the distribution returned by the encoder during the training. The sampling process has to be expressed in a way that allows the error to be backpropagated through the network. A simple trick, called reparametrisation trick, is used to make the gradient descent possible despite the random sampling that occurs halfway of the architecture and consists in using the fact that if z is a random variable following a Gaussian distribution with mean g(x) and with covariance H(x)=h(x).h^t(x) then it can be expressed as","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":363,"end":386,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:c4eac4150242_105":{"id":"c4eac4150242_105","__typename":"Paragraph","name":"6ca7","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*SE-A1yR7BXIjNL0tsmITZg@2x.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_106":{"id":"c4eac4150242_106","__typename":"Paragraph","name":"fb2c","text":"Illustration of the reparametrisation trick.","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"__ref":"ImageMetadata:1*S8CoO3TGtFBpzv8GvmgKeg@2x.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_107":{"id":"c4eac4150242_107","__typename":"Paragraph","name":"f6c4","text":"Finally, the objective function of the variational autoencoder architecture obtained this way is given by the last equation of the previous subsection in which the theoretical expectancy is replaced by a more or less accurate Monte-Carlo approximation that consists, most of the time, into a single draw. So, considering this approximation and denoting C = 1\u002F(2c), we recover the loss function derived intuitively in the previous section, composed of a reconstruction term, a regularisation term and a constant to define the relative weights of these two terms.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_108":{"id":"c4eac4150242_108","__typename":"Paragraph","name":"edbe","text":"Variational Autoencoders representation.","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"__ref":"ImageMetadata:1*eRcdr8gczweQHk--1pZF9A@2x.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_109":{"id":"c4eac4150242_109","__typename":"Paragraph","name":"3a01","text":"Takeaways","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_110":{"id":"c4eac4150242_110","__typename":"Paragraph","name":"afee","text":"The main takeways of this article are:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_111":{"id":"c4eac4150242_111","__typename":"Paragraph","name":"4bae","text":"dimensionality reduction is the process of reducing the number of features that describe some data (either by selecting only a subset of the initial features or by combining them into a reduced number new features) and, so, can be seen as an encoding process","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_112":{"id":"c4eac4150242_112","__typename":"Paragraph","name":"9c45","text":"autoencoders are neural networks architectures composed of both an encoder and a decoder that create a bottleneck to go through for data and that are trained to lose a minimal quantity of information during the encoding-decoding process (training by gradient descent iterations with the goal to reduce the reconstruction error)","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_113":{"id":"c4eac4150242_113","__typename":"Paragraph","name":"12d1","text":"due to overfitting, the latent space of an autoencoder can be extremely irregular (close points in latent space can give very different decoded data, some point of the latent space can give meaningless content once decoded, …) and, so, we can’t really define a generative process that simply consists to sample a point from the latent space and make it go through the decoder to get a new data","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_114":{"id":"c4eac4150242_114","__typename":"Paragraph","name":"1c84","text":"variational autoencoders (VAEs) are autoencoders that tackle the problem of the latent space irregularity by making the encoder return a distribution over the latent space instead of a single point and by adding in the loss function a regularisation term over that returned distribution in order to ensure a better organisation of the latent space","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_115":{"id":"c4eac4150242_115","__typename":"Paragraph","name":"8876","text":"assuming a simple underlying probabilistic model to describe our data, the pretty intuitive loss function of VAEs, composed of a reconstruction term and a regularisation term, can be carefully derived, using in particular the statistical technique of variational inference (hence the name “variational” autoencoders)","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_116":{"id":"c4eac4150242_116","__typename":"Paragraph","name":"847b","text":"To conclude, we can outline that, during the last years, GANs have benefited from much more scientific contributions than VAEs. Among other reasons, the higher interest that has been shown by the community for GANs can be partly explained by the higher degree of complexity in VAEs theoretical basis (probabilistic model and variational inference) compared to the simplicity of the adversarial training concept that rules GANs. With this post we hope that we managed to share valuable intuitions as well as strong theoretical foundations to make VAEs more accessible to newcomers, as we did for GANs earlier this year. However, now that we have discussed in depth both of them, one question remains… are you more GANs or VAEs?","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":581,"end":617,"type":"A","href":"https:\u002F\u002Ftowardsdatascience.com\u002Funderstanding-generative-adversarial-networks-gans-cd6e4651a29","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:c4eac4150242_117":{"id":"c4eac4150242_117","__typename":"Paragraph","name":"9b72","text":"Thanks for reading!","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:c4eac4150242_118":{"id":"c4eac4150242_118","__typename":"Paragraph","name":"5633","text":"Other articles written with Baptiste Rocca:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":28,"end":42,"type":"A","href":null,"anchorType":"USER","userId":"20ad1309823a","linkMetadata":null}],"dropCapImage":null},"Paragraph:c4eac4150242_119":{"id":"c4eac4150242_119","__typename":"Paragraph","name":"23a7","text":"Introduction to recommender systems\nOverview of some major recommendation algorithms.towardsdatascience.com","type":"MIXTAPE_EMBED","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":{"__typename":"MixtapeMetadata","href":"https:\u002F\u002Ftowardsdatascience.com\u002Fintroduction-to-recommender-systems-6c66cf15ada","thumbnailImageId":"1*kjouN-zV6BgpmCl5SnEjGQ.jpeg"},"markups":[{"__typename":"Markup","start":0,"end":107,"type":"A","href":"https:\u002F\u002Ftowardsdatascience.com\u002Fintroduction-to-recommender-systems-6c66cf15ada","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":0,"end":35,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":36,"end":85,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:c4eac4150242_120":{"id":"c4eac4150242_120","__typename":"Paragraph","name":"fd49","text":"Ensemble methods: bagging, boosting and stacking\nUnderstanding the key concepts of ensemble learning.towardsdatascience.com","type":"MIXTAPE_EMBED","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":{"__typename":"MixtapeMetadata","href":"https:\u002F\u002Ftowardsdatascience.com\u002Fensemble-methods-bagging-boosting-and-stacking-c9214a10a205","thumbnailImageId":"1*q6x_dETZ3wZjpcrYFntmJQ.jpeg"},"markups":[{"__typename":"Markup","start":0,"end":123,"type":"A","href":"https:\u002F\u002Ftowardsdatascience.com\u002Fensemble-methods-bagging-boosting-and-stacking-c9214a10a205","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":0,"end":48,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":49,"end":101,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"ImageMetadata:1*mbCY2_LZX2bpGX7CH80FAg.jpeg":{"id":"1*mbCY2_LZX2bpGX7CH80FAg.jpeg","__typename":"ImageMetadata","originalHeight":720,"originalWidth":1920,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*BaZPg3SRgZGVigguQCmirA.png":{"id":"1*BaZPg3SRgZGVigguQCmirA.png","__typename":"ImageMetadata","originalHeight":685,"originalWidth":1280,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*UdOybs9wOe3zW8vDAfj9VA@2x.png":{"id":"1*UdOybs9wOe3zW8vDAfj9VA@2x.png","__typename":"ImageMetadata","originalHeight":764,"originalWidth":1815,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*9_DFaRan_hX9xMZldVGNjg@2x.png":{"id":"1*9_DFaRan_hX9xMZldVGNjg@2x.png","__typename":"ImageMetadata","originalHeight":74,"originalWidth":548,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*wau5Kuv0xNL8XRjlt8PfaA@2x.png":{"id":"1*wau5Kuv0xNL8XRjlt8PfaA@2x.png","__typename":"ImageMetadata","originalHeight":42,"originalWidth":208,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*ayo0n2zq_gy7VERYmp4lrA@2x.png":{"id":"1*ayo0n2zq_gy7VERYmp4lrA@2x.png","__typename":"ImageMetadata","originalHeight":702,"originalWidth":1815,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*LRPyMAwDlio7f1_YKYI2hw@2x.png":{"id":"1*LRPyMAwDlio7f1_YKYI2hw@2x.png","__typename":"ImageMetadata","originalHeight":723,"originalWidth":1815,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*bY_ShNK6lBCQ3D9LYIfwJg@2x.png":{"id":"1*bY_ShNK6lBCQ3D9LYIfwJg@2x.png","__typename":"ImageMetadata","originalHeight":690,"originalWidth":1262,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*ek9ZFmimq9Sr1sG5Z0jXfQ@2x.png":{"id":"1*ek9ZFmimq9Sr1sG5Z0jXfQ@2x.png","__typename":"ImageMetadata","originalHeight":852,"originalWidth":1789,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*F-3zbCL_lp7EclKowfowMA@2x.png":{"id":"1*F-3zbCL_lp7EclKowfowMA@2x.png","__typename":"ImageMetadata","originalHeight":718,"originalWidth":1815,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*Qd1xKV9o-AnWtfIDhhNdFg@2x.png":{"id":"1*Qd1xKV9o-AnWtfIDhhNdFg@2x.png","__typename":"ImageMetadata","originalHeight":723,"originalWidth":1389,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*iSfaVxcGi_ELkKgAG0YRlQ@2x.png":{"id":"1*iSfaVxcGi_ELkKgAG0YRlQ@2x.png","__typename":"ImageMetadata","originalHeight":600,"originalWidth":1804,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*ejNnusxYrn1NRDZf4Kg2lw@2x.png":{"id":"1*ejNnusxYrn1NRDZf4Kg2lw@2x.png","__typename":"ImageMetadata","originalHeight":596,"originalWidth":1804,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*Q5dogodt3wzKKktE0v3dMQ@2x.png":{"id":"1*Q5dogodt3wzKKktE0v3dMQ@2x.png","__typename":"ImageMetadata","originalHeight":752,"originalWidth":1548,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*83S0T8IEJyudR_I5rI9now@2x.png":{"id":"1*83S0T8IEJyudR_I5rI9now@2x.png","__typename":"ImageMetadata","originalHeight":683,"originalWidth":1871,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*9ouOKh2w-b3NNOVx4Mw9bg@2x.png":{"id":"1*9ouOKh2w-b3NNOVx4Mw9bg@2x.png","__typename":"ImageMetadata","originalHeight":683,"originalWidth":1871,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*79AzftDm7WcQ9OfRH5Y-6g@2x.png":{"id":"1*79AzftDm7WcQ9OfRH5Y-6g@2x.png","__typename":"ImageMetadata","originalHeight":683,"originalWidth":1266,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*dxlZr07dXNYiTFWL62D7Ag@2x.png":{"id":"1*dxlZr07dXNYiTFWL62D7Ag@2x.png","__typename":"ImageMetadata","originalHeight":290,"originalWidth":564,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*5wyhpwWHlLZHGdlho_rboA@2x.png":{"id":"1*5wyhpwWHlLZHGdlho_rboA@2x.png","__typename":"ImageMetadata","originalHeight":98,"originalWidth":664,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*Nnfsrn5-aYLhuwgSYPJQzA@2x.png":{"id":"1*Nnfsrn5-aYLhuwgSYPJQzA@2x.png","__typename":"ImageMetadata","originalHeight":104,"originalWidth":782,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*j9goPhh0meH884uGL70Aqg@2x.png":{"id":"1*j9goPhh0meH884uGL70Aqg@2x.png","__typename":"ImageMetadata","originalHeight":44,"originalWidth":814,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*v56YF5KqZk35r85EZBZuAQ@2x.png":{"id":"1*v56YF5KqZk35r85EZBZuAQ@2x.png","__typename":"ImageMetadata","originalHeight":506,"originalWidth":1626,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*3dfxn11raqxXWc2WcADhqQ@2x.png":{"id":"1*3dfxn11raqxXWc2WcADhqQ@2x.png","__typename":"ImageMetadata","originalHeight":190,"originalWidth":640,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*uJPYBQauUfQtb_-r3vjhng@2x.png":{"id":"1*uJPYBQauUfQtb_-r3vjhng@2x.png","__typename":"ImageMetadata","originalHeight":104,"originalWidth":1280,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*H-q2OirDCYI1sXX3HkitBg@2x.png":{"id":"1*H-q2OirDCYI1sXX3HkitBg@2x.png","__typename":"ImageMetadata","originalHeight":42,"originalWidth":1046,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*XYyWimolMhPDMg8qCNlcwg@2x.png":{"id":"1*XYyWimolMhPDMg8qCNlcwg@2x.png","__typename":"ImageMetadata","originalHeight":595,"originalWidth":1286,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*1n6HwjwUWbmE9PvCzOVcbw@2x.png":{"id":"1*1n6HwjwUWbmE9PvCzOVcbw@2x.png","__typename":"ImageMetadata","originalHeight":521,"originalWidth":1286,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*SE-A1yR7BXIjNL0tsmITZg@2x.png":{"id":"1*SE-A1yR7BXIjNL0tsmITZg@2x.png","__typename":"ImageMetadata","originalHeight":44,"originalWidth":604,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*S8CoO3TGtFBpzv8GvmgKeg@2x.png":{"id":"1*S8CoO3TGtFBpzv8GvmgKeg@2x.png","__typename":"ImageMetadata","originalHeight":701,"originalWidth":1852,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*eRcdr8gczweQHk--1pZF9A@2x.png":{"id":"1*eRcdr8gczweQHk--1pZF9A@2x.png","__typename":"ImageMetadata","originalHeight":1002,"originalWidth":1664,"focusPercentX":null,"focusPercentY":null,"alt":null},"User:895063a310f4":{"id":"895063a310f4","__typename":"User","name":"Ludovic Benistant","username":"ludobenistant","newsletterV3":{"__ref":"NewsletterV3:2375c85c8da7"}},"NewsletterV3:2375c85c8da7":{"id":"2375c85c8da7","__typename":"NewsletterV3"},"Tag:machine-learning":{"id":"machine-learning","__typename":"Tag","displayTitle":"Machine Learning","normalizedTagSlug":"machine-learning"},"Tag:data-science":{"id":"data-science","__typename":"Tag","displayTitle":"Data Science","normalizedTagSlug":"data-science"},"Tag:deep-learning":{"id":"deep-learning","__typename":"Tag","displayTitle":"Deep Learning","normalizedTagSlug":"deep-learning"},"Tag:artificial-intelligence":{"id":"artificial-intelligence","__typename":"Tag","displayTitle":"Artificial Intelligence","normalizedTagSlug":"artificial-intelligence"},"Tag:towards-data-science":{"id":"towards-data-science","__typename":"Tag","displayTitle":"Towards Data Science","normalizedTagSlug":"towards-data-science"},"PostViewerEdge:postId:f70510919f73-viewerId:2bf0e8de62c3":{"id":"postId:f70510919f73-viewerId:2bf0e8de62c3","__typename":"PostViewerEdge","catalogsConnection":{"__typename":"EntityCatalogsConnection","catalogsContainingThis({\"type\":\"LISTS\"})":[],"predefinedContainingThis":[]}},"Post:f70510919f73":{"id":"f70510919f73","__typename":"Post","creator":{"__ref":"User:b17ebd108358"},"canonicalUrl":"","collection":{"__ref":"Collection:7f60cf5620c9"},"content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\",\"sk\":null,\"source\":null}})":{"__typename":"PostContent","isLockedPreviewOnly":false,"validatedShareKey":"","bodyModel":{"__typename":"RichText","paragraphs":[{"__ref":"Paragraph:c4eac4150242_0"},{"__ref":"Paragraph:c4eac4150242_1"},{"__ref":"Paragraph:c4eac4150242_2"},{"__ref":"Paragraph:c4eac4150242_3"},{"__ref":"Paragraph:c4eac4150242_4"},{"__ref":"Paragraph:c4eac4150242_5"},{"__ref":"Paragraph:c4eac4150242_6"},{"__ref":"Paragraph:c4eac4150242_7"},{"__ref":"Paragraph:c4eac4150242_8"},{"__ref":"Paragraph:c4eac4150242_9"},{"__ref":"Paragraph:c4eac4150242_10"},{"__ref":"Paragraph:c4eac4150242_11"},{"__ref":"Paragraph:c4eac4150242_12"},{"__ref":"Paragraph:c4eac4150242_13"},{"__ref":"Paragraph:c4eac4150242_14"},{"__ref":"Paragraph:c4eac4150242_15"},{"__ref":"Paragraph:c4eac4150242_16"},{"__ref":"Paragraph:c4eac4150242_17"},{"__ref":"Paragraph:c4eac4150242_18"},{"__ref":"Paragraph:c4eac4150242_19"},{"__ref":"Paragraph:c4eac4150242_20"},{"__ref":"Paragraph:c4eac4150242_21"},{"__ref":"Paragraph:c4eac4150242_22"},{"__ref":"Paragraph:c4eac4150242_23"},{"__ref":"Paragraph:c4eac4150242_24"},{"__ref":"Paragraph:c4eac4150242_25"},{"__ref":"Paragraph:c4eac4150242_26"},{"__ref":"Paragraph:c4eac4150242_27"},{"__ref":"Paragraph:c4eac4150242_28"},{"__ref":"Paragraph:c4eac4150242_29"},{"__ref":"Paragraph:c4eac4150242_30"},{"__ref":"Paragraph:c4eac4150242_31"},{"__ref":"Paragraph:c4eac4150242_32"},{"__ref":"Paragraph:c4eac4150242_33"},{"__ref":"Paragraph:c4eac4150242_34"},{"__ref":"Paragraph:c4eac4150242_35"},{"__ref":"Paragraph:c4eac4150242_36"},{"__ref":"Paragraph:c4eac4150242_37"},{"__ref":"Paragraph:c4eac4150242_38"},{"__ref":"Paragraph:c4eac4150242_39"},{"__ref":"Paragraph:c4eac4150242_40"},{"__ref":"Paragraph:c4eac4150242_41"},{"__ref":"Paragraph:c4eac4150242_42"},{"__ref":"Paragraph:c4eac4150242_43"},{"__ref":"Paragraph:c4eac4150242_44"},{"__ref":"Paragraph:c4eac4150242_45"},{"__ref":"Paragraph:c4eac4150242_46"},{"__ref":"Paragraph:c4eac4150242_47"},{"__ref":"Paragraph:c4eac4150242_48"},{"__ref":"Paragraph:c4eac4150242_49"},{"__ref":"Paragraph:c4eac4150242_50"},{"__ref":"Paragraph:c4eac4150242_51"},{"__ref":"Paragraph:c4eac4150242_52"},{"__ref":"Paragraph:c4eac4150242_53"},{"__ref":"Paragraph:c4eac4150242_54"},{"__ref":"Paragraph:c4eac4150242_55"},{"__ref":"Paragraph:c4eac4150242_56"},{"__ref":"Paragraph:c4eac4150242_57"},{"__ref":"Paragraph:c4eac4150242_58"},{"__ref":"Paragraph:c4eac4150242_59"},{"__ref":"Paragraph:c4eac4150242_60"},{"__ref":"Paragraph:c4eac4150242_61"},{"__ref":"Paragraph:c4eac4150242_62"},{"__ref":"Paragraph:c4eac4150242_63"},{"__ref":"Paragraph:c4eac4150242_64"},{"__ref":"Paragraph:c4eac4150242_65"},{"__ref":"Paragraph:c4eac4150242_66"},{"__ref":"Paragraph:c4eac4150242_67"},{"__ref":"Paragraph:c4eac4150242_68"},{"__ref":"Paragraph:c4eac4150242_69"},{"__ref":"Paragraph:c4eac4150242_70"},{"__ref":"Paragraph:c4eac4150242_71"},{"__ref":"Paragraph:c4eac4150242_72"},{"__ref":"Paragraph:c4eac4150242_73"},{"__ref":"Paragraph:c4eac4150242_74"},{"__ref":"Paragraph:c4eac4150242_75"},{"__ref":"Paragraph:c4eac4150242_76"},{"__ref":"Paragraph:c4eac4150242_77"},{"__ref":"Paragraph:c4eac4150242_78"},{"__ref":"Paragraph:c4eac4150242_79"},{"__ref":"Paragraph:c4eac4150242_80"},{"__ref":"Paragraph:c4eac4150242_81"},{"__ref":"Paragraph:c4eac4150242_82"},{"__ref":"Paragraph:c4eac4150242_83"},{"__ref":"Paragraph:c4eac4150242_84"},{"__ref":"Paragraph:c4eac4150242_85"},{"__ref":"Paragraph:c4eac4150242_86"},{"__ref":"Paragraph:c4eac4150242_87"},{"__ref":"Paragraph:c4eac4150242_88"},{"__ref":"Paragraph:c4eac4150242_89"},{"__ref":"Paragraph:c4eac4150242_90"},{"__ref":"Paragraph:c4eac4150242_91"},{"__ref":"Paragraph:c4eac4150242_92"},{"__ref":"Paragraph:c4eac4150242_93"},{"__ref":"Paragraph:c4eac4150242_94"},{"__ref":"Paragraph:c4eac4150242_95"},{"__ref":"Paragraph:c4eac4150242_96"},{"__ref":"Paragraph:c4eac4150242_97"},{"__ref":"Paragraph:c4eac4150242_98"},{"__ref":"Paragraph:c4eac4150242_99"},{"__ref":"Paragraph:c4eac4150242_100"},{"__ref":"Paragraph:c4eac4150242_101"},{"__ref":"Paragraph:c4eac4150242_102"},{"__ref":"Paragraph:c4eac4150242_103"},{"__ref":"Paragraph:c4eac4150242_104"},{"__ref":"Paragraph:c4eac4150242_105"},{"__ref":"Paragraph:c4eac4150242_106"},{"__ref":"Paragraph:c4eac4150242_107"},{"__ref":"Paragraph:c4eac4150242_108"},{"__ref":"Paragraph:c4eac4150242_109"},{"__ref":"Paragraph:c4eac4150242_110"},{"__ref":"Paragraph:c4eac4150242_111"},{"__ref":"Paragraph:c4eac4150242_112"},{"__ref":"Paragraph:c4eac4150242_113"},{"__ref":"Paragraph:c4eac4150242_114"},{"__ref":"Paragraph:c4eac4150242_115"},{"__ref":"Paragraph:c4eac4150242_116"},{"__ref":"Paragraph:c4eac4150242_117"},{"__ref":"Paragraph:c4eac4150242_118"},{"__ref":"Paragraph:c4eac4150242_119"},{"__ref":"Paragraph:c4eac4150242_120"}],"sections":[{"__typename":"Section","name":"3a74","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"64eb","startIndex":13,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"62d8","startIndex":39,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"9ea6","startIndex":69,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"d37b","startIndex":109,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"af0d","startIndex":118,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null}]}},"customStyleSheet":{"__ref":"CustomStyleSheet:dc5d6afeee0a"},"firstPublishedAt":1569287944756,"isIndexable":false,"isLocked":false,"isPublished":true,"isShortform":false,"layerCake":2,"primaryTopic":{"__ref":"Topic:ae5d4995e225"},"title":"Understanding Variational Autoencoders (VAEs)","isMarkedPaywallOnly":false,"mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Funderstanding-variational-autoencoders-vaes-f70510919f73","isLimitedState":false,"visibility":"PUBLIC","license":"ALL_RIGHTS_RESERVED","inResponseToPostResult":null,"allowResponses":true,"newsletterId":"","sequence":null,"tags":[{"__ref":"Tag:machine-learning"},{"__ref":"Tag:data-science"},{"__ref":"Tag:deep-learning"},{"__ref":"Tag:artificial-intelligence"},{"__ref":"Tag:towards-data-science"}],"topics":[{"__typename":"Topic","topicId":"1af65db9c2f8","name":"Artificial Intelligence"},{"__typename":"Topic","topicId":"1eca0103fff3","name":"Machine Learning"},{"__typename":"Topic","topicId":"7808efc0cf94","name":"Math"},{"__typename":"Topic","topicId":"ae5d4995e225","name":"Data Science"}],"isNewsletter":false,"isPublishToEmail":false,"socialTitle":"","socialDek":"","noIndex":null,"curationStatus":null,"metaDescription":"","latestPublishedAt":1616326686437,"readingTime":22.612264150943396,"previewContent":{"__typename":"PreviewContent","subtitle":"Building, step by step, the reasoning that leads to VAEs."},"previewImage":{"__ref":"ImageMetadata:1*mbCY2_LZX2bpGX7CH80FAg.jpeg"},"clapCount":7933,"postResponses":{"__typename":"PostResponses","count":71},"isSuspended":false,"pendingCollection":null,"statusForCollection":"APPROVED","lockedSource":"LOCKED_POST_SOURCE_NONE","pinnedAt":0,"pinnedByCreatorAt":0,"curationEligibleAt":0,"responseDistribution":"NOT_DISTRIBUTED","inResponseToEntityType":null,"viewerEdge":{"__ref":"PostViewerEdge:postId:f70510919f73-viewerId:2bf0e8de62c3"},"collaborators":[],"translationSourcePost":null,"audioVersionUrl":"","seoTitle":"","updatedAt":1639266486305,"shortformType":"SHORTFORM_TYPE_LINK","structuredData":"","seoDescription":"","latestPublishedVersion":"c4eac4150242","isAuthorNewsletter":false,"voterCount":1577,"recommenders":[],"content({})":{"__typename":"PostContent","isLockedPreviewOnly":false,"validatedShareKey":"","bodyModel":{"__typename":"RichText","paragraphs":[{"__ref":"Paragraph:c4eac4150242_0"},{"__ref":"Paragraph:c4eac4150242_1"},{"__ref":"Paragraph:c4eac4150242_2"},{"__ref":"Paragraph:c4eac4150242_3"},{"__ref":"Paragraph:c4eac4150242_4"},{"__ref":"Paragraph:c4eac4150242_5"},{"__ref":"Paragraph:c4eac4150242_6"},{"__ref":"Paragraph:c4eac4150242_7"},{"__ref":"Paragraph:c4eac4150242_8"},{"__ref":"Paragraph:c4eac4150242_9"},{"__ref":"Paragraph:c4eac4150242_10"},{"__ref":"Paragraph:c4eac4150242_11"},{"__ref":"Paragraph:c4eac4150242_12"},{"__ref":"Paragraph:c4eac4150242_13"},{"__ref":"Paragraph:c4eac4150242_14"},{"__ref":"Paragraph:c4eac4150242_15"},{"__ref":"Paragraph:c4eac4150242_16"},{"__ref":"Paragraph:c4eac4150242_17"},{"__ref":"Paragraph:c4eac4150242_18"},{"__ref":"Paragraph:c4eac4150242_19"},{"__ref":"Paragraph:c4eac4150242_20"},{"__ref":"Paragraph:c4eac4150242_21"},{"__ref":"Paragraph:c4eac4150242_22"},{"__ref":"Paragraph:c4eac4150242_23"},{"__ref":"Paragraph:c4eac4150242_24"},{"__ref":"Paragraph:c4eac4150242_25"},{"__ref":"Paragraph:c4eac4150242_26"},{"__ref":"Paragraph:c4eac4150242_27"},{"__ref":"Paragraph:c4eac4150242_28"},{"__ref":"Paragraph:c4eac4150242_29"},{"__ref":"Paragraph:c4eac4150242_30"},{"__ref":"Paragraph:c4eac4150242_31"},{"__ref":"Paragraph:c4eac4150242_32"},{"__ref":"Paragraph:c4eac4150242_33"},{"__ref":"Paragraph:c4eac4150242_34"},{"__ref":"Paragraph:c4eac4150242_35"},{"__ref":"Paragraph:c4eac4150242_36"},{"__ref":"Paragraph:c4eac4150242_37"},{"__ref":"Paragraph:c4eac4150242_38"},{"__ref":"Paragraph:c4eac4150242_39"},{"__ref":"Paragraph:c4eac4150242_40"},{"__ref":"Paragraph:c4eac4150242_41"},{"__ref":"Paragraph:c4eac4150242_42"},{"__ref":"Paragraph:c4eac4150242_43"},{"__ref":"Paragraph:c4eac4150242_44"},{"__ref":"Paragraph:c4eac4150242_45"},{"__ref":"Paragraph:c4eac4150242_46"},{"__ref":"Paragraph:c4eac4150242_47"},{"__ref":"Paragraph:c4eac4150242_48"},{"__ref":"Paragraph:c4eac4150242_49"},{"__ref":"Paragraph:c4eac4150242_50"},{"__ref":"Paragraph:c4eac4150242_51"},{"__ref":"Paragraph:c4eac4150242_52"},{"__ref":"Paragraph:c4eac4150242_53"},{"__ref":"Paragraph:c4eac4150242_54"},{"__ref":"Paragraph:c4eac4150242_55"},{"__ref":"Paragraph:c4eac4150242_56"},{"__ref":"Paragraph:c4eac4150242_57"},{"__ref":"Paragraph:c4eac4150242_58"},{"__ref":"Paragraph:c4eac4150242_59"},{"__ref":"Paragraph:c4eac4150242_60"},{"__ref":"Paragraph:c4eac4150242_61"},{"__ref":"Paragraph:c4eac4150242_62"},{"__ref":"Paragraph:c4eac4150242_63"},{"__ref":"Paragraph:c4eac4150242_64"},{"__ref":"Paragraph:c4eac4150242_65"},{"__ref":"Paragraph:c4eac4150242_66"},{"__ref":"Paragraph:c4eac4150242_67"},{"__ref":"Paragraph:c4eac4150242_68"},{"__ref":"Paragraph:c4eac4150242_69"},{"__ref":"Paragraph:c4eac4150242_70"},{"__ref":"Paragraph:c4eac4150242_71"},{"__ref":"Paragraph:c4eac4150242_72"},{"__ref":"Paragraph:c4eac4150242_73"},{"__ref":"Paragraph:c4eac4150242_74"},{"__ref":"Paragraph:c4eac4150242_75"},{"__ref":"Paragraph:c4eac4150242_76"},{"__ref":"Paragraph:c4eac4150242_77"},{"__ref":"Paragraph:c4eac4150242_78"},{"__ref":"Paragraph:c4eac4150242_79"},{"__ref":"Paragraph:c4eac4150242_80"},{"__ref":"Paragraph:c4eac4150242_81"},{"__ref":"Paragraph:c4eac4150242_82"},{"__ref":"Paragraph:c4eac4150242_83"},{"__ref":"Paragraph:c4eac4150242_84"},{"__ref":"Paragraph:c4eac4150242_85"},{"__ref":"Paragraph:c4eac4150242_86"},{"__ref":"Paragraph:c4eac4150242_87"},{"__ref":"Paragraph:c4eac4150242_88"},{"__ref":"Paragraph:c4eac4150242_89"},{"__ref":"Paragraph:c4eac4150242_90"},{"__ref":"Paragraph:c4eac4150242_91"},{"__ref":"Paragraph:c4eac4150242_92"},{"__ref":"Paragraph:c4eac4150242_93"},{"__ref":"Paragraph:c4eac4150242_94"},{"__ref":"Paragraph:c4eac4150242_95"},{"__ref":"Paragraph:c4eac4150242_96"},{"__ref":"Paragraph:c4eac4150242_97"},{"__ref":"Paragraph:c4eac4150242_98"},{"__ref":"Paragraph:c4eac4150242_99"},{"__ref":"Paragraph:c4eac4150242_100"},{"__ref":"Paragraph:c4eac4150242_101"},{"__ref":"Paragraph:c4eac4150242_102"},{"__ref":"Paragraph:c4eac4150242_103"},{"__ref":"Paragraph:c4eac4150242_104"},{"__ref":"Paragraph:c4eac4150242_105"},{"__ref":"Paragraph:c4eac4150242_106"},{"__ref":"Paragraph:c4eac4150242_107"},{"__ref":"Paragraph:c4eac4150242_108"},{"__ref":"Paragraph:c4eac4150242_109"},{"__ref":"Paragraph:c4eac4150242_110"},{"__ref":"Paragraph:c4eac4150242_111"},{"__ref":"Paragraph:c4eac4150242_112"},{"__ref":"Paragraph:c4eac4150242_113"},{"__ref":"Paragraph:c4eac4150242_114"},{"__ref":"Paragraph:c4eac4150242_115"},{"__ref":"Paragraph:c4eac4150242_116"},{"__ref":"Paragraph:c4eac4150242_117"},{"__ref":"Paragraph:c4eac4150242_118"},{"__ref":"Paragraph:c4eac4150242_119"},{"__ref":"Paragraph:c4eac4150242_120"}],"sections":[{"__typename":"Section","name":"3a74","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"64eb","startIndex":13,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"62d8","startIndex":39,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"9ea6","startIndex":69,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"d37b","startIndex":109,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"af0d","startIndex":118,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null}]}},"internalLinks({\"paging\":{\"limit\":8}})":{"__typename":"InternalLinksConnection","items":[{"__ref":"Post:3a05fc2ce720"},{"__ref":"Post:6dd0307f01c0"},{"__ref":"Post:9bc79d046f96"},{"__ref":"Post:ae8dd57a76cc"},{"__ref":"Post:a68de47158ec"},{"__ref":"Post:4ab1952f6211"},{"__ref":"Post:c11440bbf44a"},{"__ref":"Post:e3b160e7a389"}]}},"ImageMetadata:1*GqxckDnAbC_Wg_GoTnKtZQ.jpeg":{"id":"1*GqxckDnAbC_Wg_GoTnKtZQ.jpeg","__typename":"ImageMetadata","focusPercentX":null,"focusPercentY":null},"UserViewerEdge:userId:65f68453d97b-viewerId:2bf0e8de62c3":{"id":"userId:65f68453d97b-viewerId:2bf0e8de62c3","__typename":"UserViewerEdge","isFollowing":false,"isUser":false},"NewsletterV3:8664e7da18c5":{"id":"8664e7da18c5","__typename":"NewsletterV3","type":"NEWSLETTER_TYPE_AUTHOR","slug":"65f68453d97b","name":"65f68453d97b","collection":null,"user":{"__ref":"User:65f68453d97b"}},"User:65f68453d97b":{"id":"65f68453d97b","__typename":"User","name":"Catherine Wang","username":"catwang42","newsletterV3":{"__ref":"NewsletterV3:8664e7da18c5"},"bio":"Seeing the world through Political, Economic, Technological, and Cultural lens.👓 AI Practitioner who is trying to avoid the “black-box” futurism thinking 🦄","imageId":"1*GUfZ0r2OLtB6T_IxKgXr9g.jpeg","mediumMemberAt":0,"isPartnerProgramEnrolled":true,"viewerEdge":{"__ref":"UserViewerEdge:userId:65f68453d97b-viewerId:2bf0e8de62c3"},"viewerIsUser":false,"customDomainState":null,"hasSubdomain":false,"postSubscribeMembershipUpsellShownAt":0},"Post:3a05fc2ce720":{"id":"3a05fc2ce720","__typename":"Post","title":"A Data Science View of Herd Immunity, What do We Have to Pay to Stop The Virus?","mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Fa-data-science-view-of-herd-immunity-what-do-we-have-to-pay-to-stop-the-virus-3a05fc2ce720","previewImage":{"__ref":"ImageMetadata:1*GqxckDnAbC_Wg_GoTnKtZQ.jpeg"},"isPublished":true,"firstPublishedAt":1585787613566,"readingTime":10.862264150943396,"statusForCollection":"APPROVED","isLocked":true,"visibility":"LOCKED","collection":{"__ref":"Collection:7f60cf5620c9"},"creator":{"__ref":"User:65f68453d97b"},"previewContent":{"__typename":"PreviewContent","isFullContent":false}},"ImageMetadata:":{"id":"","__typename":"ImageMetadata","focusPercentX":null,"focusPercentY":null},"UserViewerEdge:userId:eac4a08942e4-viewerId:2bf0e8de62c3":{"id":"userId:eac4a08942e4-viewerId:2bf0e8de62c3","__typename":"UserViewerEdge","isFollowing":false,"isUser":false},"User:eac4a08942e4":{"id":"eac4a08942e4","__typename":"User","name":"Carolyn Manning","username":"carolyn-manning","bio":"","imageId":"1*dmbNkD5D-u45r44go_cf0g.png","mediumMemberAt":0,"isPartnerProgramEnrolled":false,"viewerEdge":{"__ref":"UserViewerEdge:userId:eac4a08942e4-viewerId:2bf0e8de62c3"},"viewerIsUser":false,"newsletterV3":null,"customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","domain":"carolyn-manning.medium.com"}},"hasSubdomain":true,"postSubscribeMembershipUpsellShownAt":0},"Post:6dd0307f01c0":{"id":"6dd0307f01c0","__typename":"Post","title":"CLI Data Gem Portfolio Project — Flatiron School","mediumUrl":"https:\u002F\u002Fcarolyn-manning.medium.com\u002Fcli-data-gem-portfolio-project-flatiron-school-6dd0307f01c0","previewImage":{"__ref":"ImageMetadata:"},"isPublished":true,"firstPublishedAt":1611019519153,"readingTime":2.641509433962264,"statusForCollection":null,"isLocked":false,"visibility":"PUBLIC","collection":null,"creator":{"__ref":"User:eac4a08942e4"},"previewContent":{"__typename":"PreviewContent","isFullContent":false}},"ImageMetadata:1*4RmE7R_-4PViLiniaMq_FA.png":{"id":"1*4RmE7R_-4PViLiniaMq_FA.png","__typename":"ImageMetadata","focusPercentX":null,"focusPercentY":null},"UserViewerEdge:userId:a5ba5219b56-viewerId:2bf0e8de62c3":{"id":"userId:a5ba5219b56-viewerId:2bf0e8de62c3","__typename":"UserViewerEdge","isFollowing":false,"isUser":false},"User:a5ba5219b56":{"id":"a5ba5219b56","__typename":"User","name":"Jefin Paul","username":"jefinpaul99","bio":"","imageId":"1*tSpgkvurl_xM__VtIXZrTg.jpeg","mediumMemberAt":0,"isPartnerProgramEnrolled":false,"viewerEdge":{"__ref":"UserViewerEdge:userId:a5ba5219b56-viewerId:2bf0e8de62c3"},"viewerIsUser":false,"newsletterV3":null,"customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","domain":"jefinpaul99.medium.com"}},"hasSubdomain":true,"postSubscribeMembershipUpsellShownAt":0},"Post:9bc79d046f96":{"id":"9bc79d046f96","__typename":"Post","title":"BOXPLOTS","mediumUrl":"https:\u002F\u002Fjefinpaul99.medium.com\u002Fboxplots-9bc79d046f96","previewImage":{"__ref":"ImageMetadata:1*4RmE7R_-4PViLiniaMq_FA.png"},"isPublished":true,"firstPublishedAt":1598363300748,"readingTime":3.9084905660377363,"statusForCollection":null,"isLocked":false,"visibility":"PUBLIC","collection":null,"creator":{"__ref":"User:a5ba5219b56"},"previewContent":{"__typename":"PreviewContent","isFullContent":false}},"ImageMetadata:1*Ij4lOqZzLLODoSXIbkDB8g.jpeg":{"id":"1*Ij4lOqZzLLODoSXIbkDB8g.jpeg","__typename":"ImageMetadata","focusPercentX":null,"focusPercentY":null},"CollectionViewerEdge:collectionId:5b301f10ddcd-viewerId:2bf0e8de62c3":{"id":"collectionId:5b301f10ddcd-viewerId:2bf0e8de62c3","__typename":"CollectionViewerEdge","isEditor":false},"Collection:5b301f10ddcd":{"id":"5b301f10ddcd","__typename":"Collection","name":"ITNEXT","description":"ITNEXT is a platform for IT developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies.","tagline":"ITNEXT is a platform for IT developers & software engineers…","domain":"itnext.io","slug":"itnext","isAuroraEligible":false,"isAuroraVisible":false,"viewerEdge":{"__ref":"CollectionViewerEdge:collectionId:5b301f10ddcd-viewerId:2bf0e8de62c3"},"canToggleEmail":false},"UserViewerEdge:userId:10287396d308-viewerId:2bf0e8de62c3":{"id":"userId:10287396d308-viewerId:2bf0e8de62c3","__typename":"UserViewerEdge","isFollowing":false,"isUser":false},"NewsletterV3:fb296b58857e":{"id":"fb296b58857e","__typename":"NewsletterV3","type":"NEWSLETTER_TYPE_AUTHOR","slug":"10287396d308","name":"10287396d308","collection":null,"user":{"__ref":"User:10287396d308"}},"User:10287396d308":{"id":"10287396d308","__typename":"User","name":"Dale Nguyen","username":"dalenguyen","newsletterV3":{"__ref":"NewsletterV3:fb296b58857e"},"bio":"Full Stack Developer \u002F JavaScript Enthusiast\u002F http:\u002F\u002Fdalenguyen.me","imageId":"1*tAsmdxlEMMP6pRqRH4vF4A.jpeg","mediumMemberAt":0,"isPartnerProgramEnrolled":true,"viewerEdge":{"__ref":"UserViewerEdge:userId:10287396d308-viewerId:2bf0e8de62c3"},"viewerIsUser":false,"customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","domain":"dalenguyen.medium.com"}},"hasSubdomain":true,"postSubscribeMembershipUpsellShownAt":0},"Post:ae8dd57a76cc":{"id":"ae8dd57a76cc","__typename":"Post","title":"Learning Data Science — Predict Adult Income with Decision Tree","mediumUrl":"https:\u002F\u002Fitnext.io\u002Flearning-data-science-predict-adult-income-with-decision-tree-ae8dd57a76cc","previewImage":{"__ref":"ImageMetadata:1*Ij4lOqZzLLODoSXIbkDB8g.jpeg"},"isPublished":true,"firstPublishedAt":1552794074509,"readingTime":4.565094339622641,"statusForCollection":"APPROVED","isLocked":false,"visibility":"PUBLIC","collection":{"__ref":"Collection:5b301f10ddcd"},"creator":{"__ref":"User:10287396d308"},"previewContent":{"__typename":"PreviewContent","isFullContent":false}},"ImageMetadata:1*Lxd_3RRDrZshK0cBGM0JSA.png":{"id":"1*Lxd_3RRDrZshK0cBGM0JSA.png","__typename":"ImageMetadata","focusPercentX":null,"focusPercentY":null},"CollectionViewerEdge:collectionId:dadcc1ee3e9e-viewerId:2bf0e8de62c3":{"id":"collectionId:dadcc1ee3e9e-viewerId:2bf0e8de62c3","__typename":"CollectionViewerEdge","isEditor":false},"Collection:dadcc1ee3e9e":{"id":"dadcc1ee3e9e","__typename":"Collection","name":"Watermarked","description":"Watermarked.io protects your intellectual property with a digital “watermark”. The watermark is invisible, inaudible and has no impact on content presentation but it is always there to prove copyright ownership and track unauthorized spread.","tagline":"Future standard for media copyright protection","domain":null,"slug":"watermarked-publication","isAuroraEligible":false,"isAuroraVisible":false,"viewerEdge":{"__ref":"CollectionViewerEdge:collectionId:dadcc1ee3e9e-viewerId:2bf0e8de62c3"},"canToggleEmail":false},"UserViewerEdge:userId:858d4d524208-viewerId:2bf0e8de62c3":{"id":"userId:858d4d524208-viewerId:2bf0e8de62c3","__typename":"UserViewerEdge","isFollowing":false,"isUser":false},"User:858d4d524208":{"id":"858d4d524208","__typename":"User","name":"Watermarked.io","username":"watermarked.io","bio":"Future standard for media copyright protection watermarked.io","imageId":"1*fX9ef-bI1dQrQoF-Y54WEw.png","mediumMemberAt":0,"isPartnerProgramEnrolled":false,"viewerEdge":{"__ref":"UserViewerEdge:userId:858d4d524208-viewerId:2bf0e8de62c3"},"viewerIsUser":false,"newsletterV3":null,"customDomainState":null,"hasSubdomain":false,"postSubscribeMembershipUpsellShownAt":0},"Post:a68de47158ec":{"id":"a68de47158ec","__typename":"Post","title":"Two-dimensional barcodes robustness analysis through the example of QR code and Aztec code","mediumUrl":"https:\u002F\u002Fmedium.com\u002Fwatermarked-publication\u002Ftwo-dimensional-barcodes-robustness-analysis-through-the-example-of-qr-code-and-aztec-code-a68de47158ec","previewImage":{"__ref":"ImageMetadata:1*Lxd_3RRDrZshK0cBGM0JSA.png"},"isPublished":true,"firstPublishedAt":1598620892852,"readingTime":12.076415094339623,"statusForCollection":"APPROVED","isLocked":false,"visibility":"PUBLIC","collection":{"__ref":"Collection:dadcc1ee3e9e"},"creator":{"__ref":"User:858d4d524208"},"previewContent":{"__typename":"PreviewContent","isFullContent":false}},"ImageMetadata:0*xlBKswkR2TnjKvdE":{"id":"0*xlBKswkR2TnjKvdE","__typename":"ImageMetadata","focusPercentX":null,"focusPercentY":null},"CollectionViewerEdge:collectionId:50f2e4f85341-viewerId:2bf0e8de62c3":{"id":"collectionId:50f2e4f85341-viewerId:2bf0e8de62c3","__typename":"CollectionViewerEdge","isEditor":false},"Collection:50f2e4f85341":{"id":"50f2e4f85341","__typename":"Collection","name":"EvidentEBM","description":"A survival guide and a shelter for the Data Sciencist and Researcher","tagline":"Evident — Your Evidence Based Community","domain":null,"slug":"evidentebm","isAuroraEligible":true,"isAuroraVisible":false,"viewerEdge":{"__ref":"CollectionViewerEdge:collectionId:50f2e4f85341-viewerId:2bf0e8de62c3"},"canToggleEmail":false},"UserViewerEdge:userId:9f33be0d7ddf-viewerId:2bf0e8de62c3":{"id":"userId:9f33be0d7ddf-viewerId:2bf0e8de62c3","__typename":"UserViewerEdge","isFollowing":false,"isUser":false},"NewsletterV3:ce9b6f3781e8":{"id":"ce9b6f3781e8","__typename":"NewsletterV3","type":"NEWSLETTER_TYPE_AUTHOR","slug":"9f33be0d7ddf","name":"9f33be0d7ddf","collection":null,"user":{"__ref":"User:9f33be0d7ddf"}},"User:9f33be0d7ddf":{"id":"9f33be0d7ddf","__typename":"User","name":"Santiago Rodrigues Manica","username":"santiagorodriguesma","newsletterV3":{"__ref":"NewsletterV3:ce9b6f3781e8"},"bio":"Physician, epidemiology enthusiast, and entrepreneur. Learning every day. Fueled by curiosity and challenges @R_M_Santiago #Lisbon","imageId":"1*ukkMgUsXPI_Pzn_nx1aBKQ.jpeg","mediumMemberAt":1587131432000,"isPartnerProgramEnrolled":true,"viewerEdge":{"__ref":"UserViewerEdge:userId:9f33be0d7ddf-viewerId:2bf0e8de62c3"},"viewerIsUser":false,"customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","domain":"santiagorodriguesma.medium.com"}},"hasSubdomain":true,"postSubscribeMembershipUpsellShownAt":0},"Post:4ab1952f6211":{"id":"4ab1952f6211","__typename":"Post","title":"Basic clinical research in RStudio","mediumUrl":"https:\u002F\u002Fmedium.com\u002Fevidentebm\u002Fbasic-clinical-research-in-rstudio-4ab1952f6211","previewImage":{"__ref":"ImageMetadata:0*xlBKswkR2TnjKvdE"},"isPublished":true,"firstPublishedAt":1607032425213,"readingTime":5.720754716981132,"statusForCollection":"APPROVED","isLocked":true,"visibility":"LOCKED","collection":{"__ref":"Collection:50f2e4f85341"},"creator":{"__ref":"User:9f33be0d7ddf"},"previewContent":{"__typename":"PreviewContent","isFullContent":false}},"ImageMetadata:1*XPh3CMmrtkWe5Cu7K8fNIA.png":{"id":"1*XPh3CMmrtkWe5Cu7K8fNIA.png","__typename":"ImageMetadata","focusPercentX":null,"focusPercentY":null},"CollectionViewerEdge:collectionId:500653fb51a1-viewerId:2bf0e8de62c3":{"id":"collectionId:500653fb51a1-viewerId:2bf0e8de62c3","__typename":"CollectionViewerEdge","isEditor":false},"Collection:500653fb51a1":{"id":"500653fb51a1","__typename":"Collection","name":"Data Syndrome","description":"Machine learning engineering for hire","tagline":"Machine learning engineering for hire","domain":"blog.datasyndrome.com","slug":"data-syndrome","isAuroraEligible":true,"isAuroraVisible":false,"viewerEdge":{"__ref":"CollectionViewerEdge:collectionId:500653fb51a1-viewerId:2bf0e8de62c3"},"canToggleEmail":false},"UserViewerEdge:userId:6b00541c45c8-viewerId:2bf0e8de62c3":{"id":"userId:6b00541c45c8-viewerId:2bf0e8de62c3","__typename":"UserViewerEdge","isFollowing":false,"isUser":false},"NewsletterV3:e91dbe50a5e3":{"id":"e91dbe50a5e3","__typename":"NewsletterV3","type":"NEWSLETTER_TYPE_AUTHOR","slug":"6b00541c45c8","name":"6b00541c45c8","collection":null,"user":{"__ref":"User:6b00541c45c8"}},"User:6b00541c45c8":{"id":"6b00541c45c8","__typename":"User","name":"Russell Jurney","username":"rjurney","newsletterV3":{"__ref":"NewsletterV3:e91dbe50a5e3"},"bio":"I am CTO at Deep Discovery where I fight global corruption with networks and AI","imageId":"1*FBP59XBi_mhBnAaisaWJKw.jpeg","mediumMemberAt":1509928121672,"isPartnerProgramEnrolled":true,"viewerEdge":{"__ref":"UserViewerEdge:userId:6b00541c45c8-viewerId:2bf0e8de62c3"},"viewerIsUser":false,"customDomainState":{"__typename":"CustomDomainState","live":null},"hasSubdomain":false,"postSubscribeMembershipUpsellShownAt":0},"Post:c11440bbf44a":{"id":"c11440bbf44a","__typename":"Post","title":"Building a Stack Overflow Tag Labeler","mediumUrl":"https:\u002F\u002Fblog.datasyndrome.com\u002Fbuilding-a-stack-overflow-tag-labeler-c11440bbf44a","previewImage":{"__ref":"ImageMetadata:1*XPh3CMmrtkWe5Cu7K8fNIA.png"},"isPublished":true,"firstPublishedAt":1607323202574,"readingTime":6.088993710691824,"statusForCollection":"APPROVED","isLocked":false,"visibility":"PUBLIC","collection":{"__ref":"Collection:500653fb51a1"},"creator":{"__ref":"User:6b00541c45c8"},"previewContent":{"__typename":"PreviewContent","isFullContent":false}},"ImageMetadata:1*fCp-aCEQzdQtotZbeVFM-g.png":{"id":"1*fCp-aCEQzdQtotZbeVFM-g.png","__typename":"ImageMetadata","focusPercentX":null,"focusPercentY":null},"UserViewerEdge:userId:9624f9c91699-viewerId:2bf0e8de62c3":{"id":"userId:9624f9c91699-viewerId:2bf0e8de62c3","__typename":"UserViewerEdge","isFollowing":false,"isUser":false},"User:9624f9c91699":{"id":"9624f9c91699","__typename":"User","name":"Colin Brinkley","username":"colinbrinkley138","bio":"","imageId":"2*rldQUDpKVhCUBqtBBE557w.png","mediumMemberAt":0,"isPartnerProgramEnrolled":false,"viewerEdge":{"__ref":"UserViewerEdge:userId:9624f9c91699-viewerId:2bf0e8de62c3"},"viewerIsUser":false,"newsletterV3":null,"customDomainState":null,"hasSubdomain":false,"postSubscribeMembershipUpsellShownAt":0},"Post:e3b160e7a389":{"id":"e3b160e7a389","__typename":"Post","title":"How Predictive Modeling Interprets the ESRB","mediumUrl":"https:\u002F\u002Fmedium.com\u002F@colinbrinkley138\u002Fhow-predictive-modeling-interprets-the-esrb-e3b160e7a389","previewImage":{"__ref":"ImageMetadata:1*fCp-aCEQzdQtotZbeVFM-g.png"},"isPublished":true,"firstPublishedAt":1572025796872,"readingTime":4.658490566037735,"statusForCollection":null,"isLocked":false,"visibility":"PUBLIC","collection":null,"creator":{"__ref":"User:9624f9c91699"},"previewContent":{"__typename":"PreviewContent","isFullContent":false}}}</script><script>window.__MIDDLEWARE_STATE__={"session":{"xsrf":""},"cache":{"cacheStatus":"HIT","shouldUseCache":true}}</script><script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/manifest.js"></script><script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/29713.js"></script><script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/main.js"></script><script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/45573.js"></script>
<script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/instrumentation.js"></script>
<script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/reporting.js"></script>
<script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/80685.js"></script>
<script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/11034.js"></script>
<script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/11615.js"></script>
<script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/90192.js"></script>
<script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/79088.js"></script>
<script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/81645.js"></script>
<script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/63303.js"></script>
<script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/5850.js"></script>
<script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/70832.js"></script>
<script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/72776.js"></script>
<script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/50327.js"></script>
<script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/5055.js"></script>
<script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/51073.js"></script>
<script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/61781.js"></script>
<script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/37801.js"></script>
<script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/26022.js"></script>
<script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/39592.js"></script>
<script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/7790.js"></script>
<script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/33673.js"></script>
<script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/95972.js"></script>
<script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/92397.js"></script>
<script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/11366.js"></script>
<script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/62182.js"></script>
<script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/58695.js"></script>
<script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/71794.js"></script>
<script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/68519.js"></script>
<script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/35285.js"></script>
<script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/46736.js"></script>
<script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/43642.js"></script>
<script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/65980.js"></script>
<script src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/Post.js"></script><script>window.main();</script><script defer="defer" src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/v64f9daad31f64f81be21cbef6184a5e31634941392597" integrity="sha512-gV/bogrUTVP2N3IzTDKzgP0Js1gg4fbwtYB6ftgLbKQu/V8yH2+lrKCfKHelh4SO3DPzKj4/glTO+tNJGDnb0A==" data-cf-beacon="{&quot;rayId&quot;:&quot;6bcd55c57eced53f&quot;,&quot;token&quot;:&quot;0b5f665943484354a59c39c6833f7078&quot;,&quot;version&quot;:&quot;2021.11.0&quot;,&quot;si&quot;:100}" crossorigin="anonymous"></script>
<iframe src="Understanding%20Variational%20Autoencoders%20(VAEs)%20|%20by%20Joseph%20Rocca%20|%20Towards%20Data%20Science_files/a16180790160.html" tabindex="-1" title="Optimizely Internal Frame" style="display: none;" width="0" hidden="" height="0"></iframe></body></html>