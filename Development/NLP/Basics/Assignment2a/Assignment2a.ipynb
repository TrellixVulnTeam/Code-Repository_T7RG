{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "78c4d45d1f56d116ef9ef6a4e0a1f0e7",
     "grade": false,
     "grade_id": "cell-4dad29f85abd546f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 2a: Language Modelling via N-grams (5 Marks)\n",
    "\n",
    "## Due: March 17, 2022\n",
    "\n",
    "Welcome to the first part of Assignment 2. In this assignment you will implement N-gram language models as discussed in lecture 3 and 4 of the course. We will be working with the [WikiText-2 dataset](https://paperswithcode.com/dataset/wikitext-2) which consists of about 100 million tokens collected from Good and Featured articles from wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    data_dir = \"gdrive/MyDrive/PlakshaNLP/Assignment2a/data/wikitext-2\"\n",
    "except:\n",
    "    data_dir = \"/datadrive/t-kabir/work/repos/PlakshaNLP/source/Assignment2a/data/wikitext-2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "208a3686cd75f206390a99b912d7d829",
     "grade": false,
     "grade_id": "cell-21f93cdd721bf4ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/blink/anaconda3/lib/python3.7/site-packages (1.16.2)\n",
      "Requirement already satisfied: pandas in /home/blink/anaconda3/lib/python3.7/site-packages (0.24.2)\n",
      "Requirement already satisfied: pytz>=2011k in /home/blink/anaconda3/lib/python3.7/site-packages (from pandas) (2018.9)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/blink/anaconda3/lib/python3.7/site-packages (from pandas) (1.16.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /home/blink/anaconda3/lib/python3.7/site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/blink/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
      "Requirement already satisfied: nltk in /home/blink/anaconda3/lib/python3.7/site-packages (3.4)\n",
      "Requirement already satisfied: six in /home/blink/anaconda3/lib/python3.7/site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: singledispatch in /home/blink/anaconda3/lib/python3.7/site-packages (from nltk) (3.4.0.3)\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1570ffda89cc3709398a7df2272bd72",
     "grade": false,
     "grade_id": "cell-5279dd8d57f286fd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/blink/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\n",
      "[nltk_data] Downloading package punkt to /home/blink/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/blink/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We start by importing libraries that we will be making use of in the assignment.\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1555fb9d5057e572d16bc628a97cd370",
     "grade": false,
     "grade_id": "cell-cf1fc2b2db15dff0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We start by loading Wiki-Text dataset into memory. It consists of 3 text files one each for train, validation and test, consisting raw text data corresponding to different wikipedia articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='data/wikitext-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a348a0412559dcb0d1f176e9e25e387",
     "grade": false,
     "grade_id": "cell-43171f704b366858",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets Loaded!\n",
      "Number of characters in train corpus: 10780437\n",
      "Number of characters in validation corpus: 1120192\n",
      "Number of characters in test corpus: 1255018\n"
     ]
    }
   ],
   "source": [
    "# Load train data\n",
    "with open(f\"{data_dir}/wiki.train.tokens\") as f:\n",
    "    wiki_train = f.read()\n",
    "\n",
    "# Load validation data\n",
    "with open(f\"{data_dir}/wiki.valid.tokens\") as f:\n",
    "    wiki_valid = f.read()\n",
    "\n",
    "# Load Test data\n",
    "with open(f\"{data_dir}/wiki.test.tokens\") as f:\n",
    "    wiki_test = f.read()\n",
    "    \n",
    "print(f\"Datasets Loaded!\")\n",
    "print(f\"Number of characters in train corpus: {len(wiki_train)}\")\n",
    "print(f\"Number of characters in validation corpus: {len(wiki_valid)}\")\n",
    "print(f\"Number of characters in test corpus: {len(wiki_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8f6ad0493e137468e30f2c2c74258e05",
     "grade": false,
     "grade_id": "cell-8e48dba7173b8e38",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n = Valkyria Chronicles III = \\n \\n Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 ,'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets print first 100 characters of the train_corpus\n",
    "wiki_train[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1346493e9471b11fe669e297f907dfba",
     "grade": false,
     "grade_id": "cell-d0144eb7bcea588b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 1: Fit n-gram probabilities (2 Marks)\n",
    "\n",
    "N-gram language models are trained by estimating the joint and conditional probabilities of all the n-grams from the training corpus. Using these probabilities we can then sample next sequence of tokens given the context or compute the probability of a complete piece of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f00167d218520c4c12feeb5b1302d17b",
     "grade": false,
     "grade_id": "cell-b81f32aabd61b562",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 1.1: Unigram Probabilities (0.75 Mark)\n",
    "\n",
    "We will start by estimating unigram probabilities from the corpus, which can be simply done by calculating the normalized frequency of each token in the corpus. Implement the `get_unigram_probs` function below that does that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16977f9ce3a0d0fcc89d5144a68d2c8a",
     "grade": false,
     "grade_id": "cell-9894d67d3e54861b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_unigram_probs(corpus):\n",
    "    \"\"\"\n",
    "    Estimates the probability of each unigram in the text by calculating\n",
    "    normalized frequency of each unigram (word).\n",
    "    \n",
    "    Inputs:\n",
    "        - corpus (str): A python string consisting of a piece of text\n",
    "    \n",
    "    Returns:\n",
    "        - unigram_probs (dict): A python dictionary with unigrams (words) as keys and \n",
    "                                their frequencies as values\n",
    "                                \n",
    "    Example Input: \"I am Sam . Sam I am . I do not like green eggs and ham .\"\n",
    "    Expected Output: {'I': 0.17647058823529413,\n",
    "             'am': 0.11764705882352941,\n",
    "             'Sam': 0.11764705882352941,\n",
    "             '.': 0.17647058823529413,\n",
    "             'do': 0.058823529411764705,\n",
    "             'not': 0.058823529411764705,\n",
    "             'like': 0.058823529411764705,\n",
    "             'green': 0.058823529411764705,\n",
    "             'eggs': 0.058823529411764705,\n",
    "             'and': 0.058823529411764705,\n",
    "             'ham': 0.058823529411764705}\n",
    "    \n",
    "    Note: To break the text into tokens you can just use corpus.split() this time,\n",
    "    Wiki corpus is already tokenized so just using split() will also work fine.\n",
    "    **Do not use `nltk.tokenize.word_tokenize` here**\n",
    "\n",
    "    Hint: `defaultdict` can often make life easier in such type of problems.\n",
    "            https://docs.python.org/3/library/collections.html#collections.defaultdict\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    tokens=np.array(corpus.split(' '))\n",
    "    vocab=np.array(list(set(tokens)))\n",
    "    print(vocab)\n",
    "    unigram_probs = {token:0 for token in list(set(tokens))}\n",
    "    probabilities=[]\n",
    "    for word in vocab:\n",
    "        probabilities.append(len(np.where(vocab==word)[0])/len(vocab))\n",
    "    \n",
    "    \n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I' 'not' 'like' 'eggs' 'ham' '.' 'do' 'and' 'green' 'Sam' 'am']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09090909090909091,\n",
       " 0.09090909090909091,\n",
       " 0.09090909090909091,\n",
       " 0.09090909090909091,\n",
       " 0.09090909090909091,\n",
       " 0.09090909090909091,\n",
       " 0.09090909090909091,\n",
       " 0.09090909090909091,\n",
       " 0.09090909090909091,\n",
       " 0.09090909090909091,\n",
       " 0.09090909090909091]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_unigram_probs('I am Sam . Sam I am . I do not like green eggs and ham .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f4e5be8dfec5715e4b53b64a2f877067",
     "grade": true,
     "grade_id": "cell-46656780cd552710",
     "locked": true,
     "points": 0.75,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Sample Test Case 1\n",
      "Input Corpus: I am Sam . Sam I am . I do not like green eggs and ham .\n",
      "Unigram Probabilities: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Expected Unigram Probabilities: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "isinstance() arg 2 must be a type or tuple of types",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-b44e0a2e42c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected Unigram Probabilities: {output_unigram_probs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mcheck_dicts_same\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_unigram_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_unigram_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample Test Case Passed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"****************************************\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-b44e0a2e42c4>\u001b[0m in \u001b[0;36mcheck_dicts_same\u001b[0;34m(dict1, dict2)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_dicts_same\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Your function output is not a dictionary!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: isinstance() arg 2 must be a type or tuple of types"
     ]
    }
   ],
   "source": [
    "def check_dicts_same(dict1, dict2):\n",
    "    if not (isinstance(dict1, dict) or isinstance(dict1, defaultdict)):\n",
    "        print(\"Your function output is not a dictionary!\")\n",
    "        return False\n",
    "    if len(dict1) != len(dict2):\n",
    "        return False\n",
    "    \n",
    "    for key in dict1:\n",
    "        val1 = dict1[key]\n",
    "        val2 = dict2[key]\n",
    "        if isinstance(val1, float) and isinstance(val2, float):\n",
    "            if not np.allclose(val1, val2, 1e-4):\n",
    "                return False\n",
    "        if val1 != val2:\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "print(\"Running Sample Test Case 1\")\n",
    "sample_corpus = 'I am Sam . Sam I am . I do not like green eggs and ham .'\n",
    "exp_unigram_probs = {'I': 0.17647058823529413,\n",
    "             'am': 0.11764705882352941,\n",
    "             'Sam': 0.11764705882352941,\n",
    "             '.': 0.17647058823529413,\n",
    "             'do': 0.058823529411764705,\n",
    "             'not': 0.058823529411764705,\n",
    "             'like': 0.058823529411764705,\n",
    "             'green': 0.058823529411764705,\n",
    "             'eggs': 0.058823529411764705,\n",
    "             'and': 0.058823529411764705,\n",
    "             'ham': 0.058823529411764705}\n",
    "\n",
    "output_unigram_probs = get_unigram_probs(sample_corpus)\n",
    "print(f\"Input Corpus: {sample_corpus}\")\n",
    "print(f\"Unigram Probabilities: {output_unigram_probs}\")\n",
    "print(f\"Expected Unigram Probabilities: {output_unigram_probs}\")\n",
    "\n",
    "assert check_dicts_same(exp_unigram_probs, output_unigram_probs)\n",
    "print(\"Sample Test Case Passed\")\n",
    "print(\"****************************************\\n\")\n",
    "    \n",
    "print(\"Running Sample Test Case 2\")\n",
    "sample_corpus = 'john likes to watch movies mary likes movies too mary also likes to watch football games'\n",
    "exp_unigram_probs = {'john': 0.0625,\n",
    "                     'likes': 0.1875,\n",
    "                     'to': 0.125,\n",
    "                     'watch': 0.125,\n",
    "                     'movies': 0.125,\n",
    "                     'mary': 0.125,\n",
    "                     'too': 0.0625,\n",
    "                     'also': 0.0625,\n",
    "                     'football': 0.0625,\n",
    "                     'games': 0.0625}\n",
    "\n",
    "output_unigram_probs = get_unigram_probs(sample_corpus)\n",
    "print(f\"Input Corpus: {sample_corpus}\")\n",
    "print(f\"Unigram Probabilities: {output_unigram_probs}\")\n",
    "print(f\"Expected Unigram Probabilities: {exp_unigram_probs}\")\n",
    "\n",
    "assert check_dicts_same(output_unigram_probs, exp_unigram_probs)\n",
    "print(\"Sample Test Case Passed\")\n",
    "print(\"****************************************\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "58e3c6b8f1ccd263ddc4b898c937f964",
     "grade": false,
     "grade_id": "cell-cad2c95328fb6979",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's get the unigram probabilities from the entire training corpus now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "347a78332953ce36c99023b81435c6f7",
     "grade": false,
     "grade_id": "cell-10bd3b75407498ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_unigram_probs = get_unigram_probs(wiki_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4a9a5cb0b8fa6d6be678916a13b44e9",
     "grade": false,
     "grade_id": "cell-e882c4441289799e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 1.2: N-gram Probabilities (1.25 Marks)\n",
    "\n",
    "Now let's estimate the probabilities for N-grams with N > 1. For N-grams instead of storing joint probability of each N-gram in the corpus i.e. p(w_n-N+1, ...,w_n-1, w_n) we store conditional probabilities p(w_n | w_n-N+1, ...,w_n-1), which can be calculated easily by using the following formula:\n",
    "\n",
    "<img src=\"https://i.ibb.co/mtFcBfh/n-gram.jpg\" alt=\"n-gram\" border=\"0\">\n",
    "\n",
    "Here the term in numerator denotes the number of N-grams with the words w_n-N+1, ...,w_n-1, w_n and the term in denominator denotes the number of (N-1) grams with the words w_n-N+1, ...,w_n-1 .\n",
    "\n",
    "Implement the `get_ngram_cond_probs` function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08f54f5cab96950494180af8cf9a469b",
     "grade": false,
     "grade_id": "cell-d2dd7df0fa9339e9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_ngram_cond_probs(corpus, N = 2):\n",
    "    \"\"\"\n",
    "    Estimates the N-gram conditional probabilities from the corpus.\n",
    "    \n",
    "    Inputs:\n",
    "        - corpus (str): A python string consisting of a piece of text\n",
    "        - N (int) : Value of  N in N-gram\n",
    "        \n",
    "    Returns:\n",
    "        - ngram_cond_probs (dict) : A dictionary with (N-1)-gram tuple as keys and values as dictionaries\n",
    "                                    such that ngram_cond_probs[(w_n-N+1, ...,w_n-1)][w_n]\n",
    "                                    stores the conditional probability p(w_n | w_n-N+1, ...,w_n-1)\n",
    "                                    See the examples below for clarity\n",
    "                                    \n",
    "    Example Input: corpus = 'I am Sam . Sam I am .', N = 2\n",
    "    Expected Output: \n",
    "    {\n",
    "         ('I',): {'am': 1.0},\n",
    "         ('am',): {'Sam': 0.5, '.': 0.5},\n",
    "         ('Sam',): {'.': 0.5, 'I': 0.5},\n",
    "         ('.',): {'Sam': 1.0}\n",
    " \n",
    "     }\n",
    "     Explanation: 'I' is always followed 'am' in the `corpus`, while 'am' is followed by 'Sam' once and '.' the other time\n",
    "                     hence you see 0.5, 0.5 probabilities for these two tokens.\n",
    "     \n",
    "    Example Input: corpus = 'I am Sam . Sam I am .', N = 3\n",
    "    Expected Output:\n",
    "    {\n",
    "         ('I', 'am'): {'Sam': 0.5, '.': 0.5},\n",
    "         ('am', 'Sam'): {'.': 1.0},\n",
    "         ('Sam', '.'): {'Sam': 1.0},\n",
    "         ('.', 'Sam'): {'I': 1.0},\n",
    "         ('Sam', 'I'): {'am': 1.0}\n",
    "     }\n",
    " \n",
    "    \"\"\"\n",
    "    \n",
    "    ngram_cond_probs = {}\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return (ngram_cond_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40ad85b65a1e0802cdc32f70411080a2",
     "grade": true,
     "grade_id": "cell-50b5e23ba10e4996",
     "locked": true,
     "points": 1.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def check_dicts_same_v2(dict1, dict2):\n",
    "    if not (isinstance(dict1, dict) or isinstance(dict1, defaultdict)):\n",
    "        print(\"Your function output is not a dictionary!\")\n",
    "        return False\n",
    "    \n",
    "    if len(dict1) != len(dict2):\n",
    "        return False\n",
    "        \n",
    "    for key in dict1:\n",
    "        val1 = dict1[key]\n",
    "        val2 = dict2[key]\n",
    "        if isinstance(val1, float) and isinstance(val2, float):\n",
    "            if not np.allclose(val1, val2, 1e-4):\n",
    "                return False\n",
    "        if (isinstance(val1, dict) or isinstance(val1, defaultdict)) \\\n",
    "            and (isinstance(val2, dict) or isinstance(val2, defaultdict)):\n",
    "            if not check_dicts_same_v2(val1, val2):\n",
    "                return False\n",
    "        if val1 != val2:\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "print(\"Running Sample Test Case 1\")\n",
    "sample_corpus = 'I am Sam . Sam I am .'\n",
    "N = 2\n",
    "exp_ngram_probs =     {\n",
    "         ('I',): {'am': 1.0},\n",
    "         ('am',): {'Sam': 0.5, '.': 0.5},\n",
    "         ('Sam',): {'.': 0.5, 'I': 0.5},\n",
    "         ('.',): {'Sam': 1.0}\n",
    "     }\n",
    "\n",
    "output_ngram_probs = get_ngram_cond_probs(sample_corpus, N = N)\n",
    "print(f\"Input Corpus: {sample_corpus}, N = {N}\")\n",
    "print(f\"N-gram Probabilities: {output_ngram_probs}\")\n",
    "print(f\"Expected N-gram Probabilities: {exp_ngram_probs}\")\n",
    "\n",
    "assert check_dicts_same_v2(output_ngram_probs, exp_ngram_probs)\n",
    "print(\"Sample Test Case Passed\")\n",
    "print(\"****************************************\\n\")\n",
    "\n",
    "print(\"Running Sample Test Case 2\")\n",
    "sample_corpus = 'I am Sam . Sam I am .'\n",
    "N = 3\n",
    "exp_ngram_probs =         {\n",
    "         ('I', 'am'): {'Sam': 0.5, '.': 0.5},\n",
    "         ('am', 'Sam'): {'.': 1.0},\n",
    "         ('Sam', '.'): {'Sam': 1.0},\n",
    "         ('.', 'Sam'): {'I': 1.0},\n",
    "         ('Sam', 'I'): {'am': 1.0}\n",
    "     }\n",
    "\n",
    "output_ngram_probs = get_ngram_cond_probs(sample_corpus, N = N)\n",
    "print(f\"Input Corpus: {sample_corpus}, N = {N}\")\n",
    "print(f\"N-gram Probabilities: {output_ngram_probs}\")\n",
    "print(f\"Expected N-gram Probabilities: {exp_ngram_probs}\")\n",
    "\n",
    "assert check_dicts_same_v2(output_ngram_probs, exp_ngram_probs)\n",
    "print(\"Sample Test Case Passed\")\n",
    "print(\"****************************************\\n\")\n",
    "\n",
    "print(\"Running Sample Test Case 3\")\n",
    "sample_corpus = 'I am Sam . Sam I am .'\n",
    "N = 4\n",
    "exp_ngram_probs =  {('I', 'am', 'Sam'): {'.': 1.0},\n",
    "             ('am', 'Sam', '.'): {'Sam': 1.0},\n",
    "             ('Sam', '.', 'Sam'): {'I': 1.0},\n",
    "             ('.', 'Sam', 'I'): {'am': 1.0},\n",
    "             ('Sam', 'I', 'am'): {'.': 1.0}}\n",
    "\n",
    "output_ngram_probs = get_ngram_cond_probs(sample_corpus, N = N)\n",
    "print(f\"Input Corpus: {sample_corpus}, N = {N}\")\n",
    "print(f\"N-gram Probabilities: {output_ngram_probs}\")\n",
    "print(f\"Expected N-gram Probabilities: {exp_ngram_probs}\")\n",
    "\n",
    "assert check_dicts_same_v2(output_ngram_probs, exp_ngram_probs)\n",
    "print(\"Sample Test Case Passed\")\n",
    "print(\"****************************************\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "027b0659ee2dc75e70d00cd89cdc3e96",
     "grade": false,
     "grade_id": "cell-0c43ed56dbe13464",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can now generate N-gram probability distributions for our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61b1117df6d92bed9230d11f967a5eaf",
     "grade": false,
     "grade_id": "cell-d817f2a967337003",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Generating Bigram Distribution\")\n",
    "bigram_prob_dist = get_ngram_cond_probs(wiki_train, N = 2)\n",
    "\n",
    "print(\"Generating Trigram Distribution\")\n",
    "trigram_prob_dist = get_ngram_cond_probs(wiki_train, N = 3)\n",
    "\n",
    "print(\"Generating 4-gram Distribution\")\n",
    "tetragram_prob_dist = get_ngram_cond_probs(wiki_train, N = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "787553d25794b79bca8e4544ed445f97",
     "grade": false,
     "grade_id": "cell-2e3e35cb9475d27b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 2: Evaluating the language models (2.5 Marks)\n",
    "\n",
    "Now that we have generated the N-gram distributions for various Ns we can evaluate the language models both quantitatively using the perplexity metric as well as qualitatively by generating text using these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "df3bbf0afd04cd2ceb252099dde2c984",
     "grade": false,
     "grade_id": "cell-85ffb0083a4e789b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 2.1: Calculating Perplexity (0.5 Marks)\n",
    "\n",
    "As discussed in Lecture 4, perplexity of a probability distribution is defined as:\n",
    "\n",
    "<img src=\"https://i.ibb.co/swXqFCz/Perplexity.jpg\" alt=\"Perplexity\" border=\"0\">\n",
    "\n",
    "For the purposes of this assignment we will only calculate perplexity of a unigram language model. Finding perplexity of N-gram models for N > 1 requires smoothing which is beyond the scope of this assignment, but interested students can read about it [here](https://web.stanford.edu/~jurafsky/slp3/3.pdf). For now, implement the `get_unigram_perplexity` function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e02e9c75a23a5f9b9e03d52354d475ea",
     "grade": false,
     "grade_id": "cell-8c5a8fb5ce9456e5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_unigram_perplexity(corpus, unigram_probs):\n",
    "    \"\"\"\n",
    "    Calculates the perplexity of a unigram language model on a text corpus\n",
    "    \n",
    "    Inputs:\n",
    "        - corpus (str): A python string consisting of a piece of text    \n",
    "        - unigram_probs (dict): A python dictionary with unigrams (words) as keys and \n",
    "                                their probabilities as values\n",
    "    Returns:\n",
    "        - perplexity (float) : Perplexity of unigram model on corpus\n",
    "    \n",
    "    Note 1: You can assume that each word/unigram that appears in the corpus, we have it's probability in `unigram_probs`\n",
    "    Note 2: Use log to the base 2 for calculating entropy\n",
    "    \"\"\"\n",
    "    \n",
    "    perplexity = None\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f004b8bc9e92500099a95d92a47b177e",
     "grade": true,
     "grade_id": "cell-407824efd9e6de69",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Perplexity for simple models like unigram can blow up easily for large corpora.\n",
    "# And Hence we only evaluate on the first 100 tokens of each data split \n",
    "wiki_train_subsample = \" \".join(wiki_train.split()[:100])\n",
    "wiki_valid_subsample = \" \".join(wiki_valid.split()[:100])\n",
    "wiki_test_subsample = \" \".join(wiki_test.split()[:100])\n",
    "\n",
    "\n",
    "train_ppl = get_unigram_perplexity(wiki_train_subsample, train_unigram_probs)\n",
    "expected_train_ppl = 66.87292185710841\n",
    "print(f\"Training Data Perplexity: {train_ppl}\")\n",
    "print(f\"Expected Training Data Perplexity: {expected_train_ppl}\")\n",
    "assert np.allclose(train_ppl, expected_train_ppl, 1e-4)\n",
    "\n",
    "print(\"**********************************\\n\")\n",
    "\n",
    "valid_ppl = get_unigram_perplexity(wiki_valid_subsample, train_unigram_probs)\n",
    "expected_valid_ppl = 73.3487215828768\n",
    "print(f\"Validation Data Perplexity: {valid_ppl}\")\n",
    "print(f\"Expected Validation Data Perplexity: {expected_valid_ppl}\")\n",
    "assert np.allclose(valid_ppl, expected_valid_ppl, 1e-4)\n",
    "\n",
    "print(\"**********************************\\n\")\n",
    "\n",
    "test_ppl = get_unigram_perplexity(wiki_test_subsample, train_unigram_probs)\n",
    "expected_test_ppl = 54.97601260063733\n",
    "print(f\"Test Data Perplexity: {test_ppl}\")\n",
    "print(f\"Expected Test Data Perplexity: {expected_test_ppl}\")\n",
    "assert np.allclose(test_ppl, expected_test_ppl, 1e-4)\n",
    "\n",
    "print(\"**********************************\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1a9084fc0d8d2a51d22fc242304f1615",
     "grade": false,
     "grade_id": "cell-4e9803311447c8f3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 2.2: Generating texts using N-gram LMs (2.5 Marks)\n",
    "\n",
    "Using the estimated N-gram probabilities we can use the N-gram language models to generate pieces of text. We start with N-1 tokens and then use them as context to predict the next token form our estimated distribution. This process is then done repeatedly till we reach the maximum specified length. Implement the `generate_text_unigram` and `generate_text_ngram` functions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ccefeddf59d10947634e2b831aa966cb",
     "grade": true,
     "grade_id": "cell-d0ca4be3123b8357",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_text_unigram(unigram_probs, max_len = 10):\n",
    "    \"\"\"\n",
    "    Generates a random piece of text by sampling from `unigram_probs`\n",
    "    \n",
    "    Inputs:\n",
    "        - unigram_probs (dict): A dictionary containing probabilities of each unigram in the dataset.\n",
    "        - max_len (int) : Maximum length of the sequence (in terms of number of words/tokens) to be generated\n",
    "        \n",
    "    Returns:\n",
    "        - gen_text (str) : Text sampled from the unigram model\n",
    "        \n",
    "    Hint: np.random.choice might come in handy (pay special attention to what to supply as the value of its argument `p`)\n",
    "    \"\"\"\n",
    "    \n",
    "    gen_text = None\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63850a95c68f862ca2694bee2e1a7a3a",
     "grade": true,
     "grade_id": "cell-034597726f1551be",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "gen_text = generate_text_unigram(train_unigram_probs, 10)\n",
    "expected_text = 'The sustainable 2013 located of of , teeth became International'\n",
    "print(f\"Generated Text: {gen_text}\")\n",
    "print(f\"Expected Text: {expected_text}\")\n",
    "assert gen_text == expected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "deac46a1f988c797d818646a0fa1aa0c",
     "grade": true,
     "grade_id": "cell-95b05dc181f536f0",
     "locked": false,
     "points": 1.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_text_ngram(ngram_probs, seed_text, max_len = 10):\n",
    "    \"\"\"\n",
    "    Generates a random piece of text by sampling from `ngram_probs`\n",
    "    \n",
    "    Inputs:\n",
    "        - ngram_probs (dict): A dictionary containing containing conditional probabilities for each N-gram in the dataset.\n",
    "        - seed_text (str): A string containing N-1 tokens to use for starting the generated sequence\n",
    "        - max_len (int) : Maximum length of the sequence (in terms of number of words/tokens) to be generated\n",
    "        \n",
    "    Returns:\n",
    "        - gen_text (str) : Text sampled from the ngram model\n",
    "        \n",
    "    Hint: np.random.choice might come in handy (pay special attention to what to supply as the value of its argument `p`)\n",
    "    \n",
    "    Examples:\n",
    "        - Let's say we want to generate text from a bigram model (N = 2). The seed text will always have N-1 tokens i.e.\n",
    "            in this case 1. For the case where seed_text is \"The\", you look up at all bigrams starting with \"The\" in `ngram_probs`.\n",
    "            let's say the sampled word is \"brown\", so next you look at all the bigrams starting with \"brown\" and sample the\n",
    "            next word and repeat the procedure.\n",
    "        - For a trigram model and seed_text \"The brown\", you will similarly start by looking at the trigrams strating with\n",
    "            \"The brown\", sample next word. let's say it is \"fox\", then you look for all the trigrams starting\n",
    "            with \"brown fox\" and sample the next word and so on.\n",
    "    \"\"\"\n",
    "    \n",
    "    gen_text = None\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "55ce2c67f03b4d6b519d89a574cf85dd",
     "grade": true,
     "grade_id": "cell-a1b6099f4b708794",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "gen_text = generate_text_ngram(bigram_prob_dist, \"The\", max_len= 20)\n",
    "expected_text = \"The resulting March 1944 . The song , piano at Victoria . Relay events of the United States , was an\"\n",
    "print(f\"Generated Text: {gen_text}\")\n",
    "print(f\"Expected Text: {expected_text}\")\n",
    "assert gen_text == expected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82a1584a9b693e723d1f24a91cbdc421",
     "grade": true,
     "grade_id": "cell-e72b515cef45a4bf",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "gen_text = generate_text_ngram(trigram_prob_dist, \"The resulting\", max_len= 20)\n",
    "expected_text = 'The resulting scuttling of the strongest aftershock since the <unk> Room . \" Mosley \\'s 1968 restoration . The bridge was written'\n",
    "print(f\"Generated Text: {gen_text}\")\n",
    "print(f\"Expected Text: {expected_text}\")\n",
    "assert gen_text == expected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a01cd8e950ba4cd12bb44d9932f2dbb",
     "grade": true,
     "grade_id": "cell-db6356c48fddf694",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "gen_text = generate_text_ngram(tetragram_prob_dist, \"= = =\", max_len= 20)\n",
    "expected_text = '= = = = Liu Kang was designed to produce . By the end of April . The months that receive the most'\n",
    "print(f\"Generated Text: {gen_text}\")\n",
    "print(f\"Expected Text: {expected_text}\")\n",
    "assert gen_text == expected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
