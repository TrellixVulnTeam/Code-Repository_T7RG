{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Student_Ver] Plaksha_CSS_Code-Mixing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Studying CSS patterns of Indian Speakers in Code-Mixed Tweets\n",
        "\n",
        "In this notebook, we will learn one of the most useful ways of processing Code-Mixed text from twitter (by utilizing word-level language identification), and then analyze initial patterns in the data that consists of tweets from/about Indian speakers.\n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "1. Downloading and Preparing Data\n",
        "2. Installing and Running LID-tool\n",
        "3. Running LID on the Tweets\n",
        "4. Analyzing Code-Mixing Patterns in Tweets\n",
        "5. Resources\n",
        "\n",
        "\n",
        "**Note:** Please click on \"Runtime\" in the File Menu above and select \"Factory reset runtime\" before following the tutorial. And select \"Save a Copy in Drive\" before running the notebook."
      ],
      "metadata": {
        "id": "xSQdCdTEYcCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading and Preparing Data"
      ],
      "metadata": {
        "id": "ukh1ajuFXP98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. About Data\n",
        "\n",
        "- We are using a custom dataset that was created for this class specifically. It has 510 tweets from/about Indian speakers.\n",
        "- We have sampled some code-mixed English-Hindi tweets in roman script from [Silent Flame's NER Code-Mixed Corpus](https://github.com/SilentFlame/Named-Entity-Recognition).\n",
        "- We have extracted tweets related to the query `india` from Twitter, using [twitter's developer API](https://developer.twitter.com/en/docs/twitter-api/tweets/search/introduction).\n",
        "- **Note:** None of this data is owned by us. It is not used or intended to be used for commercial purposes. It's being used for educational/scholarly purposes and available openly along with the code. If you decide to use it for commercial purposes, please reach out to the above parties for consent/license.\n",
        "\n",
        "Let's download the data from GitHub!"
      ],
      "metadata": {
        "id": "VvJ7iz6wYKl4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyHGK6fBXeIS"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/mohdsanadzakirizvi/plaksha_rasa/main/assignments/plaksha_tweet_data_1.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Exploring the Data"
      ],
      "metadata": {
        "id": "vFT8pkdURjWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's time to do an initial exploration of the data."
      ],
      "metadata": {
        "id": "coMuDK2xrQJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read data\n",
        "import pandas as pd\n",
        "\n",
        "tweetdf = pd.read_csv('plaksha_tweet_data_1.csv')\n",
        "tweetdf"
      ],
      "metadata": {
        "id": "dF13otxLYJnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a closer look at some of the tweets"
      ],
      "metadata": {
        "id": "VCTnBtESrVp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, twt in enumerate(tweetdf['Tweet'][36:41]):\n",
        "  print(idx, ':', twt)"
      ],
      "metadata": {
        "id": "krdiMvzcY9Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tweetdf['Tweet'][36])"
      ],
      "metadata": {
        "id": "SzoI7YIQNzgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We notice that there are certain inconsistencies in the raw tweets, let's do a basic round of preprocessing to fix that. We will normalize the text by first converting everything to lower case and then removing URLs and non-alphabetic characters."
      ],
      "metadata": {
        "id": "ot8-2ieFrhaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def text_cleaner(text):\n",
        "    #converting to lowercase\n",
        "    newString = text.lower()\n",
        "    #removing links\n",
        "    newString = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', newString) \n",
        "    #fetching alphabetic characters\n",
        "    newString = re.sub('[^a-zA-Z#@]', ' ', newString)\n",
        "    return newString"
      ],
      "metadata": {
        "id": "YXsjD58hNdCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the below tweet is much more cleaner than before. It's time to do the same for all the tweets in the dataset,"
      ],
      "metadata": {
        "id": "Mlf9FOqjr2A3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_cleaner(tweetdf['Tweet'][36]))"
      ],
      "metadata": {
        "id": "g2jouXRBODiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweetdf['cleaned_text'] = tweetdf['Tweet'].apply(text_cleaner)\n",
        "\n",
        "tweetdf.head()"
      ],
      "metadata": {
        "id": "ns7q11fWOLLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've cleaned the tweets, let's build a word cloud and see what kind of word frequency distribution we have in the underlying data."
      ],
      "metadata": {
        "id": "G6Ahhe3MsHCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import libraries\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "#Generate word cloud\n",
        "total_text = \" \".join(tweetdf.cleaned_text.tolist())\n",
        "wc = WordCloud(width=400, height=330, max_words=100, background_color='white').generate_from_text(total_text)\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O_t8Joa2O1Ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above visualization gives a rudimentary overview of topic/word distribution in the data. We can see some of the topics that are recurring in the data. We also notice that lot of high-frequency words are code-mixed words from Hindi written in roman script."
      ],
      "metadata": {
        "id": "ZpKc-_GHsQxx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing LID-tool\n",
        "\n"
      ],
      "metadata": {
        "id": "A5US2Fa5xxZu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### About LID-tool \n",
        "\n",
        "- It is a word level language identification tool for identifying Code-Mixed text of languages (like Hindi etc.) written in roman script and mixed with English.\n",
        "\n",
        "- At a broader level, we utilize a ML classifier that's trained using [MALLET](https://mimno.github.io/Mallet/index) to generate word level probabilities for language tags. We then utilize these probabilities along with the context information of surrounding words to generate language tags for each word of the input. \n",
        "\n",
        "- We also use hand-crafted dictionaries as look-up tables to cover unique, corner and conflicting cases to give a robust language identification tool. \n",
        "\n",
        "- Note: LID is shorthand for [Language Identification](https://en.wikipedia.org/wiki/Language_identification).\n",
        "\n",
        "Now that we've learned about the tool, let's install it!\n"
      ],
      "metadata": {
        "id": "xcQ2FHebtu9s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0BoSxoLBluy"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/microsoft/LID-tool.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls LID-tool/"
      ],
      "metadata": {
        "id": "EPchoMWaB7EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
        "!unzip mallet-2.0.8.zip"
      ],
      "metadata": {
        "id": "O4aytLu-Vpo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install twitter-text-python"
      ],
      "metadata": {
        "id": "f30H9SpvC77G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir LID-tool/mallet-2.0.8\n",
        "!cp -r mallet-2.0.8 LID-tool/"
      ],
      "metadata": {
        "id": "jeUYohvIF2la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd LID-tool/"
      ],
      "metadata": {
        "id": "BB5tzuYhWFJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv dictionaries/dict1hi.txt dictionaries/dicthi.txt\n",
        "!touch dictionaries/dict1hi.txt\n",
        "\n",
        "!mv dictionaries/dict1hinmov.txt dictionaries/dicthinmov.txt\n",
        "!touch dictionaries/dict1hinmov.txt"
      ],
      "metadata": {
        "id": "L01iXVFtZGS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you've run all the above cells without an error then your setup of LID-tool is complete!"
      ],
      "metadata": {
        "id": "5KdwTtgptNHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running LID on the Tweets\n",
        "\n",
        "Now that we have installed LID-tool, let's run it on a bunch of sample sentences that are given in the tool. The following are the sentences we want to be language tagged:"
      ],
      "metadata": {
        "id": "JMHHESJOSExB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat sampleinp.txt"
      ],
      "metadata": {
        "id": "m9nr-v7HolLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Initial run on sample tweets\n",
        "\n"
      ],
      "metadata": {
        "id": "K2IeruNDSJdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm sampleinp.txt_tagged"
      ],
      "metadata": {
        "id": "Tq1GNFaSiFfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python getLanguage.py sampleinp.txt"
      ],
      "metadata": {
        "id": "cZEh6NMeFM89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that in the third sentence, the words `main` and `kya` have high English probabilities even though they are actually Hindi words in that sentence's context. While the same word `main` is actually an English word in the context of fourth sentence and accordingly has high English probabilities.\n",
        "\n",
        "Let's see what this means for out final language tag output:"
      ],
      "metadata": {
        "id": "6jeK0-Ytt8Yi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat sampleinp.txt_tagged"
      ],
      "metadata": {
        "id": "ccKjnDCYSDb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the LID-tool has performed well by tagging `main` in th fourth sentence as EN. But, it has mistaked the words `main` and `kya` as EN in the third sentence. \n",
        "\n",
        "All is not bad though, since it has correctly predicted the word `karoon` as HI in the third sentence with high probability.\n",
        "\n",
        "Let's see how we can improve the above output by utilizing the concept of dictionaries."
      ],
      "metadata": {
        "id": "a7-IXXmDuk_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Improving the output by utilizing dictionaries\n",
        "\n",
        "The LID-tool has two major components: an ML model trained on n-gram features and a couple of word lists (dictionaries) in both the languages. \n",
        "\n",
        "![](https://raw.githubusercontent.com/microsoft/LID-tool/main/images/info_flow_new_lid.PNG)\n",
        "\n"
      ],
      "metadata": {
        "id": "971uJICRSSVh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's make slight change in our dictionaries. Go to `LID-tool/dictionaries/dict1hinmov.txt` file and the words \"kya\" and \"main\" in separate lines one at a time. Then let's rerun the pipeline and check what changes."
      ],
      "metadata": {
        "id": "juXMv2y4cx5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm sampleinp.txt_tagged\n",
        "!rm dictionaries/memoize_dict.pkl"
      ],
      "metadata": {
        "id": "OohQZ4Nv0yuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python getLanguage.py sampleinp.txt"
      ],
      "metadata": {
        "id": "RpBm7hWBSHz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat sampleinp.txt_tagged"
      ],
      "metadata": {
        "id": "Om9kdiyu03n7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The addition of dictionaries in the project was an engineering decision that was taken after considering the empirical results, which showed that the dictionaries complemented the performance of the ML-based classifier (MALLET) for certain corner-cases. \n",
        "\n",
        "Here are some of the problems that this method solved:\n",
        "\n",
        "**1.Dealing with “common words” that can belong to either of the languages.**\n",
        "\n",
        "For example, the English word `“to”` is one of the ways in which the Hindi word `“तो”` or `“तू”` is spelt when written in `roman script` so the word “to” will be classified differently in the following two sentences:\n",
        "\n",
        "\n",
        "**Input:**       I have to get back to my advisor\n",
        "\n",
        "**Output:**    I/EN have/EN ***to/EN*** get/EN back/EN to/EN my/EN advisor/EN \n",
        "\n",
        "**Input:**       Bhai to kabhi nahi sudhrega\n",
        "\n",
        "**Output:**    Bhai/HI ***to/HI*** kabhi/HI nahi/HI sudhrega/HI\n",
        "\n",
        "\n",
        "In this case, we make sure that the word “to” is present in both the dictionaries and the LID is supposed to focus more on the combination of ML probabilities and Context (surrounding words) to tag the language. For instance, the probability values \"1e-09\" and \"0.999999999\" indicate the word is present in dictionary(s).\n",
        "\n",
        "**2.Words that surely belong to only one language.** \n",
        "\n",
        "For example, words like “bhai”, “nahi”, “kabhi” in Hindi and words like “advisor”, “get” etc. in English. In this case, we utilize the relevant dictionary to force tag it to the correct language even if the ML classifier says otherwise.\n",
        "\n",
        "So the questions that you have to ask yourself while creating the dictionaries are: \n",
        "\n",
        "1. “Are there certain words that can be spelt the same way in both the languages?” And, \n",
        "2. “Are there common words in one language that surely can’t be used in the other language?”\n",
        "\n",
        "These are just a couple of things that we looked at while building this tool, but given your specific use-case you can consider more such engineering use-cases and customize the dictionaries accordingly. \n",
        "\n",
        "- [Implementation in Code](https://github.com/microsoft/LID-tool/blob/f6528ebd8ac77b561f10d3659799f3f8894deb09/getLanguage.py#L147)\n",
        "- [More details in Docs](https://github.com/microsoft/LID-tool/blob/main/Train_Custom_LID.md)"
      ],
      "metadata": {
        "id": "12nbKb4IhjoQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyzing Code-Mixing Patterns in Tweets"
      ],
      "metadata": {
        "id": "WKAz_MqPScon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Getting Language Tagged Tweets\n",
        "\n",
        "Due to the limitations of free version of Colab and in the interest of time, we have already computed language tags for all the tweets in the dataset. Let's fetch them!"
      ],
      "metadata": {
        "id": "sQ0s91GhROgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/mohdsanadzakirizvi/plaksha_rasa/main/assignments/tagged_tweets.txt"
      ],
      "metadata": {
        "id": "NKBzXytnRSf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('tagged_tweets.txt') as fp:\n",
        "  tagged_tweets = fp.readlines()\n",
        "\n",
        "tagged_tweets[:5]"
      ],
      "metadata": {
        "id": "f5xeZ740xozV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tagged_tweets)"
      ],
      "metadata": {
        "id": "H03yI335y3aH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Basic aggregation metrics\n",
        "\n",
        "Let's find out the number of tweets in each of the categories: Mostly En, Mostly Hi and Code-Mixed. We will also cluster the tweets later based on it."
      ],
      "metadata": {
        "id": "yg9XDOBZSiRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# threshold of 70% of total word length\n",
        "thres = 0.7\n",
        "\n",
        "outdf = []\n",
        "\n",
        "# aggregate word frequencies\n",
        "for twt in tagged_tweets:\n",
        "  en_fr, hi_fr = 0, 0\n",
        "  if not twt.startswith('##'):\n",
        "    tags = twt.split()\n",
        "    taglen = len(tags)\n",
        "    for t in tags:\n",
        "      if 'HI' in t:\n",
        "        hi_fr += 1\n",
        "      if 'EN' in t:\n",
        "        en_fr += 1\n",
        "    \n",
        "    if hi_fr/taglen >= thres:\n",
        "      outdf.append((twt, 0, 1, 0))\n",
        "    elif en_fr/taglen >= thres:\n",
        "      outdf.append((twt, 1, 0, 0))\n",
        "    else:\n",
        "      outdf.append((twt, 0, 0, 1))\n",
        "\n",
        "# create the output dataframe\n",
        "outdf = pd.DataFrame(outdf, columns = ['Tweet', 'EN', 'HI', 'CM'])\n",
        "outdf.head()"
      ],
      "metadata": {
        "id": "hn6RJJigyhG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So now we have tagged each tweet based on whether it has mostly EN, HI or Code-Mixed words. \n",
        "\n",
        "**P.S.** You can play with the threshold value `thresh` given above and see how things change. Go crazy!\n",
        "\n",
        "Let's now aggregate how many tweets fall in each category!"
      ],
      "metadata": {
        "id": "0T0lf5-wx7lP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outdf.sum()"
      ],
      "metadata": {
        "id": "L0KlqkwU2fcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do you want to see tweets falling in a particular category? Just replace the 'CM' with the required tag in code in the following line:"
      ],
      "metadata": {
        "id": "62I2Dz2s2t19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outdf[outdf['CM'] == 1]"
      ],
      "metadata": {
        "id": "WOagAddx2396"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Extracting user mentions and hashtags\n",
        "\n",
        "We now have a general idea of the distribution of EN, HI and CM heavy tweets.\n",
        " Let's try to find some explaination of what might be the reason of tweets having the prominence of one of the above languages.\n",
        "\n",
        "In order to do that, we can take advantage of one information that we have present in our tweets: hashtags (starting with `#` and user mentions (starting with `@`)"
      ],
      "metadata": {
        "id": "j8nTdgnYSnlW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tagdf = []\n",
        "\n",
        "# extract user mentions and tags\n",
        "for twt in tagged_tweets:\n",
        "  hashtags, users = [], []\n",
        "  if not twt.startswith('##'):\n",
        "    tags = twt.split()\n",
        "    taglen = len(tags)\n",
        "    for t in tags:\n",
        "      if 'OTHER' in t:\n",
        "        tok = t.split('/')[0]\n",
        "        if t.startswith('@'):\n",
        "          users.append(tok)\n",
        "        if t.startswith('#'):\n",
        "          hashtags.append(tok)\n",
        "    tagdf.append((twt, hashtags, users))\n",
        "\n",
        "# create the output dataframe\n",
        "tagdf = pd.DataFrame(tagdf, columns = ['Tweet', 'Hashtags', 'Users'])\n",
        "tagdf.head()"
      ],
      "metadata": {
        "id": "nrd7ZVZoSvPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Hypothesis\n",
        "\n",
        "Now that we have tagged the tweets by both the prominent language and the hashtags + user mentions, let's merge the two dataframes and look at the bigger picture!"
      ],
      "metadata": {
        "id": "kdvW5bm9S2Mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finaldf = outdf.merge(tagdf)\n",
        "\n",
        "finaldf.sample(20)"
      ],
      "metadata": {
        "id": "jsGE2ZBmS64t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- On running the above script multiple times, we notice that there are patterns dictating when a tweet has Code-mixing vs. when it's just all English or all Hindi.\n",
        "\n",
        "- One clear pattern is that the tweets that are from formal settings like those by news agencies or organisations (@reuters, @geologytime, @ani etc.) tend to use pure English in their tweets even when they are talking about incidents in India. It is true for even Indian news agencies.\n",
        "\n",
        "- Conversely, we see lot of Code-Mixed words or majority of Hindi words in tweets that are by individual accounts. These are everyday folks just expressing their honest opinion on certain topics (having hashtags of major incidents in Indian society). Since, Code-mixing occurs naturally to them in their speech and day to day informal interactions, there's a tendency to do the same online. Which is interesting because they aren't just going for all Hindi words or even an all Hindi script but are heavily mixing both the script and words with Roman English.\n",
        "\n",
        "- Another interesting pattern is that some organisations tend to use Code-Mixing when they are advertising a product or service (ex. @jio). Which is probably a marketing hack of making the customer feel \"closer\" to the product by speaking in \"their colloquial language\" which is usually full of Code-Mixing.\n",
        "\n",
        "- The above set of early hypothesis corroborates with previous studies of Code-Mixing that it's a spoken language phenomenon that tends to happen more in informal contexts like speech, social media and chat apps like Whatsapp, Messenger etc.\n",
        "\n",
        "- Please note that the above are but qualitative observations, we can perform more rigorous quantitative analyses to come up with solid conclusions from the data.\n",
        "\n",
        "- Do you see any other patterns? Thoughts?"
      ],
      "metadata": {
        "id": "mlsUzTSU8Zo5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resources\n",
        "\n",
        "You can refer the following resources for more details about the project:\n",
        "\n",
        "- [GitHub Page](https://github.com/microsoft/LID-tool)\n",
        "- [Papers](https://github.com/microsoft/LID-tool#papers)"
      ],
      "metadata": {
        "id": "qWmLyWIAnAxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BaEgvQFmpb2f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}