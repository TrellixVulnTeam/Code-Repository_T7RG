{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "DecisionTree.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d08b4252-b204-47ba-b32b-2adca9633a8c"
      },
      "source": [
        "<h1> Machine Learning 1 : C-M-004 Homework 4"
      ],
      "id": "d08b4252-b204-47ba-b32b-2adca9633a8c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d4a7bd9-0d28-4f40-a31b-2e8921c7f888"
      },
      "source": [
        "* Data \n",
        "\n",
        "| x1 | x2 | y |\n",
        "|--- | --- | --- |\n",
        "| F | 12 | F |\n",
        "| F | 14 | Y |\n",
        "| T | 13 | Y |\n",
        "| T | 16 | F |\n",
        "\n",
        "The best discretization for x2 from an information gain perpective\n",
        "is based on which threshold (if a threshold is θ, then x2 ≤θ is 0,\n",
        "else\n",
        "* (i) 12, (ii) 13, (iii) 14, (iv) 16 (10 points)\n",
        "Based on entropy, the first split will be based on:\n",
        "* (i) x1, (ii) x2, (iii) Doesn’t matter (10 points)"
      ],
      "id": "3d4a7bd9-0d28-4f40-a31b-2e8921c7f888"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d6ee1a9-28f2-4048-aff2-a69fccf4281e"
      },
      "source": [
        "# Answers"
      ],
      "id": "6d6ee1a9-28f2-4048-aff2-a69fccf4281e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28d58b7c-a2ea-4987-9cec-1babd59cd5b5"
      },
      "source": [
        "x1Entropy at the root node\n",
        "\n",
        "Number of F : 2 <br>\n",
        "Number of Y : 2 <br>\n",
        "\n",
        "Entropy at root node = $ - \\sum _0 ^n p_i log(p_i) $\n",
        "= $ -\\frac{2}{4} log(\\frac{2}{4}) $ - $ -\\frac{2}{4} log(\\frac{2}{4}) $\n"
      ],
      "id": "28d58b7c-a2ea-4987-9cec-1babd59cd5b5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e545612f-636b-4600-8c86-4ac9ba892cdb",
        "outputId": "fa82166f-418a-4dd1-9de6-dbd1ae1dd9dd"
      },
      "source": [
        "print('Entropy at node: ',-(2/4*math.log2(2/4))-(2/4*math.log2(2/4)))"
      ],
      "id": "e545612f-636b-4600-8c86-4ac9ba892cdb",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entropy at node:  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba4f8a20-c980-429c-82f0-445eb2524955"
      },
      "source": [
        "### Finding the entropy gain when split by x1"
      ],
      "id": "ba4f8a20-c980-429c-82f0-445eb2524955"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7955583-3663-4855-9081-e6e94ffe3eb1"
      },
      "source": [
        "If we split the node by x1, the resulting nodes will be <br>\n",
        "Node 1 : [FY] <br>\n",
        "Node 2 : [YF] <br>\n",
        "<br>\n",
        "Calculating the entropt of individual nodes:<br>\n",
        "\n",
        "\n",
        "Node 1: $ -\\frac{1}{2} log(\\frac{1}{2}) $-$ -\\frac{1}{2} log(\\frac{1}{2}) $ <br>\n",
        "Node 2: $ -\\frac{1}{2} log(\\frac{1}{2}) $-$ -\\frac{1}{2} log(\\frac{1}{2}) $"
      ],
      "id": "a7955583-3663-4855-9081-e6e94ffe3eb1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dc24772-48b0-42ee-af71-ee5f1864e19f",
        "outputId": "083cab32-282c-4b56-e52e-0656b0cb0c77"
      },
      "source": [
        "n1=-(1/2*math.log2(1/2))-(1/2*math.log2(1/2))\n",
        "n2=-(1/2*math.log2(1/2))-(1/2*math.log2(1/2))\n",
        "print('Entropy of Node 1: ',n1)\n",
        "print('Entropy of Node 2: ',n2)\n",
        "print('Entropy gain', ((2/4)*n1)+((2/4)*n2))"
      ],
      "id": "8dc24772-48b0-42ee-af71-ee5f1864e19f",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entropy of Node 1:  1.0\n",
            "Entropy of Node 2:  1.0\n",
            "Entropy gain 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4c421ee-f5b5-43f1-8981-a73ef3c4e1b9"
      },
      "source": [
        "## Finding the entropy when split by x2"
      ],
      "id": "e4c421ee-f5b5-43f1-8981-a73ef3c4e1b9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ecab44c-8d0c-4ca2-a128-fb9bf017f603"
      },
      "source": [
        "import math"
      ],
      "id": "5ecab44c-8d0c-4ca2-a128-fb9bf017f603",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7669b0b4-458f-464f-b058-6819c0f2a437"
      },
      "source": [
        "Here we can have 4 possibilities\n",
        "* When 12 is used as threshold, x2 becomes [T,F,F,F], Hence ouput is classified as [F] and [YYF]\n",
        "* When 14 is used as threshold, x3 becomes [T,T,T,F],hence output is classified as [FYY] and [F]\n",
        "* When 13 is used as threshold, x3 becomes [T,F,T,F],hence output is classified as [FY] and [YF]\n",
        "* When 16 is used as threshold, x4 becomes [T,T,T,T],hence output is classified as [FYYF] and []\n",
        "\n",
        "\n",
        "\n",
        "Finding all four possible entropy gains\n",
        "\n",
        "* Case 1, when 12 is threshold"
      ],
      "id": "7669b0b4-458f-464f-b058-6819c0f2a437"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a68a20c4-77fd-4560-be6c-f53c44b7aabf",
        "outputId": "46f8415d-00c7-4315-dd01-b827bfb8e363"
      },
      "source": [
        "n1=-(1/1*math.log2(1/1))\n",
        "n2=-(2/3*math.log2(2/3))-(1/3*math.log2(1/3))\n",
        "print('Entropy of Node 1: ',n1)\n",
        "print('Entropy of Node 2: ',n2)\n",
        "print('Entropy gain', ((1/4)*n1)+((3/4)*n2))"
      ],
      "id": "a68a20c4-77fd-4560-be6c-f53c44b7aabf",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entropy of Node 1:  -0.0\n",
            "Entropy of Node 2:  0.9182958340544896\n",
            "Entropy gain 0.6887218755408672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91ac4799-19db-43fb-85d5-9cacb6b42e60"
      },
      "source": [
        "\n",
        "* Case 1, when 14 is threshold"
      ],
      "id": "91ac4799-19db-43fb-85d5-9cacb6b42e60"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e958df7c-97fc-496e-abd3-0c3ca916548b",
        "outputId": "38aa0e4a-2d12-4c32-a20b-5c57fa5a5af0"
      },
      "source": [
        "n1=-(1/3*math.log2(1/3))-(2/3*math.log2(2/3))\n",
        "n2=-(1/1*math.log2(1/1))\n",
        "print('Entropy of Node 1: ',n1)\n",
        "print('Entropy of Node 2: ',n2)\n",
        "print('Entropy gain', ((3/4)*n1)+((1/4)*n2))"
      ],
      "id": "e958df7c-97fc-496e-abd3-0c3ca916548b",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entropy of Node 1:  0.9182958340544896\n",
            "Entropy of Node 2:  -0.0\n",
            "Entropy gain 0.6887218755408672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9637d81e-ffae-41f4-9018-a77a52dfbf22"
      },
      "source": [
        "\n",
        "* Case 1, when 13 is threshold"
      ],
      "id": "9637d81e-ffae-41f4-9018-a77a52dfbf22"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e89782c7-45e6-485d-bd83-239c0e73e800",
        "outputId": "16e4390c-6406-41f6-beae-65513af1d91f"
      },
      "source": [
        "n1=-(1/2*math.log2(1/2))-(1/2*math.log2(1/2))\n",
        "n2=-(1/2*math.log2(1/2))-(1/2*math.log2(1/2))\n",
        "print('Entropy of Node 1: ',n1)\n",
        "print('Entropy of Node 2: ',n2)\n",
        "print('Entropy gain', ((2/4)*n1)+((2/4)*n2))"
      ],
      "id": "e89782c7-45e6-485d-bd83-239c0e73e800",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entropy of Node 1:  1.0\n",
            "Entropy of Node 2:  1.0\n",
            "Entropy gain 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb8ced23-dcde-40b2-b7f8-60e49765dc8e"
      },
      "source": [
        "\n",
        "* Case 1, when 16 is threshold"
      ],
      "id": "fb8ced23-dcde-40b2-b7f8-60e49765dc8e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e48ee200-6deb-44cd-8894-a14b75992210",
        "outputId": "02d32cf2-dcd6-4c9b-8095-7f167f64bd5e"
      },
      "source": [
        "n1=-(2/4*math.log2(2/4))-(2/4*math.log2(2/4))\n",
        "print('Entropy of Node 1: ',n1)\n",
        "print('Entropy gain', ((4/4)*n1))"
      ],
      "id": "e48ee200-6deb-44cd-8894-a14b75992210",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entropy of Node 1:  1.0\n",
            "Entropy gain 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7943482-4318-4914-8ea3-1eb6d8156314"
      },
      "source": [
        "<h2> Answers and Conclusions </h2>\n",
        "\n",
        "* The best discretization for x2 based on the above result is based on the value 12 and 14. \n",
        "When the split is by considering 12 as threshold or 14 as threshold, we get the lowest entropy gain. \n",
        "\n",
        "* When split is based on x2, with 12 and 14 we get the nodes with lower entropy. Hence, the split would be the best based on x2. When splitting by other parameters it creates a \n"
      ],
      "id": "c7943482-4318-4914-8ea3-1eb6d8156314"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "bf9cac29-be2e-4ba2-bb4b-9e49da727782"
      },
      "source": [
        "# 2. Is a random forest of random forests a good idea? \n",
        "* No, Absolutely not\n",
        "* Yes, of course\n",
        "* Maybe, varies from case to case\n",
        "* None of the above "
      ],
      "id": "bf9cac29-be2e-4ba2-bb4b-9e49da727782"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "b15753c8-e9de-4def-af1f-9b63e5a7ae2d"
      },
      "source": [
        "<h2> Null Hypothesis</h2>\n",
        "<h4> A random forest contains a lot of low performers whose outputs are then aggregated together to give the final output. Here, it is ensured that the different low performer models created are as independent from each other. <br>\n",
        "    If we create a random forest, it will itself be a high performing model. When we try to create a random forest using a random forest, the models would be less independent of each other and hence the random forest or random forest would perform low and the individual models may not be low performers. </h4>"
      ],
      "id": "b15753c8-e9de-4def-af1f-9b63e5a7ae2d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cab70f1-bee0-4c8a-8f32-1dd486b257de"
      },
      "source": [
        "#Testing the same with Wisconsin Dataset"
      ],
      "id": "3cab70f1-bee0-4c8a-8f32-1dd486b257de",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c6248b5-8755-43db-ae53-cde0bf4a3882",
        "outputId": "52a1d1e7-bc79-455e-b3c0-0dd60ef1ac36"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "inputs,outputs=load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Creating a decision dree classifier object\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dtClassifier=DecisionTreeClassifier(random_state=1)\n",
        "\n",
        "# Using K Fold cross validation\n",
        "k_fold=KFold(n_splits=5,random_state=1,shuffle=True)\n",
        "scores=[]\n",
        "for train_index,test_index in k_fold.split(inputs):\n",
        "    x_train=inputs[train_index]\n",
        "    y_train=outputs[train_index]\n",
        "    x_test=inputs[test_index]\n",
        "    y_test=outputs[test_index]\n",
        "    \n",
        "    dtClassifier.fit(x_train,y_train)\n",
        "    scores.append(dtClassifier.score(x_test,y_test))\n",
        "print(sum(scores)/len(scores))\n",
        "# Creating a random forest\n",
        "random_forest=BaggingClassifier(base_estimator=dtClassifier,n_estimators=20,random_state=1)\n",
        "#K Fold on Random forest\n",
        "# Using K Fold cross validation\n",
        "\n",
        "k_fold=KFold(n_splits=5,random_state=1,shuffle=True)\n",
        "scores=[]\n",
        "for train_index,test_index in k_fold.split(inputs):\n",
        "    x_train=inputs[train_index]\n",
        "    y_train=outputs[train_index]\n",
        "    x_test=inputs[test_index]\n",
        "    y_test=outputs[test_index]\n",
        "    \n",
        "    random_forest.fit(x_train,y_train)\n",
        "    scores.append(random_forest.score(x_test,y_test))\n",
        "print(sum(scores)/len(scores))\n",
        "\n",
        "# Creating a forest of forests\n",
        "random_forest_1=BaggingClassifier(base_estimator=random_forest,n_estimators=20,random_state=1)\n",
        "#K Fold on Random forest\n",
        "# Using K Fold cross validation\n",
        "\n",
        "k_fold=KFold(n_splits=5,random_state=1,shuffle=True)\n",
        "scores=[]\n",
        "for train_index,test_index in k_fold.split(inputs):\n",
        "    x_train=inputs[train_index]\n",
        "    y_train=outputs[train_index]\n",
        "    x_test=inputs[test_index]\n",
        "    y_test=outputs[test_index]\n",
        "    \n",
        "    random_forest_1.fit(x_train,y_train)\n",
        "    scores.append(random_forest_1.score(x_test,y_test))\n",
        "print(sum(scores)/len(scores))\n",
        "\n",
        "# Creating a forest of forests of forests\n",
        "random_forest_2=BaggingClassifier(base_estimator=random_forest_1,n_estimators=20,random_state=1)\n",
        "#K Fold on Random forest\n",
        "# Using K Fold cross validation\n",
        "\n",
        "k_fold=KFold(n_splits=5,random_state=1,shuffle=True)\n",
        "scores=[]\n",
        "for train_index,test_index in k_fold.split(inputs):\n",
        "    x_train=inputs[train_index]\n",
        "    y_train=outputs[train_index]\n",
        "    x_test=inputs[test_index]\n",
        "    y_test=outputs[test_index]\n",
        "    \n",
        "    random_forest_2.fit(x_train,y_train)\n",
        "    scores.append(random_forest_2.score(x_test,y_test))\n",
        "print(sum(scores)/len(scores))"
      ],
      "id": "4c6248b5-8755-43db-ae53-cde0bf4a3882",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9297314081664337\n",
            "0.9437820214252446\n",
            "0.9578481602235678\n",
            "0.9508306163639186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0e288cf-3040-48ac-8c5f-a76167fde0d7"
      },
      "source": [
        "<h3>\n",
        "From the above testing, it can be seen that the accuracy is increasing for a forest of forest. Hence, rejecting Null hypothesis.\n",
        "\n",
        "One creating a forest of forest of forest, it can be seen that the accuracy has dropped a little, but it is still more than the random forest itself. Hence, it can be that the Bagging Classifier tries to create independent models which are low performing random forest classifiers and then it finally creates a deeper and wider tree which are low performing classifiers. \n",
        "    \n",
        "</h3>\n",
        "\n",
        "<h2> C. Maybe, varies from case to case"
      ],
      "id": "b0e288cf-3040-48ac-8c5f-a76167fde0d7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "205bb82c-7ecb-4f24-a850-cf1933ef1a37"
      },
      "source": [
        ""
      ],
      "id": "205bb82c-7ecb-4f24-a850-cf1933ef1a37",
      "execution_count": null,
      "outputs": []
    }
  ]
}